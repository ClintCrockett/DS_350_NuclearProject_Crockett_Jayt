[
  {
    "objectID": "w02/Second_Quarto_File.html",
    "href": "w02/Second_Quarto_File.html",
    "title": "Second_Quarto_File",
    "section": "",
    "text": "Code\n# Working with the Flights Data Set\navg_delay <- flights %>%\n  filter(arr_delay > 0) %>%\n  group_by(origin) %>% \n  summarise(avg_delay_total = mean(arr_delay, na.rm = TRUE)) %>% \n  ungroup()\n\nplot_ly(avg_delay, type='barpolar', r=~avg_delay_total, theta=~origin, color=~origin) %>%  \n  layout(\n    title = \"2013 NYC Flight Average Arrival Delays\",\n    polar = list(\n      radialaxis = list(title = \"Number of Delays\"),\n      angularaxis = list(categoryarray = flights$origin)\n    )\n )\n\n\n\n\n\n\n\nOkay technically this isn’t a pie chart so I should be good… right? Originally I wanted to to do a racial bar chart but I could not find anything on plotly and ggplot didn’t work like I wanted it to. I’d love to learn if there is a way to create it if you have any tips."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "about",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "w01/First_Quarto_File.html",
    "href": "w01/First_Quarto_File.html",
    "title": "First_Quarto_File",
    "section": "",
    "text": "Code\n## Sourced Articles\n\n### Article 1: Top 10 Movies on Netflix Right Now(https://www.netflix.com/tudum/top10)\n\n### First we have Netflix showing the top viewed movies on their streaming service. It looks visually nice when you get to see the movie and the ranking on the top left corner. It is clean, however, when you scroll down to look at why the data looks very strange. We have Kpop Demon Hunters sitting at first followed by the rest of the movies. Next to the titles is a column of weeks and views. All the weeks shown are different most at 1 which is unknown if it is current or of all time. The visual isn't very pleasing either. Just straight forward with no interesting or stand out visuals. Just basic, almost like an excel sheet. It looks clean but maybe something a little more fun to look at could go a long way.\n\nplot(1:20)\nFirst we have Netflix showing the top viewed movies on their streaming service. At first it looks visually nice when you get to see the movie and the ranking on the top left corner. However, when you scroll down to look at why the data looks very strange. We have Kpop Demon Hunters sitting at first followed by the rest of the movies. Next to the titles is a column of weeks and views. All the weeks shown are different most at 1 which is unknown if it is current or of all time. The visual isn’t very pleasing either. Just straight forward with no interesting or stand out visuals. Just basic, almost like an excel sheet. It looks clean but maybe something a little more fun to look at could go a long way."
  },
  {
    "objectID": "w01/First_Quarto_File.html#article-1-top-10-movies-on-netflix-right-nowhttpswww.netflix.comtudumtop10",
    "href": "w01/First_Quarto_File.html#article-1-top-10-movies-on-netflix-right-nowhttpswww.netflix.comtudumtop10",
    "title": "First_Quarto_File",
    "section": "Article 1: Top 10 Movies on Netflix Right Now(https://www.netflix.com/tudum/top10)",
    "text": "Article 1: Top 10 Movies on Netflix Right Now(https://www.netflix.com/tudum/top10)\nNext we have Anychart. Any chart looks like a place just to post what the best looking data visualizations there are. It’s a great website but it uses none of them itself, maybe to avoid bias. Many of the graphs created are very unique but some are a little too busy to look at."
  },
  {
    "objectID": "w01/First_Quarto_File.html#article-2-top-data-visualizations-on-travel-burgers-shootings-and-light",
    "href": "w01/First_Quarto_File.html#article-2-top-data-visualizations-on-travel-burgers-shootings-and-light",
    "title": "First_Quarto_File",
    "section": "Article 2: Top Data Visualizations on Travel, Burgers, Shootings, and Light",
    "text": "Article 2: Top Data Visualizations on Travel, Burgers, Shootings, and Light"
  },
  {
    "objectID": "w01/First_Quarto_File.html#httpswww.anychart.comblog20190809top-data-visualizations-dataviz-weekly",
    "href": "w01/First_Quarto_File.html#httpswww.anychart.comblog20190809top-data-visualizations-dataviz-weekly",
    "title": "First_Quarto_File",
    "section": "(https://www.anychart.com/blog/2019/08/09/top-data-visualizations-dataviz-weekly/)",
    "text": "(https://www.anychart.com/blog/2019/08/09/top-data-visualizations-dataviz-weekly/)\nLastly we have Tastewise which is a website looking at the popularity of a culture’s food. Showing trends somewhere, maybe globally, ingredients and flavors, and some links that don’t work… They used a variety of different graphs and visuals but some look like completely different designs almost as if they don’t go together. Most of the data is not fully understandable because the labels aren’t very strong to explain what it’s showing."
  },
  {
    "objectID": "w01/First_Quarto_File.html#article-3-korean-food-trend-overview",
    "href": "w01/First_Quarto_File.html#article-3-korean-food-trend-overview",
    "title": "First_Quarto_File",
    "section": "Article 3: Korean Food trend overview",
    "text": "Article 3: Korean Food trend overview"
  },
  {
    "objectID": "w01/First_Quarto_File.html#httpstastewise.iofoodtrendskoreandishes",
    "href": "w01/First_Quarto_File.html#httpstastewise.iofoodtrendskoreandishes",
    "title": "First_Quarto_File",
    "section": "(https://tastewise.io/foodtrends/korean#dishes)",
    "text": "(https://tastewise.io/foodtrends/korean#dishes)"
  },
  {
    "objectID": "w03/readme.html",
    "href": "w03/readme.html",
    "title": "DS350_website_template",
    "section": "",
    "text": "What are the top 20 movies and what streaming service offers the most?\nWhat streaming service would be the best for a child?\nWhat streaming service should I buy for the next month that will have the most popular movies?"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DS350 Nuclear Project",
    "section": "",
    "text": "To learn more about Quarto websites visit https://quarto.org/docs/websites.\n\n\nCode\n1 + 1\n\n\n[1] 2"
  },
  {
    "objectID": "project_page.html",
    "href": "project_page.html",
    "title": "project_page",
    "section": "",
    "text": "Here’s an overview …\n\n\nProject 1 Project 2 Project 3"
  },
  {
    "objectID": "case_studies/gapminder1.html",
    "href": "case_studies/gapminder1.html",
    "title": "gapminder1",
    "section": "",
    "text": "Code\nggplot(gapminder2, aes(x = lifeExp, y = gdpPercap, size = pop, color = continent)) + \n  geom_point(alpha = 0.7) +\n  scale_x_continuous(breaks = seq(40, 80, by = 20)) +\n  scale_size_continuous(range = c(1,12), breaks = c(2.5e5, 5e5, 7.5e5, 1e6, 1.25e6)/100) +\n  scale_y_continuous(trans = \"sqrt\", breaks = seq(0, 50000, by = 10000)) +\n  facet_wrap(~ year, nrow=1) +\n  labs(\n    x = \"Life Expectancy\",\n    y = \"GDP per Capita\",\n    size = \"Population (×100k)\",\n    color = \"Continent\") +\n  theme_bw() +\n  theme(\n    legend.position = \"right\",\n#    panel.spacing.x = unit(10, \"cm\")\n  )"
  },
  {
    "objectID": "case_studies.html",
    "href": "case_studies.html",
    "title": "project_page",
    "section": "",
    "text": "Here’s an overview …\n\n\nGapminder Part 1\nGapminder Part 2\nWings to Fly\nCombining Height Files\nWorld Data Investigations\nTake Me Out to the Ballgame\nStocks Tidyquant\nLeaflet Layers\nDistance Between Savior Names, Part 1\nDistance Between Savior Names, Part 1"
  },
  {
    "objectID": "case_studies/gapminder2.html",
    "href": "case_studies/gapminder2.html",
    "title": "gapminder2",
    "section": "",
    "text": "Code\nggplot(gapminder2, aes(x = year, y = gdpPercap, color = continent)) +\n  geom_line(aes(group = country), alpha = 0.3) +\n    geom_point(aes(size = pop / 100000), alpha = 0.5) + \n  geom_line(data = gap_summary, \n            aes(x = year, y = weighted_gdp),\n            linewidth = 0.7,\n            color = \"black\") +\n  geom_point(data = gap_summary,\n             aes(x = year, y = weighted_gdp, size = total_pop / 100000), color = \"black\") +\n  \n#  geom_point(data = gapminder2,\n#             aes(x = year, y = weighted_gdp, size = total_pop / 100000)) +  # dots sized by continent population\n  facet_wrap(~ continent, nrow = 1) +\n  scale_size_continuous(range = c(.5, 3)) +  # adjust range as needed\n  labs(\n    y = \"GDP per Capita\",\n    size = \"Population (100k)\",\n    color = \"Continent\"\n  ) +\n  theme_minimal()+\n  theme(strip.background = element_rect(fill = \"gray90\", color = \"gray50\"),\n        panel.border = element_rect(color = \"black\", fill = NA, linewidth = 0.5)\n)"
  },
  {
    "objectID": "w04/gun_deaths.html",
    "href": "w04/gun_deaths.html",
    "title": "gun_deaths",
    "section": "",
    "text": "The interactive visualization created by Periscopic on U.S. Gun Deaths in 2018 presents an emotional and data-driven exploration of every gun death that year.\nEach line represents a life cut short, with its length showing the number of years of life lost. After exploring the filters for age, gender, race, and intent, several clear patterns emerge.\nMost gun deaths in 2018 were suicides, particularly among middle-aged white males. In contrast, homicides were more common among younger individuals. The visualization also makes it clear that suicides, though often less publicized, far outnumber homicides in the United States.\nThis data tells a story not only about how people die but also about who they were — and what could have been. Understanding these patterns helps us think about how to target prevention and awareness efforts throughout the year."
  },
  {
    "objectID": "w04/gun_deaths.html#research-question",
    "href": "w04/gun_deaths.html#research-question",
    "title": "gun_deaths",
    "section": "Research Question",
    "text": "Research Question\nWhich demographic and intent-based groups experienced the highest numbers of gun deaths in 2018, and how might these patterns vary by season?\nThis question will guide the following visualizations, which aim to help a public-awareness client plan campaigns emphasizing different groups or issues in different seasons."
  },
  {
    "objectID": "w04/gun_deaths.html#data-loading",
    "href": "w04/gun_deaths.html#data-loading",
    "title": "gun_deaths",
    "section": "Data Loading",
    "text": "Data Loading\n\n\nCode\nlibrary(tidyverse)\ngun_data <- read_csv(\"https://raw.githubusercontent.com/fivethirtyeight/guns-data/master/full_data.csv\")\n\n## glimpse(gun_data)\n\n## head(gun_data)\n\n\nSummary: This dataset contains over 33,000 records representing individual gun deaths in 2018. It includes columns such as age, sex, race, month, intent, and geographic information, allowing for analysis of who was affected and how circumstances vary.\nVisualization 1 — Gun Deaths by Intent\nThis first visualization provides a general overview of the types of gun deaths recorded in 2018.\n\n\nCode\ngun_data %>%\ncount(intent) %>%\nggplot(aes(x = reorder(intent, -n), y = n, fill = intent)) +\ngeom_col(show.legend = FALSE) +\nlabs(title = \"Gun Deaths by Intent (2018)\",\nx = \"Intent\",\ny = \"Number of Deaths\") +\ntheme_minimal() +\ntheme(axis.text.x = element_text(angle = 20, hjust = 1))\n\n\n\n\n\nDescription: This plot shows that suicides account for the vast majority of gun deaths in 2018, followed by homicides. This highlights how prevention efforts could focus more on mental health resources and suicide awareness campaigns, especially targeting high-risk groups.\nVisualization 2 — Gun Deaths by Month\nTo address the client’s request for identifying seasonal emphasis areas, this chart examines monthly variations.\n\n\nCode\ngun_data %>%\ncount(month) %>%\nggplot(aes(x = month, y = n, group = 1)) +\ngeom_line(color = \"steelblue\", size = 1) +\ngeom_point(color = \"steelblue\", size = 2) +\nlabs(title = \"Gun Deaths by Month (2018)\",\nx = \"Month\",\ny = \"Number of Deaths\") +\ntheme_minimal()\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\nDescription: There appears to be a slight increase in gun deaths during the summer months (June–August), suggesting that awareness campaigns focusing on safe conflict resolution and stress management might be most effective during this time. Winter months generally show fewer deaths, possibly due to seasonal behaviors or reduced outdoor activity.\nVisualization 3 — Gun Deaths by Age Group\nNext, we group individuals by age to understand which life stages are most affected.\n\n\nCode\ngun_data %>%\nmutate(age_group = cut(age, breaks = c(0, 20, 40, 60, 80, 100),\nlabels = c(\"0–19\", \"20–39\", \"40–59\", \"60–79\", \"80+\"))) %>%\ncount(age_group) %>%\nggplot(aes(x = age_group, y = n, fill = age_group)) +\ngeom_col(show.legend = FALSE) +\nlabs(title = \"Gun Deaths by Age Group (2018)\",\nx = \"Age Group\",\ny = \"Number of Deaths\") +\ntheme_minimal()\n\n\n\n\n\nDescription: Gun deaths are most common among individuals aged 20–39, followed by those aged 40–59. Younger adults appear to be at the highest risk, indicating that outreach efforts and educational campaigns targeting this demographic could have the greatest impact.\nVisualization 4 — Gun Deaths by Gender and Intent\nFinally, this chart explores gender differences in types of gun deaths.\n\n\nCode\ngun_data %>%\ncount(sex, intent) %>%\nggplot(aes(x = sex, y = n, fill = intent)) +\ngeom_col(position = \"dodge\") +\nlabs(title = \"Gun Deaths by Gender and Intent (2018)\",\nx = \"Gender\",\ny = \"Number of Deaths\",\nfill = \"Intent\") +\ntheme_minimal()\n\n\n\n\n\nDescription: Males represent the overwhelming majority of gun deaths, particularly in suicides. Female deaths occur far less frequently but are more likely to be homicides. This suggests that messaging for male audiences could focus on mental health and suicide prevention, while female-focused campaigns could emphasize domestic violence prevention and safety resources.\nConclusion\nThe 2018 data on gun deaths reveal significant trends across intent, age, gender, and season. Most deaths result from suicide, primarily among men aged 20–59. Seasonal patterns indicate slightly higher rates during the summer months, suggesting an opportunity to time prevention campaigns accordingly.\nBy tailoring campaigns to highlight mental health awareness in men, safety in younger adults, and prevention messaging during the summer, organizations can maximize their impact in reducing gun-related deaths in the U.S."
  },
  {
    "objectID": "w10/string_task.html",
    "href": "w10/string_task.html",
    "title": "string_task",
    "section": "",
    "text": "Code\npacman::p_load(tidyverse, stringr, stringi, rio)\n\n\nQuestion 2 (With the randomletters.txt file, pull out every 1700 letter (for example, 1, 1700, 3400, 5100, …) and find the quote that is hidden—the quote ends with a period.)\n\n\nCode\nchars <- readr::read_file('https://byuistats.github.io/M335/data/randomletters.txt')\ninculde_all <- paste(chars, collapse = \"\")\nbreak_string <- str_split(inculde_all, \"\")[[1]]\n\nposition <- c(1,seq(0, length(break_string), by = 1700))\n\nbreak_string[position]\n\n\n [1] \"t\" \"h\" \"e\" \" \" \"p\" \"l\" \"u\" \"r\" \"a\" \"l\" \" \" \"o\" \"f\" \" \" \"a\" \"n\" \"e\" \"c\" \"d\"\n[20] \"o\" \"t\" \"e\" \" \" \"i\" \"s\" \" \" \"n\" \"o\" \"t\" \" \" \"d\" \"a\" \"t\" \"a\" \".\" \"z\" \" \" \"a\"\n[39] \"n\" \"f\" \"r\" \"a\"\n\n\nQuestion 3 (With the randomletters_wnumbers.txt file, find all the numbers hidden, and convert those numbers to letters using the letters order in the alphabet to decipher the message. For example, a 1=a, 2=b,…, 26=z (Hint: the message starts with “experts”).)\n\n\nCode\nchars_num <- readr::read_lines(\"https://byuistats.github.io/M335/data/randomletters_wnumbers.txt\")\n\nnum_words <- str_extract_all(chars_num, \"[:digit:]+\")[[1]]\nnum_words <- as.integer(num_words)\nnums <- letters[num_words]\npaste(nums, collapse = \"\")\n\n\n[1] \"expertsoftenpossessmoredatathanjudgment\"\n\n\nQuestion 4\nWith the randomletters.txt file, remove all the spaces and periods from the string then find the longest sequence of vowels.\n\n\nCode\ncleaned <- str_remove_all(chars, \"[ \\\\.]\")\nvowel_sequences <- str_extract_all(cleaned, \"[aeiouy]+\")[[1]]\nlongest_vowel_sequence <- vowel_sequences[which.max(nchar(vowel_sequences))]\nlongest_vowel_sequence\n\n\n[1] \"ayeyeya\""
  },
  {
    "objectID": "w10/counting_words_task.html",
    "href": "w10/counting_words_task.html",
    "title": "counting_words_task",
    "section": "",
    "text": "Code\npacman::p_load(readr, tidyverse, stringr, stringi, rio, pander, ggplot2)\n\nscriptures <- read_csv(\"https://github.com/beandog/lds-scriptures/raw/master/csv/lds-scriptures.csv\")\n\n\nRows: 41995 Columns: 19\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (13): volume_title, book_title, volume_long_title, book_long_title, volu...\ndbl  (6): volume_id, book_id, chapter_id, verse_id, chapter_number, verse_nu...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\nCode\nbom_nt <- scriptures %>% \n  filter(volume_title %in% c(\"New Testament\", \"Book of Mormon\")) %>% \n  mutate(\n    num_words = str_count(scripture_text, \" \") + 1\n  )\n\nglimpse(bom_nt)\n\n\nRows: 14,561\nColumns: 20\n$ volume_id          <dbl> 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, …\n$ book_id            <dbl> 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40,…\n$ chapter_id         <dbl> 930, 930, 930, 930, 930, 930, 930, 930, 930, 930, 9…\n$ verse_id           <dbl> 23146, 23147, 23148, 23149, 23150, 23151, 23152, 23…\n$ volume_title       <chr> \"New Testament\", \"New Testament\", \"New Testament\", …\n$ book_title         <chr> \"Matthew\", \"Matthew\", \"Matthew\", \"Matthew\", \"Matthe…\n$ volume_long_title  <chr> \"The New Testament\", \"The New Testament\", \"The New …\n$ book_long_title    <chr> \"The Gospel According to St Matthew\", \"The Gospel A…\n$ volume_subtitle    <chr> \"Of our Lord and Saviour Jesus Christ\", \"Of our Lor…\n$ book_subtitle      <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ volume_short_title <chr> \"NT\", \"NT\", \"NT\", \"NT\", \"NT\", \"NT\", \"NT\", \"NT\", \"NT…\n$ book_short_title   <chr> \"Matt.\", \"Matt.\", \"Matt.\", \"Matt.\", \"Matt.\", \"Matt.…\n$ volume_lds_url     <chr> \"nt\", \"nt\", \"nt\", \"nt\", \"nt\", \"nt\", \"nt\", \"nt\", \"nt…\n$ book_lds_url       <chr> \"matt\", \"matt\", \"matt\", \"matt\", \"matt\", \"matt\", \"ma…\n$ chapter_number     <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ verse_number       <dbl> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, …\n$ scripture_text     <chr> \"The book of the generation of Jesus Christ, the so…\n$ verse_title        <chr> \"Matthew 1:1\", \"Matthew 1:2\", \"Matthew 1:3\", \"Matth…\n$ verse_short_title  <chr> \"Matt. 1:1\", \"Matt. 1:2\", \"Matt. 1:3\", \"Matt. 1:4\",…\n$ num_words          <dbl> 16, 14, 16, 12, 16, 21, 12, 12, 12, 12, 16, 14, 12,…\n\n\n\n\n\n\nCode\n# Average verse length\navg_verse_length <- bom_nt %>%\n  group_by(volume_title) %>%\n  summarise(avg_words = mean(num_words, na.rm = TRUE))\n\npander(avg_verse_length) \n\n\n\n\n\n\n\n\n\nvolume_title\navg_words\n\n\n\n\nBook of Mormon\n40.42\n\n\nNew Testament\n22.67\n\n\n\n\n\n\n\n\n\n\nCode\n# Count \"Jesus\" mentions per volume\njesus_count <- bom_nt %>%\n  mutate(jesus_mentions = str_count(scripture_text, regex(\"Jesus\", ignore_case = TRUE))) %>%\n  group_by(volume_title) %>%\n  summarise(total_mentions = sum(jesus_mentions, na.rm = TRUE))\n\njesus_count\n\n\n# A tibble: 2 × 2\n  volume_title   total_mentions\n  <chr>                   <int>\n1 Book of Mormon            184\n2 New Testament             984\n\n\n\n\n\n\n\nCode\nbom <- bom_nt %>%\n  filter(volume_title == \"Book of Mormon\")\n\n# Optional glimpse of word counts per verse\nbom %>% select(book_title, num_words) %>% glimpse()\n\n\nRows: 6,604\nColumns: 2\n$ book_title <chr> \"1 Nephi\", \"1 Nephi\", \"1 Nephi\", \"1 Nephi\", \"1 Nephi\", \"1 N…\n$ num_words  <dbl> 68, 25, 27, 56, 28, 46, 33, 46, 30, 20, 35, 18, 47, 78, 43,…\n\n\n\n\n\n\n\nCode\n# Visualize\nggplot(bom, aes(x = fct_reorder(book_title, num_words, .fun = median), y = num_words)) +\n  geom_boxplot(fill = \"skyblue\") +\n  labs(\n    title = \"Distribution of Verse Lengths in the Book of Mormon\",\n    x = \"Book\",\n    y = \"Number of Words per Verse\"\n  ) +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))"
  },
  {
    "objectID": "Notes/string_notes.html",
    "href": "Notes/string_notes.html",
    "title": "string_notes",
    "section": "",
    "text": "Code\nchars <- readr::read_lines('https://byuistats.github.io/M335/data/randomletters.txt')\n\nchars_num <- readr::read_lines(\"https://byuistats.github.io/M335/data/randomletters_wnumbers.txt\")\n\n\nTask 1:\n\nWhat is the length of the randomletters string? ﻿﻿\nRemove all the ‘e’ and ‘a’ letters and then tell me how long the string is. and ‘a’ letters ﻿﻿\nHow many times is the name ‘jim’ in the string?\n﻿﻿Tell me which character locations have three “a”’s in a row. ﻿﻿\nShow all the sequences with 3 of the same letter in a row. (hardest one?) ﻿﻿\nSplit the string so that instead of a stzing of length 70000, it is a vector of character strings that eacl have length 1.{r} str_length(chars)\n\n\n\nCode\nstr_extract_all(chars, \"(.)\\\\1{2}\")[[1]]\n\n\n [1] \"nnn\" \"zzz\" \"qqq\" \"ccc\" \"jjj\" \"zzz\" \"...\" \"ooo\" \"vvv\" \"lll\" \"vvv\" \"yyy\"\n[13] \"vvv\" \"xxx\" \"fff\" \"vvv\" \"xxx\" \"lll\" \"yyy\" \"nnn\" \"fff\" \"ccc\" \"kkk\" \"zzz\"\n[25] \"nnn\" \"aaa\" \"kkk\" \"hhh\" \"uuu\" \"ccc\" \"bbb\" \"rrr\" \"qqq\" \"nnn\" \"ccc\" \"nnn\"\n[37] \"kkk\" \"ggg\" \"lll\" \"xxx\" \"ddd\" \"...\" \"ooo\" \"...\" \"ggg\" \"uuu\" \"hhh\" \"...\"\n[49] \"lll\" \"fff\" \"yyy\" \"qqq\" \"ttt\" \"www\" \"xxx\" \"ddd\" \"qqq\" \"...\" \"ggg\" \"iii\"\n[61] \"lll\" \"ggg\" \"hhh\" \"vvv\" \"vvv\" \"vvv\" \"yyy\" \"rrr\" \"rrr\" \"mmm\" \"   \" \"mmm\"\n[73] \"hhh\" \"rrr\" \"zzz\" \"www\" \"iii\" \"   \" \"yyy\" \"rrr\" \"ttt\" \"aaa\" \"mmm\""
  },
  {
    "objectID": "task_manager.html",
    "href": "task_manager.html",
    "title": "project_page",
    "section": "",
    "text": "Project 1\n\n\nCounting Words\nStrings and Regex"
  },
  {
    "objectID": "w10/savior_names.html",
    "href": "w10/savior_names.html",
    "title": "savior_names",
    "section": "",
    "text": "Code\npacman::p_load(readr, tidyverse, stringr, stringi, rio, pander, ggplot2, dplyr)\n\nsavior_names <- read_rds(\"https://byuistats.github.io/M335/data/BoM_SaviorNames.rds\")\nglimpse(savior_names)\n\n\nRows: 112\nColumns: 6\n$ Book          <chr> \"Ether\", \"Mosiah\", \"Mormon\", \"3 Nephi\", \"Mosiah\", \"Mosia…\n$ chapter_verse <chr> \"4:7\", \"3:8\", \"9:29\", \"10:10\", \"7:19\", \"15:4\", \"7:27\", \"…\n$ name          <chr> \"the Father of the heavens and of the earth, and all thi…\n$ reference     <chr> \"Ether 4:7\", \"Mosiah 3:8\", \"Mormon 9:29\", \"3 Nephi 10:10…\n$ nchar         <int> 75, 40, 39, 37, 36, 34, 29, 29, 28, 28, 28, 27, 27, 26, …\n$ words         <int> 16, 7, 8, 6, 7, 6, 6, 6, 6, 6, 5, 4, 5, 5, 5, 4, 5, 5, 5…\n\n\nCode\nscriptures <- read_csv(\"https://github.com/beandog/lds-scriptures/raw/master/csv/lds-scriptures.csv\")\n\nbom_nt <- scriptures %>% \n  filter(volume_title %in% c(\"New Testament\", \"Book of Mormon\")) %>% \n  mutate(num_words = str_count(scripture_text, \" \") + 1)\n\n# Only Book of Mormon verses\nbom_text <- bom_nt %>% filter(volume_title == \"Book of Mormon\")"
  },
  {
    "objectID": "w10/savior_names.html#analysis-of-words-between-savior-names",
    "href": "w10/savior_names.html#analysis-of-words-between-savior-names",
    "title": "savior_names",
    "section": "Analysis of Words Between Savior Names",
    "text": "Analysis of Words Between Savior Names\nThe histogram above shows the distribution of the number of words between mentions of Savior names in the Book of Mormon. Most Savior names occur fairly close together, indicating clusters of references in key chapters. There are occasional long stretches without a Savior name, which contribute to the long tail of the distribution. On average, there are 24.7962406 words between mentions, suggesting that readers encounter references to the Savior regularly throughout the text."
  },
  {
    "objectID": "case_studies/wings_to_fly.html",
    "href": "case_studies/wings_to_fly.html",
    "title": "wings_to_fly",
    "section": "",
    "text": "Code\n# glimpse(flights)\n\n\n\n\nCode\nfl_before_noon <- flights %>%\n  filter(!is.na(sched_dep_time), sched_dep_time < 1200)\n\ndep75 <- fl_before_noon %>%\n  filter(!is.na(dep_delay)) %>%\n  group_by(origin, carrier) %>%\n  summarise(\n    n = n(),\n    p75 = quantile(dep_delay, probs = 0.75, na.rm = TRUE),\n    median = median(dep_delay, na.rm = TRUE),\n    mean = mean(dep_delay, na.rm = TRUE),\n    .groups = \"drop\"\n  ) %>%\n  filter(n >= 50) %>%\n  left_join(airlines, by = \"carrier\") %>%\n  arrange(origin, p75)\n\nbest_by_origin <- dep75 %>%\n  group_by(origin) %>%\n  slice_min(order_by = p75, n = 1, with_ties = FALSE) %>%\n  ungroup()\n\n\n# dep75 %>% print(n = 200)\n# best_by_origin\n\ntop_carriers <- fl_before_noon %>%\n  count(origin, carrier) %>%\n  group_by(origin) %>%\n  slice_max(n, n = 8) %>%\n  ungroup() %>%\n  distinct(origin, carrier)\n\nplot_data <- fl_before_noon %>%\n  inner_join(top_carriers, by = c(\"origin\", \"carrier\")) %>%\n  left_join(airlines, by = \"carrier\")\n\n\n\n\nCode\np1 <- ggplot(plot_data, aes(x = fct_reorder(name, dep_delay, .fun = median, .desc = FALSE),\n                            y = dep_delay)) +\n  geom_boxplot(outlier.shape = NA, alpha = 0.6) +\n  geom_jitter(width = 0.25, alpha = 0.12, size = 0.6) +\n  facet_wrap(~origin, scales = \"free_x\") +\n  labs(x = \"Airline\", y = \"Departure delay (minutes)\",\n       title = \"Departure delays for flights scheduled before noon (jitter = individual flights)\") +\n  coord_flip() +\n  theme_minimal()\n\nprint(p1)\n\n\n\n\n\n\n\nCode\np2 <- ggplot(dep75, aes(x = reorder(name, p75), y = p75, fill = origin)) +\n  geom_col() +\n  facet_wrap(~origin, scales = \"free\") +\n  geom_text(aes(label = paste0(\"n=\", n)), hjust = -0.05, size = 3) +\n  labs(x = \"Airline\", y = \"75th percentile of departure delay (minutes)\",\n       title = \"75th percentile of departure delay for flights scheduled before noon\") +\n  coord_flip() +\n  theme_minimal()\n\nprint(p2)\n\n\n\n\n\n\n\nCode\ndelta_flights <- flights %>%\n  filter(carrier == \"DL\") %>%\n  filter(!is.na(arr_delay), !is.na(origin))\n\ndelta_late_summary <- delta_flights %>%\n  group_by(origin) %>%\n  summarise(\n    n = n(),\n    prop_any_delay = mean(arr_delay > 0, na.rm = TRUE),\n    prop_15plus = mean(arr_delay > 15, na.rm = TRUE),\n    avg_delay = mean(arr_delay, na.rm = TRUE),\n    median_delay = median(arr_delay, na.rm = TRUE),\n    p75 = quantile(arr_delay, 0.75, na.rm = TRUE),\n    .groups = \"drop\"\n  ) %>%\n  arrange(prop_any_delay)\n\npander(delta_late_summary)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\norigin\nn\nprop_any_delay\nprop_15plus\navg_delay\nmedian_delay\np75\n\n\n\n\nJFK\n20559\n0.309\n0.1662\n-2.379\n-11\n5\n\n\nLGA\n22804\n0.3655\n0.1926\n3.928\n-7\n9\n\n\nEWR\n4295\n0.4016\n0.2049\n8.78\n-4\n10\n\n\n\n\n\nCode\n# Q2 plots: ECDF (shows individual-flight complexity with rugs) and bar chart of proportions\np3 <- ggplot(delta_flights, aes(x = arr_delay, colour = origin)) +\n  stat_ecdf(size = 1) +\n  geom_jitter(aes(y = 0), height = 0.02, alpha = 0.2, show.legend = FALSE) +\n  labs(x = \"Arrival delay (minutes)\", y = \"ECDF\",\n       title = \"Delta (DL) arrival delay ECDF by origin\",\n       subtitle = \"Rugs show individual flights\") +\n  xlim(-60, 180) +\n  theme_minimal()\n\n\nprint(p3)\n\n\n\n\n\n\n\nCode\np4 <- delta_late_summary %>%\n  pivot_longer(cols = c(prop_any_delay, prop_15plus),\n               names_to = \"threshold\", values_to = \"proportion\") %>%\n  mutate(threshold = recode(threshold,\n                            prop_any_delay = \"Any delay (>0 min)\",\n                            prop_15plus = \"Significant delay (>15 min)\")) %>%\n  ggplot(aes(x = origin, y = proportion, fill = origin)) +\n  geom_col() +\n  geom_text(aes(label = scales::percent(proportion, accuracy = 0.1)), vjust = -0.5) +\n  facet_wrap(~threshold) +\n  labs(y = \"Proportion of flights late\", title = \"Proportion of Delta flights arriving late by origin\") +\n  theme_minimal()\nprint(p4)"
  },
  {
    "objectID": "w05/stock_task.html",
    "href": "w05/stock_task.html",
    "title": "stock_task",
    "section": "",
    "text": "Code\nstock_raw <- read_csv(\n  \"https://raw.githubusercontent.com/byuistats/data/master/Dart_Expert_Dow_6month_anova/Dart_Expert_Dow_6month_anova.csv\"\n) %>%\n  clean_names()\n\n\nRows: 300 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): contest_period, variable\ndbl (1): value\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\n# glimpse(stock_raw)\n\n\n\n\nCode\nstock_tidy <- stock_raw %>%\n  clean_names() %>%\n  mutate(\n    month_end = str_extract(contest_period, \"^[A-Za-z]+\"),\n    year_end = str_extract(contest_period, \"[0-9]{4}\") %>% as.integer()\n  )\n\n# glimpse(stock_tidy)\n\n\n\n\nCode\nstock_tidy <- stock_tidy %>%\n  mutate(\n    month_end = str_sub(month_end, 1, 3),\n    month_end = factor(month_end,\n         levels = c(\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\",\"Jun\",\"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\")))\nwrite_rds(stock_tidy, \"stock_tidy.rds\")\n\ntest_read <- read_rds(\"stock_tidy.rds\")\n# glimpse(test_read)\n\n\n\n\nCode\ndjia_table <- stock_tidy %>%\n  filter(variable == \"DJIA\") %>%\n  select(month_end, year_end, value) %>%\n  pivot_wider(\n    names_from = year_end,\n    values_from = value\n  ) %>%\n  arrange(month_end)   # ensures Jan → Dec in rows\n\n\npander(djia_table)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmonth_end\n1990\n1991\n1992\n1993\n1994\n1995\n1996\n1997\n1998\n\n\n\n\nJan\n2.5\n17.7\n3.6\n7.7\n-6.2\n16\n10.2\n16.2\n15\n\n\nFeb\n11.5\n7.6\n4.2\n3.7\n-5.3\n19.6\n1.3\n20.8\n7.1\n\n\nMar\n-2.3\n4.4\n-0.3\n7.3\n1.5\n15.3\n0.6\n8.3\n-13.1\n\n\nApr\n-9.2\n3.4\n-0.1\n5.2\n4.4\n14\n5.8\n20.2\n-11.8\n\n\nMay\n-8.5\n4.4\n-5\n5.7\n6.9\n8.2\n7.2\n3\nNA\n\n\nJun\n-12.8\n-3.3\n-2.8\n4.9\n-0.3\n13.1\n15.1\n3.8\nNA\n\n\nJul\n-9.3\n6.6\n0.2\n8\n3.6\n9.3\n15.5\n-0.7\nNA\n\n\nAug\n-0.8\n6.5\n-0.8\n11.2\n1.8\n15\n19.6\n-0.3\nNA\n\n\nSep\n11\n8.6\n2.5\n5.5\n3.2\n15.6\n20.1\n10.7\nNA\n\n\nOct\n15.8\n7.2\n9\n1.6\n7.3\n18.4\n9.6\n7.6\nNA\n\n\nNov\n16.2\n10.6\n5.8\n0.5\n12.8\n14.8\n15.3\n22.5\nNA\n\n\nDec\n17.3\n17.6\n6.7\n1.3\n19.5\n9\n13.3\n10.6\nNA"
  },
  {
    "objectID": "case_studies/height_files.html",
    "href": "case_studies/height_files.html",
    "title": "height_flies",
    "section": "",
    "text": "Code\nbavaria_19th <- read_dta(\"data/germanconscr.dta\")\n\nbls <- read_dta(\"data/germanprison.dta\") \n\nsoldiers <- read.dbf(\"../case_studies/data/B6090.DBF\", as.is = TRUE)\n\nDart_Expert <- read_csv(\"../case_studies/data/Dart_Expert.csv\")\n\n# bls_height <- read_csv(\"https://raw.githubusercontent.com/hadley/r4ds/main/data/heights.csv\") %>%\n# mutate(birth_year = 1950) # mid-20th century\n\nwisconsin <- read_sav(\"data/main05022005.sav\")\n\n\n\n\nCode\n# head(soldiers)\n# str(soldiers)\n\n\n\n\nCode\nbavaria_clean <- bavaria_19th %>%\nmutate(\n  birth_year = bdec,\n  height.cm = height,\n  height.in = height / 2.54,\n  study = \"Bavarian Conscripts 19th Century\"\n) %>%\n  select(birth_year, height.cm, height.in, study)\n\n\n\n\nCode\nbls_clean <- bls %>%\nmutate(\n  birth_year = bdec,\n  height.cm = height,\n  height.in = height / 2.54,\n  study = \"Bureau of Labor Statistics\"\n) %>%\n  select(birth_year, height.cm, height.in, study)\n\n\n\n\nCode\nsoldiers_clean <- soldiers %>%\nmutate(\n  birth_year = GEBJZ, \n  height.cm = CMETER,\n  height.in = CMETER / 2.54,\n  study = \"18th Century German Soldiers\"\n) %>%\n  select(birth_year, height.cm, height.in, study)\n\n\n\n\nCode\nwisconsin_clean <- wisconsin %>%\nmutate(\n  birth_year = 18 + DOBY, \n  height.cm = RT216F * 2.54,\n  height.in = RT216F,\n  study = \"Wisconsin Survey\"\n) %>% \n  select(birth_year, height.cm, height.in, study)\n\n\n\n\nCode\nall_data <- bind_rows(\n  bavaria_clean,\n  bls_clean,\n  soldiers_clean,\n  wisconsin_clean\n) %>% \n  filter(!is.na(birth_year)) %>%\n  filter(birth_year > 1700, height.in > 45, height.in < 85)\n\n\n\n\nCode\nggplot(all_data, aes(birth_year, height.in)) +\n  geom_point(alpha = 0.1) +\n  geom_smooth(method = \"gam\", formula = y ~ s(x, k = 3),   # chat gave me this to add in the line\n    se = FALSE,\n    color = \"red\",\n    linewidth = 1.2\n  ) +\n  labs(\n    title = \"Trend in Male Height\",\n    x = \"Birth Year\",\n    y = \"Height (inches)\"\n  ) +\n  theme_minimal(base_size = 14)\n\n\n\n\n\n\nInterpretation and Conclusion\nTo make my final dataset, I pulled together four different height datasets: Bavarian conscripts, German soldiers, BLS prisoners, and the Wisconsin survey. Each dataset used different column names and sometimes different measurement units, so the first thing I did was rename the variables and convert the heights so everything matched. After that, I combined them into one dataset, removed rows with missing birth years, and filtered out impossible heights like 0 inches. I also kept only birth years after 1700 so the graph would focus on the main historical period we care about.\nWhen I compare this project to the earlier height assignment, the stories they tell are really different. The worldwide dataset from before showed a clear pattern that humans have gotten taller over time. But in this dataset, the height trend looks basically flat. That’s because the studies I used only give a few scattered birth years, not a continuous timeline, so all the points show up in clumps. It’s not that the data contradicts the earlier project—it’s just answering a different question using way less complete information.\nSo if someone said “humans are getting taller over time,” based on the data I’d say no they are not. However, the previous global data covered hundreds of years and multiple countries, while this one only has a few groups from specific years. Based on the data here, height doesn’t really change much at all. My conclusion is that people have gotten taller in the big picture, but within the limited birth years in this assignment, there’s basically no height change showing up."
  },
  {
    "objectID": "week_one.html",
    "href": "week_one.html",
    "title": "project_page",
    "section": "",
    "text": "Case Study 1 Case Study 2"
  },
  {
    "objectID": "w05/country_height.html",
    "href": "w05/country_height.html",
    "title": "country_height",
    "section": "",
    "text": "Code\nggplot(heights, aes(x = year_decade, y = height_in, group = country)) +\n  geom_line(data = heights %>% filter(country != \"Germany\"), color = \"grey80\") + # all other countries muted\n  geom_line(data = heights %>% filter(country == \"Germany\"), color = \"red\", size = 1.2) + # Germany bold\n  geom_point(data = heights %>% filter(country == \"Germany\"), color = \"red\", size = 2) +\n  labs(\n    title = \"Average Heights Over Time by Country\",\n    x = \"Year / Decade\",\n    y = \"Height (inches)\"\n  ) +\n  theme_minimal()\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\nBased on this dataset, there is a pretty clear upward trend in average human height over time. Even though the countries start at different baseline heights, most of them show gradual increases across the decades. The pattern is very noticeable when you look at the highlighted Germany line, which steadily rises over the 19th and 20th centuries. So if someone argues that humans have gotten taller over the years, this data definitely supports that idea."
  },
  {
    "objectID": "weeks/week_ten.html",
    "href": "weeks/week_ten.html",
    "title": "week_ten",
    "section": "",
    "text": "Counting Words\nString Task"
  },
  {
    "objectID": "weeks/week_four.html",
    "href": "weeks/week_four.html",
    "title": "project_page",
    "section": "",
    "text": "Country Height\nStock Task"
  },
  {
    "objectID": "weeks/week_eleven.html",
    "href": "weeks/week_eleven.html",
    "title": "project_page",
    "section": "",
    "text": "First Task"
  },
  {
    "objectID": "weeks/week_two.html",
    "href": "weeks/week_two.html",
    "title": "project_page",
    "section": "",
    "text": "First Task"
  },
  {
    "objectID": "weeks/week_six.html",
    "href": "weeks/week_six.html",
    "title": "project_page",
    "section": "",
    "text": "Age of Planes\nWrite Github Issues"
  },
  {
    "objectID": "weeks/week_three.html",
    "href": "weeks/week_three.html",
    "title": "project_page",
    "section": "",
    "text": "Gun Deaths\nWings to Fly"
  },
  {
    "objectID": "weeks/week_seven.html",
    "href": "weeks/week_seven.html",
    "title": "project_page",
    "section": "",
    "text": "About Time\nCar Wash"
  },
  {
    "objectID": "weeks/week_twelve.html",
    "href": "weeks/week_twelve.html",
    "title": "project_page",
    "section": "",
    "text": "First Task"
  },
  {
    "objectID": "weeks/week_nine.html",
    "href": "weeks/week_nine.html",
    "title": "project_page",
    "section": "",
    "text": "Data Search\nMaps Leaflets"
  },
  {
    "objectID": "weeks/week_eight.html",
    "href": "weeks/week_eight.html",
    "title": "project_page",
    "section": "",
    "text": "US Cities\nIdaho Water"
  },
  {
    "objectID": "weeks/week_one.html",
    "href": "weeks/week_one.html",
    "title": "project_page",
    "section": "",
    "text": "First Task"
  },
  {
    "objectID": "weeks/week_five.html",
    "href": "weeks/week_five.html",
    "title": "project_page",
    "section": "",
    "text": "Polish Visualization\nVisualization for Presentation"
  },
  {
    "objectID": "weeks/w03/readme.html",
    "href": "weeks/w03/readme.html",
    "title": "DS350_website_template",
    "section": "",
    "text": "What are the top 20 movies and what streaming service offers the most?\nWhat streaming service would be the best for a child?\nWhat streaming service should I buy for the next month that will have the most popular movies?"
  },
  {
    "objectID": "weeks/w04/gun_deaths.html",
    "href": "weeks/w04/gun_deaths.html",
    "title": "gun_deaths",
    "section": "",
    "text": "The interactive visualization created by Periscopic on U.S. Gun Deaths in 2018 presents an emotional and data-driven exploration of every gun death that year.\nEach line represents a life cut short, with its length showing the number of years of life lost. After exploring the filters for age, gender, race, and intent, several clear patterns emerge.\nMost gun deaths in 2018 were suicides, particularly among middle-aged white males. In contrast, homicides were more common among younger individuals. The visualization also makes it clear that suicides, though often less publicized, far outnumber homicides in the United States.\nThis data tells a story not only about how people die but also about who they were — and what could have been. Understanding these patterns helps us think about how to target prevention and awareness efforts throughout the year."
  },
  {
    "objectID": "weeks/w04/gun_deaths.html#research-question",
    "href": "weeks/w04/gun_deaths.html#research-question",
    "title": "gun_deaths",
    "section": "Research Question",
    "text": "Research Question\nWhich demographic and intent-based groups experienced the highest numbers of gun deaths in 2018, and how might these patterns vary by season?\nThis question will guide the following visualizations, which aim to help a public-awareness client plan campaigns emphasizing different groups or issues in different seasons."
  },
  {
    "objectID": "weeks/w04/gun_deaths.html#data-loading",
    "href": "weeks/w04/gun_deaths.html#data-loading",
    "title": "gun_deaths",
    "section": "Data Loading",
    "text": "Data Loading\n\n\nCode\nlibrary(tidyverse)\ngun_data <- read_csv(\"https://raw.githubusercontent.com/fivethirtyeight/guns-data/master/full_data.csv\")\n\n## glimpse(gun_data)\n\n## head(gun_data)\n\n\nSummary: This dataset contains over 33,000 records representing individual gun deaths in 2018. It includes columns such as age, sex, race, month, intent, and geographic information, allowing for analysis of who was affected and how circumstances vary.\nVisualization 1 — Gun Deaths by Intent\nThis first visualization provides a general overview of the types of gun deaths recorded in 2018.\n\n\nCode\ngun_data %>%\ncount(intent) %>%\nggplot(aes(x = reorder(intent, -n), y = n, fill = intent)) +\ngeom_col(show.legend = FALSE) +\nlabs(title = \"Gun Deaths by Intent (2018)\",\nx = \"Intent\",\ny = \"Number of Deaths\") +\ntheme_minimal() +\ntheme(axis.text.x = element_text(angle = 20, hjust = 1))\n\n\n\n\n\nDescription: This plot shows that suicides account for the vast majority of gun deaths in 2018, followed by homicides. This highlights how prevention efforts could focus more on mental health resources and suicide awareness campaigns, especially targeting high-risk groups.\nVisualization 2 — Gun Deaths by Month\nTo address the client’s request for identifying seasonal emphasis areas, this chart examines monthly variations.\n\n\nCode\ngun_data %>%\ncount(month) %>%\nggplot(aes(x = month, y = n, group = 1)) +\ngeom_line(color = \"steelblue\", size = 1) +\ngeom_point(color = \"steelblue\", size = 2) +\nlabs(title = \"Gun Deaths by Month (2018)\",\nx = \"Month\",\ny = \"Number of Deaths\") +\ntheme_minimal()\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\nDescription: There appears to be a slight increase in gun deaths during the summer months (June–August), suggesting that awareness campaigns focusing on safe conflict resolution and stress management might be most effective during this time. Winter months generally show fewer deaths, possibly due to seasonal behaviors or reduced outdoor activity.\nVisualization 3 — Gun Deaths by Age Group\nNext, we group individuals by age to understand which life stages are most affected.\n\n\nCode\ngun_data %>%\nmutate(age_group = cut(age, breaks = c(0, 20, 40, 60, 80, 100),\nlabels = c(\"0–19\", \"20–39\", \"40–59\", \"60–79\", \"80+\"))) %>%\ncount(age_group) %>%\nggplot(aes(x = age_group, y = n, fill = age_group)) +\ngeom_col(show.legend = FALSE) +\nlabs(title = \"Gun Deaths by Age Group (2018)\",\nx = \"Age Group\",\ny = \"Number of Deaths\") +\ntheme_minimal()\n\n\n\n\n\nDescription: Gun deaths are most common among individuals aged 20–39, followed by those aged 40–59. Younger adults appear to be at the highest risk, indicating that outreach efforts and educational campaigns targeting this demographic could have the greatest impact.\nVisualization 4 — Gun Deaths by Gender and Intent\nFinally, this chart explores gender differences in types of gun deaths.\n\n\nCode\ngun_data %>%\ncount(sex, intent) %>%\nggplot(aes(x = sex, y = n, fill = intent)) +\ngeom_col(position = \"dodge\") +\nlabs(title = \"Gun Deaths by Gender and Intent (2018)\",\nx = \"Gender\",\ny = \"Number of Deaths\",\nfill = \"Intent\") +\ntheme_minimal()\n\n\n\n\n\nDescription: Males represent the overwhelming majority of gun deaths, particularly in suicides. Female deaths occur far less frequently but are more likely to be homicides. This suggests that messaging for male audiences could focus on mental health and suicide prevention, while female-focused campaigns could emphasize domestic violence prevention and safety resources.\nConclusion\nThe 2018 data on gun deaths reveal significant trends across intent, age, gender, and season. Most deaths result from suicide, primarily among men aged 20–59. Seasonal patterns indicate slightly higher rates during the summer months, suggesting an opportunity to time prevention campaigns accordingly.\nBy tailoring campaigns to highlight mental health awareness in men, safety in younger adults, and prevention messaging during the summer, organizations can maximize their impact in reducing gun-related deaths in the U.S."
  },
  {
    "objectID": "weeks/w05/stock_task.html",
    "href": "weeks/w05/stock_task.html",
    "title": "stock_task",
    "section": "",
    "text": "Code\nstock_raw <- read_csv(\n  \"https://raw.githubusercontent.com/byuistats/data/master/Dart_Expert_Dow_6month_anova/Dart_Expert_Dow_6month_anova.csv\"\n) %>%\n  clean_names()\n\n\nRows: 300 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): contest_period, variable\ndbl (1): value\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\n# pander(stock_raw)\n\n\n\n\nCode\nstock_tidy <- stock_raw %>%\n  clean_names() %>%\n  mutate(\n    month_end = str_extract(contest_period, \"^[A-Za-z]+\"),\n    year_end = str_extract(contest_period, \"[0-9]{4}\") %>% as.integer()\n  )\n\npander(stock_tidy)\n\n\n\n\n\n\n\n\n\n\n\n\ncontest_period\nvariable\nvalue\nmonth_end\nyear_end\n\n\n\n\nJanuary-June1990\nPROS\n12.7\nJanuary\n1990\n\n\nFebruary-July1990\nPROS\n26.4\nFebruary\n1990\n\n\nMarch-August1990\nPROS\n2.5\nMarch\n1990\n\n\nApril-September1990\nPROS\n-20\nApril\n1990\n\n\nMay-October1990\nPROS\n-37.8\nMay\n1990\n\n\nJune-November1990\nPROS\n-33.3\nJune\n1990\n\n\nJuly-December1990\nPROS\n-10.2\nJuly\n1990\n\n\nAugust1990-January1991\nPROS\n-20.3\nAugust\n1990\n\n\nSeptember1990-February1991\nPROS\n38.9\nSeptember\n1990\n\n\nOctober1990-March1991\nPROS\n20.2\nOctober\n1990\n\n\nNovember1990-April1991\nPROS\n50.6\nNovember\n1990\n\n\nDecember1990-May1991\nPROS\n66.9\nDecember\n1990\n\n\nJanuary-June1991\nPROS\n7.5\nJanuary\n1991\n\n\nFebruary-July1991\nPROS\n17.5\nFebruary\n1991\n\n\nMarch-August1991\nPROS\n39.6\nMarch\n1991\n\n\nApril-September1991\nPROS\n15.6\nApril\n1991\n\n\nMay-October1991\nPROS\n12.4\nMay\n1991\n\n\nJune-November1991\nPROS\n3\nJune\n1991\n\n\nJuly-December1991\nPROS\n12.3\nJuly\n1991\n\n\nAugust1991-January1992\nPROS\n39.3\nAugust\n1991\n\n\nSeptember1991-February1992\nPROS\n51.2\nSeptember\n1991\n\n\nOctober1991-March1992\nPROS\n25.2\nOctober\n1991\n\n\nNovember1991-April1992\nPROS\n-3.3\nNovember\n1991\n\n\nDecember1991-May1992\nPROS\n7.7\nDecember\n1991\n\n\nJanuary-June1992\nPROS\n-21\nJanuary\n1992\n\n\nFebruary-July1992\nPROS\n-13\nFebruary\n1992\n\n\nMarch-August1992\nPROS\n-2.5\nMarch\n1992\n\n\nApril-September1992\nPROS\n-19.6\nApril\n1992\n\n\nMay-October1992\nPROS\n6.3\nMay\n1992\n\n\nJune-November1992\nPROS\n-5.1\nJune\n1992\n\n\nJuly-December1992\nPROS\n14.1\nJuly\n1992\n\n\nAugust1992-January1993\nPROS\n15.6\nAugust\n1992\n\n\nSeptember1992-February1993\nPROS\n-26.7\nSeptember\n1992\n\n\nOctober1992-March1993\nPROS\n25.2\nOctober\n1992\n\n\nNovember1992-April1993\nPROS\n-13.9\nNovember\n1992\n\n\nDecember1992-May1993\nPROS\n27.9\nDecember\n1992\n\n\nJanuary1993-June1993\nPROS\n-6.6\nJanuary\n1993\n\n\nFebruary1993-July1993\nPROS\n29.1\nFebruary\n1993\n\n\nMarch1993-August1993\nPROS\n0.3\nMarch\n1993\n\n\nApril1993-September1993\nPROS\n2.6\nApril\n1993\n\n\nMay1993-October1993\nPROS\n5\nMay\n1993\n\n\nJune1993-November1993\nPROS\n-7.4\nJune\n1993\n\n\nJuly1993-Dec.1993\nPROS\n2.2\nJuly\n1993\n\n\nAugust1993-January1994\nPROS\n27.8\nAugust\n1993\n\n\nSeptember1993-February1994\nPROS\n3.7\nSeptember\n1993\n\n\nOctober1993-March1994\nPROS\n4.7\nOctober\n1993\n\n\nNovember1993-April1994\nPROS\n5.4\nNovember\n1993\n\n\nDecember1993-May1994\nPROS\n-9.5\nDecember\n1993\n\n\nJanuary-June1994\nPROS\n-13.1\nJanuary\n1994\n\n\nFebruary-July1994\nPROS\n-10\nFebruary\n1994\n\n\nMarch-August1994\nPROS\n28.4\nMarch\n1994\n\n\nApril-September1994\nPROS\n10.6\nApril\n1994\n\n\nMay-October1994\nPROS\n27.2\nMay\n1994\n\n\nJune-November1994\nPROS\n37\nJune\n1994\n\n\nJuly-December1994\nPROS\n-15.8\nJuly\n1994\n\n\nAugust1994-January1995\nPROS\n20.4\nAugust\n1994\n\n\nSeptember1994-Febuary1995\nPROS\n5.4\nSeptember\n1994\n\n\nOctober1994-March1995\nPROS\n-14.8\nOctober\n1994\n\n\nNovember1994-April1995\nPROS\n12.1\nNovember\n1994\n\n\nDecember1994-May1995\nPROS\n10.8\nDecember\n1994\n\n\nJanuary1995-June1995\nPROS\n72.7\nJanuary\n1995\n\n\nFebuary-July1995\nPROS\n30.5\nFebuary\n1995\n\n\nMarch-August1995\nPROS\n26.7\nMarch\n1995\n\n\nApril1995-September1995\nPROS\n75\nApril\n1995\n\n\nMay-October1995\nPROS\n12.6\nMay\n1995\n\n\nJune-November1995\nPROS\n31\nJune\n1995\n\n\nJuly1995-December1995\nPROS\n-11\nJuly\n1995\n\n\nAugust1995-January1996\nPROS\n28.1\nAugust\n1995\n\n\nSeptember1995-February1996\nPROS\n15.1\nSeptember\n1995\n\n\nOctober1995-March1996\nPROS\n1.5\nOctober\n1995\n\n\nNovember1995-April1996\nPROS\n10.8\nNovember\n1995\n\n\nDecember1995-May1996\nPROS\n2\nDecember\n1995\n\n\nJanuary-June1996\nPROS\n-9.2\nJanuary\n1996\n\n\nFebruary-July1996\nPROS\n-8.6\nFebruary\n1996\n\n\nMarch-August1996\nPROS\n31.7\nMarch\n1996\n\n\nApril-September1996\nPROS\n8.7\nApril\n1996\n\n\nMay-October1996\nPROS\n7\nMay\n1996\n\n\nJune-November1996\nPROS\n5.1\nJune\n1996\n\n\nJuly-December1996\nPROS\n41.2\nJuly\n1996\n\n\nAugust1996-January1997\nPROS\n7.7\nAugust\n1996\n\n\nSeptember1996-February1997\nPROS\n47.6\nSeptember\n1996\n\n\nOctober1996-March1997\nPROS\n-10\nOctober\n1996\n\n\nNovember1996-April1997\nPROS\n-13.6\nNovember\n1996\n\n\nDecember1996-May1997\nPROS\n10.5\nDecember\n1996\n\n\nJanuary-June1997\nPROS\n20.2\nJanuary\n1997\n\n\nFebruary-July1997\nPROS\n29.3\nFebruary\n1997\n\n\nMarch-August1997\nPROS\n20.7\nMarch\n1997\n\n\nApril-September1997\nPROS\n50.3\nApril\n1997\n\n\nMay-October1997\nPROS\n38.4\nMay\n1997\n\n\nJune-November1997\nPROS\n-3.5\nJune\n1997\n\n\nJuly-December1997\nPROS\n-14.1\nJuly\n1997\n\n\nAugust1997-January1998\nPROS\n14.3\nAugust\n1997\n\n\nSeptember1997-February1998\nPROS\n10.9\nSeptember\n1997\n\n\nOctober1997-March1998\nPROS\n5.5\nOctober\n1997\n\n\nNovember1997-April1998\nPROS\n17.4\nNovember\n1997\n\n\nDecember1997-May1998\nPROS\n0\nDecember\n1997\n\n\nJanuary-June1998\nPROS\n24.4\nJanuary\n1998\n\n\nFebruary-July1998\nPROS\n39.3\nFebruary\n1998\n\n\nMarch-August1998\nPROS\n-18.8\nMarch\n1998\n\n\nApril-September1998\nPROS\n-20.1\nApril\n1998\n\n\nJanuary-June1990\nDARTS\n0\nJanuary\n1990\n\n\nFebruary-July1990\nDARTS\n1.8\nFebruary\n1990\n\n\nMarch-August1990\nDARTS\n-14.3\nMarch\n1990\n\n\nApril-September1990\nDARTS\n-7.2\nApril\n1990\n\n\nMay-October1990\nDARTS\n-16.3\nMay\n1990\n\n\nJune-November1990\nDARTS\n-27.4\nJune\n1990\n\n\nJuly-December1990\nDARTS\n-22.5\nJuly\n1990\n\n\nAugust1990-January1991\nDARTS\n-37.3\nAugust\n1990\n\n\nSeptember1990-February1991\nDARTS\n-2.5\nSeptember\n1990\n\n\nOctober1990-March1991\nDARTS\n11.2\nOctober\n1990\n\n\nNovember1990-April1991\nDARTS\n72.9\nNovember\n1990\n\n\nDecember1990-May1991\nDARTS\n16.6\nDecember\n1990\n\n\nJanuary-June1991\nDARTS\n28.7\nJanuary\n1991\n\n\nFebruary-July1991\nDARTS\n44.8\nFebruary\n1991\n\n\nMarch-August1991\nDARTS\n71.3\nMarch\n1991\n\n\nApril-September1991\nDARTS\n2.8\nApril\n1991\n\n\nMay-October1991\nDARTS\n38\nMay\n1991\n\n\nJune-November1991\nDARTS\n-23.2\nJune\n1991\n\n\nJuly-December1991\nDARTS\n4.1\nJuly\n1991\n\n\nAugust1991-January1992\nDARTS\n-14\nAugust\n1991\n\n\nSeptember1991-February1992\nDARTS\n11.7\nSeptember\n1991\n\n\nOctober1991-March1992\nDARTS\n1.1\nOctober\n1991\n\n\nNovember1991-April1992\nDARTS\n-3.1\nNovember\n1991\n\n\nDecember1991-May1992\nDARTS\n-1.4\nDecember\n1991\n\n\nJanuary-June1992\nDARTS\n7.7\nJanuary\n1992\n\n\nFebruary-July1992\nDARTS\n15.4\nFebruary\n1992\n\n\nMarch-August1992\nDARTS\n3.6\nMarch\n1992\n\n\nApril-September1992\nDARTS\n5.7\nApril\n1992\n\n\nMay-October1992\nDARTS\n-5.7\nMay\n1992\n\n\nJune-November1992\nDARTS\n6.9\nJune\n1992\n\n\nJuly-December1992\nDARTS\n1.8\nJuly\n1992\n\n\nAugust1992-January1993\nDARTS\n-13.9\nAugust\n1992\n\n\nSeptember1992-February1993\nDARTS\n15.6\nSeptember\n1992\n\n\nOctober1992-March1993\nDARTS\n18.7\nOctober\n1992\n\n\nNovember1992-April1993\nDARTS\n-3.6\nNovember\n1992\n\n\nDecember1992-May1993\nDARTS\n6.6\nDecember\n1992\n\n\nJanuary1993-June1993\nDARTS\n4.7\nJanuary\n1993\n\n\nFebruary1993-July1993\nDARTS\n-43\nFebruary\n1993\n\n\nMarch1993-August1993\nDARTS\n-5.6\nMarch\n1993\n\n\nApril1993-September1993\nDARTS\n-17.7\nApril\n1993\n\n\nMay1993-October1993\nDARTS\n-4.9\nMay\n1993\n\n\nJune1993-November1993\nDARTS\n-21.4\nJune\n1993\n\n\nJuly1993-Dec.1993\nDARTS\n42.2\nJuly\n1993\n\n\nAugust1993-January1994\nDARTS\n18.5\nAugust\n1993\n\n\nSeptember1993-February1994\nDARTS\n1.5\nSeptember\n1993\n\n\nOctober1993-March1994\nDARTS\n-9.2\nOctober\n1993\n\n\nNovember1993-April1994\nDARTS\n-10.5\nNovember\n1993\n\n\nDecember1993-May1994\nDARTS\n1.4\nDecember\n1993\n\n\nJanuary-June1994\nDARTS\n-8.7\nJanuary\n1994\n\n\nFebruary-July1994\nDARTS\n16.9\nFebruary\n1994\n\n\nMarch-August1994\nDARTS\n-4.3\nMarch\n1994\n\n\nApril-September1994\nDARTS\n20.6\nApril\n1994\n\n\nMay-October1994\nDARTS\n10.3\nMay\n1994\n\n\nJune-November1994\nDARTS\n-6.8\nJune\n1994\n\n\nJuly-December1994\nDARTS\n5.3\nJuly\n1994\n\n\nAugust1994-January1995\nDARTS\n2.8\nAugust\n1994\n\n\nSeptember1994-Febuary1995\nDARTS\n11.9\nSeptember\n1994\n\n\nOctober1994-March1995\nDARTS\n3.8\nOctober\n1994\n\n\nNovember1994-April1995\nDARTS\n1.4\nNovember\n1994\n\n\nDecember1994-May1995\nDARTS\n9\nDecember\n1994\n\n\nJanuary1995-June1995\nDARTS\n11.8\nJanuary\n1995\n\n\nFebuary-July1995\nDARTS\n16.5\nFebuary\n1995\n\n\nMarch-August1995\nDARTS\n11.4\nMarch\n1995\n\n\nApril1995-September1995\nDARTS\n3.3\nApril\n1995\n\n\nMay-October1995\nDARTS\n17.6\nMay\n1995\n\n\nJune-November1995\nDARTS\n23.8\nJune\n1995\n\n\nJuly1995-December1995\nDARTS\n18.7\nJuly\n1995\n\n\nAugust1995-January1996\nDARTS\n-2.4\nAugust\n1995\n\n\nSeptember1995-February1996\nDARTS\n25.4\nSeptember\n1995\n\n\nOctober1995-March1996\nDARTS\n50.5\nOctober\n1995\n\n\nNovember1995-April1996\nDARTS\n24.4\nNovember\n1995\n\n\nDecember1995-May1996\nDARTS\n11.5\nDecember\n1995\n\n\nJanuary-June1996\nDARTS\n-5.3\nJanuary\n1996\n\n\nFebruary-July1996\nDARTS\n2.6\nFebruary\n1996\n\n\nMarch-August1996\nDARTS\n-5.7\nMarch\n1996\n\n\nApril-September1996\nDARTS\n7.8\nApril\n1996\n\n\nMay-October1996\nDARTS\n2\nMay\n1996\n\n\nJune-November1996\nDARTS\n6.2\nJune\n1996\n\n\nJuly-December1996\nDARTS\n6.9\nJuly\n1996\n\n\nAugust1996-January1997\nDARTS\n4.7\nAugust\n1996\n\n\nSeptember1996-February1997\nDARTS\n24.6\nSeptember\n1996\n\n\nOctober1996-March1997\nDARTS\n-16.9\nOctober\n1996\n\n\nNovember1996-April1997\nDARTS\n-9.7\nNovember\n1996\n\n\nDecember1996-May1997\nDARTS\n-21.4\nDecember\n1996\n\n\nJanuary-June1997\nDARTS\n18\nJanuary\n1997\n\n\nFebruary-July1997\nDARTS\n-13.9\nFebruary\n1997\n\n\nMarch-August1997\nDARTS\n0.1\nMarch\n1997\n\n\nApril-September1997\nDARTS\n35.6\nApril\n1997\n\n\nMay-October1997\nDARTS\n20.7\nMay\n1997\n\n\nJune-November1997\nDARTS\n6.5\nJune\n1997\n\n\nJuly-December1997\nDARTS\n6.5\nJuly\n1997\n\n\nAugust1997-January1998\nDARTS\n-9\nAugust\n1997\n\n\nSeptember1997-February1998\nDARTS\n-3.3\nSeptember\n1997\n\n\nOctober1997-March1998\nDARTS\n13.3\nOctober\n1997\n\n\nNovember1997-April1998\nDARTS\n-10.5\nNovember\n1997\n\n\nDecember1997-May1998\nDARTS\n28.5\nDecember\n1997\n\n\nJanuary-June1998\nDARTS\n3.2\nJanuary\n1998\n\n\nFebruary-July1998\nDARTS\n-10.1\nFebruary\n1998\n\n\nMarch-August1998\nDARTS\n-20.4\nMarch\n1998\n\n\nApril-September1998\nDARTS\n-34.2\nApril\n1998\n\n\nJanuary-June1990\nDJIA\n2.5\nJanuary\n1990\n\n\nFebruary-July1990\nDJIA\n11.5\nFebruary\n1990\n\n\nMarch-August1990\nDJIA\n-2.3\nMarch\n1990\n\n\nApril-September1990\nDJIA\n-9.2\nApril\n1990\n\n\nMay-October1990\nDJIA\n-8.5\nMay\n1990\n\n\nJune-November1990\nDJIA\n-12.8\nJune\n1990\n\n\nJuly-December1990\nDJIA\n-9.3\nJuly\n1990\n\n\nAugust1990-January1991\nDJIA\n-0.8\nAugust\n1990\n\n\nSeptember1990-February1991\nDJIA\n11\nSeptember\n1990\n\n\nOctober1990-March1991\nDJIA\n15.8\nOctober\n1990\n\n\nNovember1990-April1991\nDJIA\n16.2\nNovember\n1990\n\n\nDecember1990-May1991\nDJIA\n17.3\nDecember\n1990\n\n\nJanuary-June1991\nDJIA\n17.7\nJanuary\n1991\n\n\nFebruary-July1991\nDJIA\n7.6\nFebruary\n1991\n\n\nMarch-August1991\nDJIA\n4.4\nMarch\n1991\n\n\nApril-September1991\nDJIA\n3.4\nApril\n1991\n\n\nMay-October1991\nDJIA\n4.4\nMay\n1991\n\n\nJune-November1991\nDJIA\n-3.3\nJune\n1991\n\n\nJuly-December1991\nDJIA\n6.6\nJuly\n1991\n\n\nAugust1991-January1992\nDJIA\n6.5\nAugust\n1991\n\n\nSeptember1991-February1992\nDJIA\n8.6\nSeptember\n1991\n\n\nOctober1991-March1992\nDJIA\n7.2\nOctober\n1991\n\n\nNovember1991-April1992\nDJIA\n10.6\nNovember\n1991\n\n\nDecember1991-May1992\nDJIA\n17.6\nDecember\n1991\n\n\nJanuary-June1992\nDJIA\n3.6\nJanuary\n1992\n\n\nFebruary-July1992\nDJIA\n4.2\nFebruary\n1992\n\n\nMarch-August1992\nDJIA\n-0.3\nMarch\n1992\n\n\nApril-September1992\nDJIA\n-0.1\nApril\n1992\n\n\nMay-October1992\nDJIA\n-5\nMay\n1992\n\n\nJune-November1992\nDJIA\n-2.8\nJune\n1992\n\n\nJuly-December1992\nDJIA\n0.2\nJuly\n1992\n\n\nAugust1992-January1993\nDJIA\n-0.8\nAugust\n1992\n\n\nSeptember1992-February1993\nDJIA\n2.5\nSeptember\n1992\n\n\nOctober1992-March1993\nDJIA\n9\nOctober\n1992\n\n\nNovember1992-April1993\nDJIA\n5.8\nNovember\n1992\n\n\nDecember1992-May1993\nDJIA\n6.7\nDecember\n1992\n\n\nJanuary1993-June1993\nDJIA\n7.7\nJanuary\n1993\n\n\nFebruary1993-July1993\nDJIA\n3.7\nFebruary\n1993\n\n\nMarch1993-August1993\nDJIA\n7.3\nMarch\n1993\n\n\nApril1993-September1993\nDJIA\n5.2\nApril\n1993\n\n\nMay1993-October1993\nDJIA\n5.7\nMay\n1993\n\n\nJune1993-November1993\nDJIA\n4.9\nJune\n1993\n\n\nJuly1993-Dec.1993\nDJIA\n8\nJuly\n1993\n\n\nAugust1993-January1994\nDJIA\n11.2\nAugust\n1993\n\n\nSeptember1993-February1994\nDJIA\n5.5\nSeptember\n1993\n\n\nOctober1993-March1994\nDJIA\n1.6\nOctober\n1993\n\n\nNovember1993-April1994\nDJIA\n0.5\nNovember\n1993\n\n\nDecember1993-May1994\nDJIA\n1.3\nDecember\n1993\n\n\nJanuary-June1994\nDJIA\n-6.2\nJanuary\n1994\n\n\nFebruary-July1994\nDJIA\n-5.3\nFebruary\n1994\n\n\nMarch-August1994\nDJIA\n1.5\nMarch\n1994\n\n\nApril-September1994\nDJIA\n4.4\nApril\n1994\n\n\nMay-October1994\nDJIA\n6.9\nMay\n1994\n\n\nJune-November1994\nDJIA\n-0.3\nJune\n1994\n\n\nJuly-December1994\nDJIA\n3.6\nJuly\n1994\n\n\nAugust1994-January1995\nDJIA\n1.8\nAugust\n1994\n\n\nSeptember1994-Febuary1995\nDJIA\n3.2\nSeptember\n1994\n\n\nOctober1994-March1995\nDJIA\n7.3\nOctober\n1994\n\n\nNovember1994-April1995\nDJIA\n12.8\nNovember\n1994\n\n\nDecember1994-May1995\nDJIA\n19.5\nDecember\n1994\n\n\nJanuary1995-June1995\nDJIA\n16\nJanuary\n1995\n\n\nFebuary-July1995\nDJIA\n19.6\nFebuary\n1995\n\n\nMarch-August1995\nDJIA\n15.3\nMarch\n1995\n\n\nApril1995-September1995\nDJIA\n14\nApril\n1995\n\n\nMay-October1995\nDJIA\n8.2\nMay\n1995\n\n\nJune-November1995\nDJIA\n13.1\nJune\n1995\n\n\nJuly1995-December1995\nDJIA\n9.3\nJuly\n1995\n\n\nAugust1995-January1996\nDJIA\n15\nAugust\n1995\n\n\nSeptember1995-February1996\nDJIA\n15.6\nSeptember\n1995\n\n\nOctober1995-March1996\nDJIA\n18.4\nOctober\n1995\n\n\nNovember1995-April1996\nDJIA\n14.8\nNovember\n1995\n\n\nDecember1995-May1996\nDJIA\n9\nDecember\n1995\n\n\nJanuary-June1996\nDJIA\n10.2\nJanuary\n1996\n\n\nFebruary-July1996\nDJIA\n1.3\nFebruary\n1996\n\n\nMarch-August1996\nDJIA\n0.6\nMarch\n1996\n\n\nApril-September1996\nDJIA\n5.8\nApril\n1996\n\n\nMay-October1996\nDJIA\n7.2\nMay\n1996\n\n\nJune-November1996\nDJIA\n15.1\nJune\n1996\n\n\nJuly-December1996\nDJIA\n15.5\nJuly\n1996\n\n\nAugust1996-January1997\nDJIA\n19.6\nAugust\n1996\n\n\nSeptember1996-February1997\nDJIA\n20.1\nSeptember\n1996\n\n\nOctober1996-March1997\nDJIA\n9.6\nOctober\n1996\n\n\nNovember1996-April1997\nDJIA\n15.3\nNovember\n1996\n\n\nDecember1996-May1997\nDJIA\n13.3\nDecember\n1996\n\n\nJanuary-June1997\nDJIA\n16.2\nJanuary\n1997\n\n\nFebruary-July1997\nDJIA\n20.8\nFebruary\n1997\n\n\nMarch-August1997\nDJIA\n8.3\nMarch\n1997\n\n\nApril-September1997\nDJIA\n20.2\nApril\n1997\n\n\nMay-October1997\nDJIA\n3\nMay\n1997\n\n\nJune-November1997\nDJIA\n3.8\nJune\n1997\n\n\nJuly-December1997\nDJIA\n-0.7\nJuly\n1997\n\n\nAugust1997-January1998\nDJIA\n-0.3\nAugust\n1997\n\n\nSeptember1997-February1998\nDJIA\n10.7\nSeptember\n1997\n\n\nOctober1997-March1998\nDJIA\n7.6\nOctober\n1997\n\n\nNovember1997-April1998\nDJIA\n22.5\nNovember\n1997\n\n\nDecember1997-May1998\nDJIA\n10.6\nDecember\n1997\n\n\nJanuary-June1998\nDJIA\n15\nJanuary\n1998\n\n\nFebruary-July1998\nDJIA\n7.1\nFebruary\n1998\n\n\nMarch-August1998\nDJIA\n-13.1\nMarch\n1998\n\n\nApril-September1998\nDJIA\n-11.8\nApril\n1998\n\n\n\n\n\n\n\nCode\nstock_tidy <- stock_tidy %>%\n  mutate(\n    month_end = str_sub(month_end, 1, 3),\n    month_end = factor(month_end,\n         levels = c(\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\",\"Jun\",\"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\")))\nwrite_rds(stock_tidy, \"stock_tidy.rds\")\n\ntest_read <- read_rds(\"stock_tidy.rds\")\n# glimpse(test_read)\n\n\n\n\nCode\ndjia_table <- stock_tidy %>%\n  filter(variable == \"DJIA\") %>%\n  select(month_end, year_end, value) %>%\n  pivot_wider(\n    names_from = year_end,\n    values_from = value\n  ) %>%\n  arrange(month_end)   # ensures Jan → Dec in rows\n\n\npander(djia_table)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmonth_end\n1990\n1991\n1992\n1993\n1994\n1995\n1996\n1997\n1998\n\n\n\n\nJan\n2.5\n17.7\n3.6\n7.7\n-6.2\n16\n10.2\n16.2\n15\n\n\nFeb\n11.5\n7.6\n4.2\n3.7\n-5.3\n19.6\n1.3\n20.8\n7.1\n\n\nMar\n-2.3\n4.4\n-0.3\n7.3\n1.5\n15.3\n0.6\n8.3\n-13.1\n\n\nApr\n-9.2\n3.4\n-0.1\n5.2\n4.4\n14\n5.8\n20.2\n-11.8\n\n\nMay\n-8.5\n4.4\n-5\n5.7\n6.9\n8.2\n7.2\n3\nNA\n\n\nJun\n-12.8\n-3.3\n-2.8\n4.9\n-0.3\n13.1\n15.1\n3.8\nNA\n\n\nJul\n-9.3\n6.6\n0.2\n8\n3.6\n9.3\n15.5\n-0.7\nNA\n\n\nAug\n-0.8\n6.5\n-0.8\n11.2\n1.8\n15\n19.6\n-0.3\nNA\n\n\nSep\n11\n8.6\n2.5\n5.5\n3.2\n15.6\n20.1\n10.7\nNA\n\n\nOct\n15.8\n7.2\n9\n1.6\n7.3\n18.4\n9.6\n7.6\nNA\n\n\nNov\n16.2\n10.6\n5.8\n0.5\n12.8\n14.8\n15.3\n22.5\nNA\n\n\nDec\n17.3\n17.6\n6.7\n1.3\n19.5\n9\n13.3\n10.6\nNA"
  },
  {
    "objectID": "weeks/w05/country_height.html",
    "href": "weeks/w05/country_height.html",
    "title": "country_height",
    "section": "",
    "text": "Code\nggplot(heights, aes(x = year_decade, y = height_in, group = country)) +\n  geom_line(data = heights %>% filter(country != \"Germany\"), color = \"grey80\") + # all other countries muted\n  geom_line(data = heights %>% filter(country == \"Germany\"), color = \"red\", size = 1.2) + # Germany bold\n  geom_point(data = heights %>% filter(country == \"Germany\"), color = \"red\", size = 2) +\n  labs(\n    title = \"Average Heights Over Time by Country\",\n    x = \"Year / Decade\",\n    y = \"Height (inches)\"\n  ) +\n  theme_minimal()\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\nBased on this dataset, there is a pretty clear upward trend in average human height over time. Even though the countries start at different baseline heights, most of them show gradual increases across the decades. The pattern is very noticeable when you look at the highlighted Germany line, which steadily rises over the 19th and 20th centuries. So if someone argues that humans have gotten taller over the years, this data definitely supports that idea."
  },
  {
    "objectID": "weeks/w02/Second_Quarto_File.html",
    "href": "weeks/w02/Second_Quarto_File.html",
    "title": "Second_Quarto_File",
    "section": "",
    "text": "Code\n# Working with the Flights Data Set\navg_delay <- flights %>%\n  filter(arr_delay > 0) %>%\n  group_by(origin) %>% \n  summarise(avg_delay_total = mean(arr_delay, na.rm = TRUE)) %>% \n  ungroup()\n\nplot_ly(avg_delay, type='barpolar', r=~avg_delay_total, theta=~origin, color=~origin) %>%  \n  layout(\n    title = \"2013 NYC Flight Average Arrival Delays\",\n    polar = list(\n      radialaxis = list(title = \"Number of Delays\"),\n      angularaxis = list(categoryarray = flights$origin)\n    )\n )\n\n\n\n\n\n\n\nOkay technically this isn’t a pie chart so I should be good… right? Originally I wanted to to do a racial bar chart but I could not find anything on plotly and ggplot didn’t work like I wanted it to. I’d love to learn if there is a way to create it if you have any tips."
  },
  {
    "objectID": "weeks/w10/string_task.html",
    "href": "weeks/w10/string_task.html",
    "title": "string_task",
    "section": "",
    "text": "Code\npacman::p_load(tidyverse, stringr, stringi, rio)\n\n\nQuestion 2 (With the randomletters.txt file, pull out every 1700 letter (for example, 1, 1700, 3400, 5100, …) and find the quote that is hidden—the quote ends with a period.)\n\n\nCode\nchars <- readr::read_file('https://byuistats.github.io/M335/data/randomletters.txt')\ninculde_all <- paste(chars, collapse = \"\")\nbreak_string <- str_split(inculde_all, \"\")[[1]]\n\nposition <- c(1,seq(0, length(break_string), by = 1700))\n\nbreak_string[position]\n\n\n [1] \"t\" \"h\" \"e\" \" \" \"p\" \"l\" \"u\" \"r\" \"a\" \"l\" \" \" \"o\" \"f\" \" \" \"a\" \"n\" \"e\" \"c\" \"d\"\n[20] \"o\" \"t\" \"e\" \" \" \"i\" \"s\" \" \" \"n\" \"o\" \"t\" \" \" \"d\" \"a\" \"t\" \"a\" \".\" \"z\" \" \" \"a\"\n[39] \"n\" \"f\" \"r\" \"a\"\n\n\nQuestion 3 (With the randomletters_wnumbers.txt file, find all the numbers hidden, and convert those numbers to letters using the letters order in the alphabet to decipher the message. For example, a 1=a, 2=b,…, 26=z (Hint: the message starts with “experts”).)\n\n\nCode\nchars_num <- readr::read_lines(\"https://byuistats.github.io/M335/data/randomletters_wnumbers.txt\")\n\nnum_words <- str_extract_all(chars_num, \"[:digit:]+\")[[1]]\nnum_words <- as.integer(num_words)\nnums <- letters[num_words]\npaste(nums, collapse = \"\")\n\n\n[1] \"expertsoftenpossessmoredatathanjudgment\"\n\n\nQuestion 4\nWith the randomletters.txt file, remove all the spaces and periods from the string then find the longest sequence of vowels.\n\n\nCode\ncleaned <- str_remove_all(chars, \"[ \\\\.]\")\nvowel_sequences <- str_extract_all(cleaned, \"[aeiouy]+\")[[1]]\nlongest_vowel_sequence <- vowel_sequences[which.max(nchar(vowel_sequences))]\nlongest_vowel_sequence\n\n\n[1] \"ayeyeya\""
  },
  {
    "objectID": "weeks/w10/counting_words_task.html",
    "href": "weeks/w10/counting_words_task.html",
    "title": "counting_words_task",
    "section": "",
    "text": "Code\npacman::p_load(readr, tidyverse, stringr, stringi, rio, pander, ggplot2)\n\nscriptures <- read_csv(\"https://github.com/beandog/lds-scriptures/raw/master/csv/lds-scriptures.csv\")\n\n\nRows: 41995 Columns: 19\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (13): volume_title, book_title, volume_long_title, book_long_title, volu...\ndbl  (6): volume_id, book_id, chapter_id, verse_id, chapter_number, verse_nu...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\nCode\nbom_nt <- scriptures %>% \n  filter(volume_title %in% c(\"New Testament\", \"Book of Mormon\")) %>% \n  mutate(\n    num_words = str_count(scripture_text, \" \") + 1\n  )\n\nglimpse(bom_nt)\n\n\nRows: 14,561\nColumns: 20\n$ volume_id          <dbl> 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, …\n$ book_id            <dbl> 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40,…\n$ chapter_id         <dbl> 930, 930, 930, 930, 930, 930, 930, 930, 930, 930, 9…\n$ verse_id           <dbl> 23146, 23147, 23148, 23149, 23150, 23151, 23152, 23…\n$ volume_title       <chr> \"New Testament\", \"New Testament\", \"New Testament\", …\n$ book_title         <chr> \"Matthew\", \"Matthew\", \"Matthew\", \"Matthew\", \"Matthe…\n$ volume_long_title  <chr> \"The New Testament\", \"The New Testament\", \"The New …\n$ book_long_title    <chr> \"The Gospel According to St Matthew\", \"The Gospel A…\n$ volume_subtitle    <chr> \"Of our Lord and Saviour Jesus Christ\", \"Of our Lor…\n$ book_subtitle      <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ volume_short_title <chr> \"NT\", \"NT\", \"NT\", \"NT\", \"NT\", \"NT\", \"NT\", \"NT\", \"NT…\n$ book_short_title   <chr> \"Matt.\", \"Matt.\", \"Matt.\", \"Matt.\", \"Matt.\", \"Matt.…\n$ volume_lds_url     <chr> \"nt\", \"nt\", \"nt\", \"nt\", \"nt\", \"nt\", \"nt\", \"nt\", \"nt…\n$ book_lds_url       <chr> \"matt\", \"matt\", \"matt\", \"matt\", \"matt\", \"matt\", \"ma…\n$ chapter_number     <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ verse_number       <dbl> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, …\n$ scripture_text     <chr> \"The book of the generation of Jesus Christ, the so…\n$ verse_title        <chr> \"Matthew 1:1\", \"Matthew 1:2\", \"Matthew 1:3\", \"Matth…\n$ verse_short_title  <chr> \"Matt. 1:1\", \"Matt. 1:2\", \"Matt. 1:3\", \"Matt. 1:4\",…\n$ num_words          <dbl> 16, 14, 16, 12, 16, 21, 12, 12, 12, 12, 16, 14, 12,…\n\n\n\n\n\n\nCode\n# Average verse length\navg_verse_length <- bom_nt %>%\n  group_by(volume_title) %>%\n  summarise(avg_words = mean(num_words, na.rm = TRUE))\n\npander(avg_verse_length) \n\n\n\n\n\n\n\n\n\nvolume_title\navg_words\n\n\n\n\nBook of Mormon\n40.42\n\n\nNew Testament\n22.67\n\n\n\n\n\n\n\n\n\n\nCode\n# Count \"Jesus\" mentions per volume\njesus_count <- bom_nt %>%\n  mutate(jesus_mentions = str_count(scripture_text, regex(\"Jesus\", ignore_case = TRUE))) %>%\n  group_by(volume_title) %>%\n  summarise(total_mentions = sum(jesus_mentions, na.rm = TRUE))\n\njesus_count\n\n\n# A tibble: 2 × 2\n  volume_title   total_mentions\n  <chr>                   <int>\n1 Book of Mormon            184\n2 New Testament             984\n\n\n\n\n\n\n\nCode\nbom <- bom_nt %>%\n  filter(volume_title == \"Book of Mormon\")\n\n# Optional glimpse of word counts per verse\nbom %>% select(book_title, num_words) %>% glimpse()\n\n\nRows: 6,604\nColumns: 2\n$ book_title <chr> \"1 Nephi\", \"1 Nephi\", \"1 Nephi\", \"1 Nephi\", \"1 Nephi\", \"1 N…\n$ num_words  <dbl> 68, 25, 27, 56, 28, 46, 33, 46, 30, 20, 35, 18, 47, 78, 43,…\n\n\n\n\n\n\n\nCode\n# Visualize\nggplot(bom, aes(x = fct_reorder(book_title, num_words, .fun = median), y = num_words)) +\n  geom_boxplot(fill = \"skyblue\") +\n  labs(\n    title = \"Distribution of Verse Lengths in the Book of Mormon\",\n    x = \"Book\",\n    y = \"Number of Words per Verse\"\n  ) +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))"
  },
  {
    "objectID": "weeks/w10/savior_names.html",
    "href": "weeks/w10/savior_names.html",
    "title": "savior_names",
    "section": "",
    "text": "Code\npacman::p_load(readr, tidyverse, stringr, stringi, rio, pander, ggplot2, dplyr)\n\nsavior_names <- read_rds(\"https://byuistats.github.io/M335/data/BoM_SaviorNames.rds\")\nglimpse(savior_names)\n\n\nRows: 112\nColumns: 6\n$ Book          <chr> \"Ether\", \"Mosiah\", \"Mormon\", \"3 Nephi\", \"Mosiah\", \"Mosia…\n$ chapter_verse <chr> \"4:7\", \"3:8\", \"9:29\", \"10:10\", \"7:19\", \"15:4\", \"7:27\", \"…\n$ name          <chr> \"the Father of the heavens and of the earth, and all thi…\n$ reference     <chr> \"Ether 4:7\", \"Mosiah 3:8\", \"Mormon 9:29\", \"3 Nephi 10:10…\n$ nchar         <int> 75, 40, 39, 37, 36, 34, 29, 29, 28, 28, 28, 27, 27, 26, …\n$ words         <int> 16, 7, 8, 6, 7, 6, 6, 6, 6, 6, 5, 4, 5, 5, 5, 4, 5, 5, 5…\n\n\nCode\nscriptures <- read_csv(\"https://github.com/beandog/lds-scriptures/raw/master/csv/lds-scriptures.csv\")\n\nbom_nt <- scriptures %>% \n  filter(volume_title %in% c(\"New Testament\", \"Book of Mormon\")) %>% \n  mutate(num_words = str_count(scripture_text, \" \") + 1)\n\n# Only Book of Mormon verses\nbom_text <- bom_nt %>% filter(volume_title == \"Book of Mormon\")"
  },
  {
    "objectID": "weeks/w10/savior_names.html#analysis-of-words-between-savior-names",
    "href": "weeks/w10/savior_names.html#analysis-of-words-between-savior-names",
    "title": "savior_names",
    "section": "Analysis of Words Between Savior Names",
    "text": "Analysis of Words Between Savior Names\nThe histogram above shows the distribution of the number of words between mentions of Savior names in the Book of Mormon. Most Savior names occur fairly close together, indicating clusters of references in key chapters. There are occasional long stretches without a Savior name, which contribute to the long tail of the distribution. On average, there are 24.7962406 words between mentions, suggesting that readers encounter references to the Savior regularly throughout the text."
  },
  {
    "objectID": "weeks/w01/First_Quarto_File.html",
    "href": "weeks/w01/First_Quarto_File.html",
    "title": "First_Quarto_File",
    "section": "",
    "text": "Code\n## Sourced Articles\n\n### Article 1: Top 10 Movies on Netflix Right Now(https://www.netflix.com/tudum/top10)\n\n### First we have Netflix showing the top viewed movies on their streaming service. It looks visually nice when you get to see the movie and the ranking on the top left corner. It is clean, however, when you scroll down to look at why the data looks very strange. We have Kpop Demon Hunters sitting at first followed by the rest of the movies. Next to the titles is a column of weeks and views. All the weeks shown are different most at 1 which is unknown if it is current or of all time. The visual isn't very pleasing either. Just straight forward with no interesting or stand out visuals. Just basic, almost like an excel sheet. It looks clean but maybe something a little more fun to look at could go a long way.\n\nplot(1:20)\nFirst we have Netflix showing the top viewed movies on their streaming service. At first it looks visually nice when you get to see the movie and the ranking on the top left corner. However, when you scroll down to look at why the data looks very strange. We have Kpop Demon Hunters sitting at first followed by the rest of the movies. Next to the titles is a column of weeks and views. All the weeks shown are different most at 1 which is unknown if it is current or of all time. The visual isn’t very pleasing either. Just straight forward with no interesting or stand out visuals. Just basic, almost like an excel sheet. It looks clean but maybe something a little more fun to look at could go a long way."
  },
  {
    "objectID": "weeks/w01/First_Quarto_File.html#article-1-top-10-movies-on-netflix-right-nowhttpswww.netflix.comtudumtop10",
    "href": "weeks/w01/First_Quarto_File.html#article-1-top-10-movies-on-netflix-right-nowhttpswww.netflix.comtudumtop10",
    "title": "First_Quarto_File",
    "section": "Article 1: Top 10 Movies on Netflix Right Now(https://www.netflix.com/tudum/top10)",
    "text": "Article 1: Top 10 Movies on Netflix Right Now(https://www.netflix.com/tudum/top10)\nNext we have Anychart. Any chart looks like a place just to post what the best looking data visualizations there are. It’s a great website but it uses none of them itself, maybe to avoid bias. Many of the graphs created are very unique but some are a little too busy to look at."
  },
  {
    "objectID": "weeks/w01/First_Quarto_File.html#article-2-top-data-visualizations-on-travel-burgers-shootings-and-light",
    "href": "weeks/w01/First_Quarto_File.html#article-2-top-data-visualizations-on-travel-burgers-shootings-and-light",
    "title": "First_Quarto_File",
    "section": "Article 2: Top Data Visualizations on Travel, Burgers, Shootings, and Light",
    "text": "Article 2: Top Data Visualizations on Travel, Burgers, Shootings, and Light"
  },
  {
    "objectID": "weeks/w01/First_Quarto_File.html#httpswww.anychart.comblog20190809top-data-visualizations-dataviz-weekly",
    "href": "weeks/w01/First_Quarto_File.html#httpswww.anychart.comblog20190809top-data-visualizations-dataviz-weekly",
    "title": "First_Quarto_File",
    "section": "(https://www.anychart.com/blog/2019/08/09/top-data-visualizations-dataviz-weekly/)",
    "text": "(https://www.anychart.com/blog/2019/08/09/top-data-visualizations-dataviz-weekly/)\nLastly we have Tastewise which is a website looking at the popularity of a culture’s food. Showing trends somewhere, maybe globally, ingredients and flavors, and some links that don’t work… They used a variety of different graphs and visuals but some look like completely different designs almost as if they don’t go together. Most of the data is not fully understandable because the labels aren’t very strong to explain what it’s showing."
  },
  {
    "objectID": "weeks/w01/First_Quarto_File.html#article-3-korean-food-trend-overview",
    "href": "weeks/w01/First_Quarto_File.html#article-3-korean-food-trend-overview",
    "title": "First_Quarto_File",
    "section": "Article 3: Korean Food trend overview",
    "text": "Article 3: Korean Food trend overview"
  },
  {
    "objectID": "weeks/w01/First_Quarto_File.html#httpstastewise.iofoodtrendskoreandishes",
    "href": "weeks/w01/First_Quarto_File.html#httpstastewise.iofoodtrendskoreandishes",
    "title": "First_Quarto_File",
    "section": "(https://tastewise.io/foodtrends/korean#dishes)",
    "text": "(https://tastewise.io/foodtrends/korean#dishes)"
  },
  {
    "objectID": "weeks/w05/polish_visualization.html",
    "href": "weeks/w05/polish_visualization.html",
    "title": "polish_visualization",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\ngun_data <- read_csv(\"https://raw.githubusercontent.com/fivethirtyeight/guns-data/master/full_data.csv\")\n\n## glimpse(gun_data)\n\n## head(gun_data)\n\n\n\n\nCode\ngun_data %>%\n  count(intent) %>%\n  ggplot(aes(x = reorder(intent, -n), y = n, fill = intent)) +\n  scale_fill_manual(\n  values = c(\n    \"Suicide\" = \"red\",\n    \"Homicide\" = \"gray80\",\n    \"Accidental\" = \"gray80\",\n    \"Undetermined\" = \"gray80\"\n    )\n  ) + \n  geom_col(show.legend = FALSE) +\n  labs(title = \"Gun Deaths by Intent (2018)\",\n       x = \"Intent\",\n       y = \"Number of Deaths\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 20, hjust = 1))\n\n\n\n\n\n\n\nThis graph makes the story immediately clear: suicide is the leading cause of gun deaths in 2018 by a huge margin. The red bar for suicides dwarfs every other category, with more than 60,000 deaths. Homicide is the second largest category but is still far behind, while accidental and undetermined gun deaths are extremely small in comparison. If we are trying to understand the primary driver behind gun mortality in the U.S., this chart shows that the conversation has to include mental health, crisis intervention, and suicide prevention—not just violent crime.\n\n\nCode\nsuicides_monthly <- gun_data %>%\n  filter(intent == \"Suicide\") %>%\n  count(month)\n\ngun_data %>%\n  count(month) %>%\n  ggplot(aes(month, n, group = 1)) +\n  geom_line(color = \"steelblue\") +\n  geom_point() +\n  geom_line(data = suicides_monthly, aes(month, n), color = \"red\") +\n  geom_point(data = suicides_monthly, aes(month, n), color = \"red\") +\n  labs(\n    title = \"Total Gun Deaths vs. Suicide Deaths by Month\",\n    subtitle = \"Red = suicides; Blue = total gun deaths\",\n    x = \"Month\",\n    y = \"Number of Deaths\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThe monthly trend reinforces the earlier finding. Total gun deaths move up and down slightly throughout the year, peaking in the summer, but suicides stay consistently high every single month. The red suicide line tracks closely underneath the blue total line, showing that suicides make up a massive portion of gun deaths no matter the season. This means suicide prevention should not be seasonal; it’s a year-round issue. Even when total gun deaths drop, suicide deaths remain steady, demonstrating their large and persistent impact.\n\n\nCode\nage_data <- gun_data %>%\n  mutate(age_group = cut(age, breaks = c(0, 20, 40, 60, 80, 100), labels = c(\"0–19\", \"20–39\", \"40–59\", \"60–79\", \"80+\")))\n\ntotal_by_age <- age_data %>% \n  count(age_group)\n\nsuicides_by_age <- age_data %>%\n  filter(intent == \"Suicide\") %>%\n  count(age_group)\n\nggplot() +\n  geom_col(data = total_by_age, aes(x = age_group, y = n), fill = \"gray80\") +\n  geom_col(data = suicides_by_age, aes(x = age_group, y = n), fill = \"red\", alpha = 0.7) +\n  labs(\n    title = \"Gun Deaths by Age Group (2018)\",\n    subtitle = \"Red bars show suicide deaths within each age group\",\n    x = \"Age Group\",\n    y = \"Number of Deaths\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThe age group chart adds important context. The gray bars represent total gun deaths within each age group, while the red sections show how many of those deaths were suicides. Suicide is a major contributor in every age range, especially for adults aged 20-39 and 40-59, where the red portion dominates the bar. Even in older groups, suicide represents a big chunk of deaths. This breakdown shows that suicide is not just a young-person issue or an old-person issue—it affects a wide range of adults and is a major source of gun mortality across most of the lifespan.\n\n\nCode\ngun_data %>%\ncount(sex, intent) %>%\nggplot(aes(x = sex, y = n, fill = intent)) +\ngeom_col(position = \"dodge\") +\nlabs(title = \"Gun Deaths by Gender and Intent (2018)\",\nx = \"Gender\",\ny = \"Number of Deaths\",\nfill = \"Intent\") +\n  scale_fill_manual(\n  values = c(\n    \"Suicide\" = \"red\",\n    \"Homicide\" = \"#6f7c52\",\n    \"Accidental\" = \"#70898b\",\n    \"Undetermined\" = \"#8e7f97\"\n    )\n  ) +\ntheme_minimal()\n\n\n\n\n\n\n\n\nGender patterns reveal another important side of the story. Men experience far more gun deaths than women. Most of those deaths among men are suicides, shown here in red. Women experience fewer gun deaths overall, but the distribution of causes differs: homicides make up a larger share for females, while suicides dominate for males. This suggests that suicide prevention efforts need to be especially targeted toward men, who are at the highest risk of dying by suicide with a firearm.\n\n\n\nIt shows up as the largest category overall, it stays high throughout the year, it accounts for a major share of deaths in nearly every age group, and it is the dominant cause among men.\nThis means that any serious attempt to reduce gun deaths must go beyond crime-focused strategies and address mental health, access to crisis support, firearm storage practices, and suicide prevention at a national level."
  },
  {
    "objectID": "weeks/w05/polish_visualization.html#research-question",
    "href": "weeks/w05/polish_visualization.html#research-question",
    "title": "polish_visualization",
    "section": "Research Question",
    "text": "Research Question\nWhich combinations of intent, age, month, and gender experienced the most gun deaths in the data? The answer helps schedule different prevention messages during the year."
  },
  {
    "objectID": "weeks/w05/polish_visualization.html#data-loading",
    "href": "weeks/w05/polish_visualization.html#data-loading",
    "title": "polish_visualization",
    "section": "Data Loading",
    "text": "Data Loading\n\n\nCode\nlibrary(tidyverse)\n\n# Download FiveThirtyEight gun data\ngun_data <- read_csv(\n  \"https://raw.githubusercontent.com/fivethirtyeight/guns-data/master/full_data.csv\",\n  show_col_types = FALSE\n)\n\nnew_theme <- theme_minimal(base_size = 14) +\n  theme(\n    plot.title = element_text(face = \"bold\"),\n    panel.grid.minor = element_blank(),\n    axis.text.x = element_text(angle = 25, hjust = 1)\n  )\n\n\nThe dataset has more than 33,000 rows with age, gender, race, month, intent, and location fields. That gives us many options to filter or group the statistics."
  },
  {
    "objectID": "weeks/w05/polish_visualization.html#visualization-1-intent-drives-the-story",
    "href": "weeks/w05/polish_visualization.html#visualization-1-intent-drives-the-story",
    "title": "Polished Gun Death Visualizations",
    "section": "Visualization 1 — Intent Drives the Story",
    "text": "Visualization 1 — Intent Drives the Story\nThe first visual establishes scale: intent types and their relative share of deaths.\n\n\nCode\nintent_summary <- gun_focus |>\n  mutate(intent = str_to_sentence(intent)) |>\n  count(intent, name = \"deaths\") |>\n  mutate(intent = fct_reorder(intent, deaths))\n\nggplot(intent_summary, aes(x = intent, y = deaths, fill = intent)) +\n  geom_col(width = 0.65, show.legend = FALSE) +\n  geom_text(\n    aes(label = comma(deaths)),\n    vjust = -0.4,\n    family = \"sans\",\n    size = 3.5\n  ) +\n  scale_y_continuous(labels = comma, expand = expansion(mult = c(0, 0.08))) +\n  scale_fill_manual(values = intent_palette) +\n  labs(\n    title = \"Suicides Dominate U.S. Gun Deaths\",\n    subtitle = glue::glue(\"Counts for {analysis_year} from CDC WONDER extracts\"),\n    x = \"Intent Type\",\n    y = \"Number of Deaths\",\n    caption = \"Source: FiveThirtyEight gun deaths data set\"\n  ) +\n  polished_theme\n\n\n\n\n\nSuicide dwarfs every other intent, accounting for roughly two-thirds of deaths. The subtitle explicitly cites the data year so stakeholders know the context immediately."
  },
  {
    "objectID": "weeks/w05/polish_visualization.html#visualization-2-seasonal-pulse-by-month",
    "href": "weeks/w05/polish_visualization.html#visualization-2-seasonal-pulse-by-month",
    "title": "Polished Gun Death Visualizations",
    "section": "Visualization 2 — Seasonal Pulse by Month",
    "text": "Visualization 2 — Seasonal Pulse by Month\nConference presentations usually want a hook for timing. This panel highlights the months with the highest counts and uses a consistent color language.\n\n\nCode\nmonthly_pattern <- gun_focus |>\n  mutate(\n    month_num = suppressWarnings(as.integer(as.character(month))),\n    month_label = case_when(\n      !is.na(month_num) ~ month.name[pmax(1, pmin(12, month_num))],\n      TRUE ~ str_to_title(month)\n    ),\n    month_label = factor(month_label, levels = month.name)\n  ) |>\n  count(month_label, name = \"deaths\") |>\n  drop_na(month_label) |>\n  arrange(month_label) |>\n  mutate(is_peak = deaths == max(deaths))\n\nggplot(monthly_pattern, aes(x = month_label, y = deaths, group = 1)) +\n  geom_area(fill = \"lightsteelblue1\", alpha = 0.8) +\n  geom_line(color = \"steelblue\", size = 1.1) +\n  geom_point(aes(color = is_peak), size = 3) +\n  scale_color_manual(values = c(\"FALSE\" = \"steelblue\", \"TRUE\" = \"firebrick\"), guide = \"none\") +\n  scale_y_continuous(labels = comma) +\n  labs(\n    title = \"Summer Shows a Measurable Uptick\",\n    subtitle = \"Peak months stand out for scheduling seasonal prevention messaging\",\n    x = \"Month\",\n    y = \"Number of Deaths\"\n  ) +\n  polished_theme +\n  theme(axis.text.x = element_text(angle = 35, hjust = 1))\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\nThe red point calls out the peak month and makes it easy for viewers to recall when planning campaigns."
  },
  {
    "objectID": "weeks/w05/polish_visualization.html#visualization-3-age-bands-emphasize-target-audiences",
    "href": "weeks/w05/polish_visualization.html#visualization-3-age-bands-emphasize-target-audiences",
    "title": "Polished Gun Death Visualizations",
    "section": "Visualization 3 — Age Bands Emphasize Target Audiences",
    "text": "Visualization 3 — Age Bands Emphasize Target Audiences\n\n\nCode\nage_summary <- gun_focus |>\n  filter(!is.na(age) & age < 100) |>\n  mutate(\n    age_band = cut(\n      age,\n      breaks = c(0, 18, 30, 45, 60, 75, 100),\n      labels = c(\"0–17\", \"18–29\", \"30–44\", \"45–59\", \"60–74\", \"75+\"),\n      right = FALSE\n    )\n  ) |>\n  count(age_band, name = \"deaths\") |>\n  mutate(age_band = fct_rev(age_band))\n\nggplot(age_summary, aes(x = deaths, y = age_band, fill = age_band)) +\n  geom_col(width = 0.7, show.legend = FALSE) +\n  geom_text(\n    aes(label = comma(deaths)),\n    hjust = -0.05,\n    size = 3.3\n  ) +\n  scale_x_continuous(labels = comma, expand = expansion(mult = c(0, 0.1))) +\n  scale_fill_manual(values = rep(\"seagreen3\", nrow(age_summary))) +\n  labs(\n    title = \"Young to Middle-Aged Adults Bear the Brunt\",\n    subtitle = \"Ages 18–44 represent the largest blocks of lost life\",\n    x = \"Number of Deaths\",\n    y = \"Age Band\"\n  ) +\n  polished_theme\n\n\n\n\n\nFlipping the axes enables long labels and a callout that the 18–44 brackets are the primary outreach target."
  },
  {
    "objectID": "weeks/w05/polish_visualization.html#visualization-4-gender-and-intent-facets",
    "href": "weeks/w05/polish_visualization.html#visualization-4-gender-and-intent-facets",
    "title": "Polished Gun Death Visualizations",
    "section": "Visualization 4 — Gender and Intent Facets",
    "text": "Visualization 4 — Gender and Intent Facets\n\n\nCode\ngender_intent <- gun_focus |>\n  mutate(intent = str_to_sentence(intent)) |>\n  count(sex, intent, name = \"deaths\") |>\n  group_by(intent) |>\n  mutate(percent = deaths / sum(deaths)) |>\n  ungroup() |>\n  drop_na(sex)\n\nggplot(gender_intent, aes(x = sex, y = percent, fill = sex)) +\n  geom_col(width = 0.55) +\n  facet_wrap(~intent, scales = \"free_y\") +\n  scale_y_continuous(labels = percent_format(accuracy = 1)) +\n  scale_fill_manual(values = sex_palette, guide = \"none\") +\n  labs(\n    title = \"Male Deaths Dominate, Especially for Suicide\",\n    subtitle = \"Facets make it simple to compare how intent splits between men and women\",\n    x = \"Gender\",\n    y = \"Percent within Intent\"\n  ) +\n  polished_theme\n\n\n\n\n\nFaceting separates the intents so that the suicide panel instantly communicates that men drive nearly nine out of ten cases, whereas the homicide panel shows a noticeably higher female share."
  },
  {
    "objectID": "weeks/w05/polish_visualization.html#conclusion",
    "href": "weeks/w05/polish_visualization.html#conclusion",
    "title": "polish_visualization",
    "section": "Conclusion",
    "text": "Conclusion\nThe refreshed graphics stay close to the original assignment but use a consistent minimalist theme to support conference slides. They show that suicides dominate the data, summer months are slightly higher, adults aged 20–59 face the greatest risk, and male suicide prevention should be a major talking point."
  },
  {
    "objectID": "weeks/w05/polish_visualization.html#book-exercises-28.3.1",
    "href": "weeks/w05/polish_visualization.html#book-exercises-28.3.1",
    "title": "polish_visualization",
    "section": "Book Exercises 28.3.1",
    "text": "Book Exercises 28.3.1\nI completed two short annotation exercises from the book section to practice labeling data.\n\nExercise 1\n\n\nCode\nbest_car <- mpg %>% slice_max(hwy, n = 1, with_ties = FALSE)\n\nggplot(mpg, aes(displ, hwy)) +\n  geom_point(alpha = 0.4) +\n  annotate(\n    \"label\",\n    x = best_car$displ,\n    y = best_car$hwy,\n    label = paste(best_car$manufacturer, best_car$model)\n  ) +\n  labs(title = \"Highlighting the Best Highway MPG\") +\n  new_theme\n\n\n\n\n\n\n\nExercise 2\n\n\nCode\nggplot(mpg, aes(displ, hwy)) +\n  geom_point(alpha = 0.4) +\n  annotate(\"rect\", xmin = 5, xmax = 7, ymin = 10, ymax = 20, alpha = 0.1, fill = \"red\") +\n  annotate(\"text\", x = 6, y = 22, label = \"Large engines\\nlow MPG\") +\n  labs(title = \"Marking a Low-Efficiency Zone\") +\n  new_theme\n\n\n\n\n\nThese exercises simply add labels and shaded regions, matching the beginner-level ideas from section 28.3.1."
  },
  {
    "objectID": "weeks/w03/gun_deaths.html",
    "href": "weeks/w03/gun_deaths.html",
    "title": "gun_deaths",
    "section": "",
    "text": "Code\ngun_data %>%\ncount(intent) %>%\nggplot(aes(x = reorder(intent, -n), y = n, fill = intent)) +\ngeom_col(show.legend = FALSE) +\nlabs(title = \"Gun Deaths by Intent (2018)\",\nx = \"Intent\",\ny = \"Number of Deaths\") +\ntheme_minimal() +\ntheme(axis.text.x = element_text(angle = 20, hjust = 1))\n\n\n\n\n\nThis plot shows that suicides account for the vast majority of gun deaths in 2018, followed by homicides. This highlights how prevention efforts could focus more on mental health resources and suicide awareness campaigns, especially targeting high-risk groups.\nTo address the client’s request for identifying seasonal emphasis areas, this chart examines monthly variations.\n\n\nCode\ngun_data %>%\ncount(month) %>%\nggplot(aes(x = month, y = n, group = 1)) +\ngeom_line(color = \"steelblue\", size = 1) +\ngeom_point(color = \"steelblue\", size = 2) +\nlabs(title = \"Gun Deaths by Month (2018)\",\nx = \"Month\",\ny = \"Number of Deaths\") +\ntheme_minimal()\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\nThere appears to be a slight increase in gun deaths during the summer months (June–August), suggesting that awareness campaigns focusing on safe conflict resolution and stress management might be most effective during this time. Winter months generally show fewer deaths, possibly due to seasonal behaviors or reduced outdoor activity.\nNext, we group individuals by age to understand which life stages are most affected.\n\n\nCode\ngun_data %>%\nmutate(age_group = cut(age, breaks = c(0, 20, 40, 60, 80, 100),\nlabels = c(\"0–19\", \"20–39\", \"40–59\", \"60–79\", \"80+\"))) %>%\ncount(age_group) %>%\nggplot(aes(x = age_group, y = n, fill = age_group)) +\ngeom_col(show.legend = FALSE) +\nlabs(title = \"Gun Deaths by Age Group (2018)\",\nx = \"Age Group\",\ny = \"Number of Deaths\") +\ntheme_minimal()\n\n\n\n\n\nGun deaths are most common among individuals aged 20–39, followed by those aged 40–59. Younger adults appear to be at the highest risk, indicating that outreach efforts and educational campaigns targeting this demographic could have the greatest impact.\nFinally, this chart explores gender differences in types of gun deaths.\n\n\nCode\ngun_data %>%\ncount(sex, intent) %>%\nggplot(aes(x = sex, y = n, fill = intent)) +\ngeom_col(position = \"dodge\") +\nlabs(title = \"Gun Deaths by Gender and Intent (2018)\",\nx = \"Gender\",\ny = \"Number of Deaths\",\nfill = \"Intent\") +\ntheme_minimal()\n\n\n\n\n\nMales represent the overwhelming majority of gun deaths, particularly in suicides. Female deaths occur far less frequently but are more likely to be homicides. This suggests that messaging for male audiences could focus on mental health and suicide prevention, while female-focused campaigns could emphasize domestic violence prevention and safety resources."
  },
  {
    "objectID": "weeks/w03/gun_deaths.html#research-question",
    "href": "weeks/w03/gun_deaths.html#research-question",
    "title": "gun_deaths",
    "section": "Research Question",
    "text": "Research Question\nWhich demographic and intent-based groups experienced the highest numbers of gun deaths in 2018, and how might these patterns vary by season?\nThis question will guide the following visualizations, which aim to help a public-awareness client plan campaigns emphasizing different groups or issues in different seasons."
  },
  {
    "objectID": "weeks/w03/gun_deaths.html#data-loading",
    "href": "weeks/w03/gun_deaths.html#data-loading",
    "title": "gun_deaths",
    "section": "Data Loading",
    "text": "Data Loading\n\n\nCode\nlibrary(tidyverse)\ngun_data <- read_csv(\"https://raw.githubusercontent.com/fivethirtyeight/guns-data/master/full_data.csv\")\n\n## glimpse(gun_data)\n\n## head(gun_data)\n\n\nSummary: This dataset contains over 33,000 records representing individual gun deaths in 2018. It includes columns such as age, sex, race, month, intent, and geographic information, allowing for analysis of who was affected and how circumstances vary.\nVisualization 1 — Gun Deaths by Intent\nThis first visualization provides a general overview of the types of gun deaths recorded in 2018.\n\n\nCode\ngun_data %>%\ncount(intent) %>%\nggplot(aes(x = reorder(intent, -n), y = n, fill = intent)) +\ngeom_col(show.legend = FALSE) +\nlabs(title = \"Gun Deaths by Intent (2018)\",\nx = \"Intent\",\ny = \"Number of Deaths\") +\ntheme_minimal() +\ntheme(axis.text.x = element_text(angle = 20, hjust = 1))\n\n\n\n\n\nDescription: This plot shows that suicides account for the vast majority of gun deaths in 2018, followed by homicides. This highlights how prevention efforts could focus more on mental health resources and suicide awareness campaigns, especially targeting high-risk groups.\nVisualization 2 — Gun Deaths by Month\nTo address the client’s request for identifying seasonal emphasis areas, this chart examines monthly variations.\n\n\nCode\ngun_data %>%\ncount(month) %>%\nggplot(aes(x = month, y = n, group = 1)) +\ngeom_line(color = \"steelblue\", size = 1) +\ngeom_point(color = \"steelblue\", size = 2) +\nlabs(title = \"Gun Deaths by Month (2018)\",\nx = \"Month\",\ny = \"Number of Deaths\") +\ntheme_minimal()\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\nDescription: There appears to be a slight increase in gun deaths during the summer months (June–August), suggesting that awareness campaigns focusing on safe conflict resolution and stress management might be most effective during this time. Winter months generally show fewer deaths, possibly due to seasonal behaviors or reduced outdoor activity.\nVisualization 3 — Gun Deaths by Age Group\nNext, we group individuals by age to understand which life stages are most affected.\n\n\nCode\ngun_data %>%\nmutate(age_group = cut(age, breaks = c(0, 20, 40, 60, 80, 100),\nlabels = c(\"0–19\", \"20–39\", \"40–59\", \"60–79\", \"80+\"))) %>%\ncount(age_group) %>%\nggplot(aes(x = age_group, y = n, fill = age_group)) +\ngeom_col(show.legend = FALSE) +\nlabs(title = \"Gun Deaths by Age Group (2018)\",\nx = \"Age Group\",\ny = \"Number of Deaths\") +\ntheme_minimal()\n\n\n\n\n\nDescription: Gun deaths are most common among individuals aged 20–39, followed by those aged 40–59. Younger adults appear to be at the highest risk, indicating that outreach efforts and educational campaigns targeting this demographic could have the greatest impact.\nVisualization 4 — Gun Deaths by Gender and Intent\nFinally, this chart explores gender differences in types of gun deaths.\n\n\nCode\ngun_data %>%\ncount(sex, intent) %>%\nggplot(aes(x = sex, y = n, fill = intent)) +\ngeom_col(position = \"dodge\") +\nlabs(title = \"Gun Deaths by Gender and Intent (2018)\",\nx = \"Gender\",\ny = \"Number of Deaths\",\nfill = \"Intent\") +\ntheme_minimal()\n\n\n\n\n\nDescription: Males represent the overwhelming majority of gun deaths, particularly in suicides. Female deaths occur far less frequently but are more likely to be homicides. This suggests that messaging for male audiences could focus on mental health and suicide prevention, while female-focused campaigns could emphasize domestic violence prevention and safety resources.\nConclusion\nThe 2018 data on gun deaths reveal significant trends across intent, age, gender, and season. Most deaths result from suicide, primarily among men aged 20–59. Seasonal patterns indicate slightly higher rates during the summer months, suggesting an opportunity to time prevention campaigns accordingly.\nBy tailoring campaigns to highlight mental health awareness in men, safety in younger adults, and prevention messaging during the summer, organizations can maximize their impact in reducing gun-related deaths in the U.S."
  },
  {
    "objectID": "weeks/w04/stock_task.html",
    "href": "weeks/w04/stock_task.html",
    "title": "stock_task",
    "section": "",
    "text": "The contestant_period column is not “tidy” we want to create a month_end and a year_end column from the information it contains.\n\n\nCode\nstock <- stock %>% \n  separate(contest_period, into = c(\"month_start\", \"end_part\"), sep = \"-\") %>% \n  mutate(\n    month_end = str_sub(end_part, 1, -5),\n    year_end = str_sub(end_part, -4)\n)\n\n\nSave your “tidy” data as an .rds object. (as an optional challenge, see if you can read in the saved file!)\n\n\nCode\nstock_rsd <- saveRDS(stock, \"stock.rsd\")\nstock_rsd <- readRDS(\"stock.rsd\")\n\nhead(stock_rsd)\n\n\n# A tibble: 6 × 6\n  month_start end_part      variable value month_end year_end\n  <chr>       <chr>         <chr>    <dbl> <chr>     <chr>   \n1 January     June1990      PROS      12.7 June      1990    \n2 February    July1990      PROS      26.4 July      1990    \n3 March       August1990    PROS       2.5 August    1990    \n4 April       September1990 PROS     -20   September 1990    \n5 May         October1990   PROS     -37.8 October   1990    \n6 June        November1990  PROS     -33.3 November  1990    \n\n\nUse code to create a table of the DJIA returns that matches the table shown below (apply pivot_wider() to the data). Pay attention to detail.\n\n\nCode\nstock_order <- stock_rsd %>%\n  mutate(\n    month_end = factor(\n      month_end,\n      levels = c(\"January\", \"February\", \"March\", \"April\", \"May\", \"June\", \"July\", \"August\", \"September\", \"October\", \"November\", \"December\")\n    )\n  ) %>% \n  filter(variable == \"DJIA\") %>% \n  select(month_end, year_end, value) %>%\n  pivot_wider(\n    names_from = year_end,\n    values_from = value\n  ) %>%\n  arrange(month_end) %>%\n  mutate(across(\n    .cols = -month_end,\n    .fns = ~ ifelse(is.na(.x), \"-\", .x)\n  ))\n\npander(stock_order)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmonth_end\n1990\n1991\n1992\n1993\n1994\n1995\n1996\n1997\n1998\n\n\n\n\nJanuary\n-\n-0.8\n6.5\n-0.8\n11.2\n1.8\n15\n19.6\n-0.3\n\n\nFebruary\n-\n11\n8.6\n2.5\n5.5\n-\n15.6\n20.1\n10.7\n\n\nMarch\n-\n15.8\n7.2\n9\n1.6\n7.3\n18.4\n9.6\n7.6\n\n\nApril\n-\n16.2\n10.6\n5.8\n0.5\n12.8\n14.8\n15.3\n22.5\n\n\nMay\n-\n17.3\n17.6\n6.7\n1.3\n19.5\n9\n13.3\n10.6\n\n\nJune\n2.5\n17.7\n3.6\n7.7\n-6.2\n16\n10.2\n16.2\n15\n\n\nJuly\n11.5\n7.6\n4.2\n3.7\n-5.3\n19.6\n1.3\n20.8\n7.1\n\n\nAugust\n-2.3\n4.4\n-0.3\n7.3\n1.5\n15.3\n0.6\n8.3\n-13.1\n\n\nSeptember\n-9.2\n3.4\n-0.1\n5.2\n4.4\n14\n5.8\n20.2\n-11.8\n\n\nOctober\n-8.5\n4.4\n-5\n5.7\n6.9\n8.2\n7.2\n3\n-\n\n\nNovember\n-12.8\n-3.3\n-2.8\n4.9\n-0.3\n13.1\n15.1\n3.8\n-\n\n\nDecember\n-9.3\n6.6\n0.2\n-\n3.6\n9.3\n15.5\n-0.7\n-\n\n\nNA\n-\n-\n-\n8\n-\n3.2\n-\n-\n-"
  },
  {
    "objectID": "weeks/w04/country_height.html",
    "href": "weeks/w04/country_height.html",
    "title": "country_height",
    "section": "",
    "text": "Code\nggplot(heights, aes(x = year_decade, y = height_in, group = country)) +\n  geom_line(data = heights %>% filter(country != \"Germany\"), color = \"grey80\") + \n  geom_line(data = heights %>% filter(country == \"Germany\"), color = \"red\", linewidth = 1.2) + # Germany bold\n  geom_point(data = heights %>% filter(country == \"Germany\"), color = \"red\", size = 2) +\n  labs(\n    title = \"Average Heights Over Time by Country\",\n    x = \"Year / Decade\",\n    y = \"Height (inches)\"\n  ) +\n  theme_minimal() + \n  theme(\n    plot.title = element_text(size = 18, family = \"serif\", face = \"bold\")\n  )\n\n\n\n\n\nBased on this dataset, there is a pretty clear upward trend in average human height over time. Even though the countries start at different baseline heights, most of them show gradual increases across the decades. The pattern is very noticeable when you look at the highlighted Germany line, which steadily rises over the 19th and 20th centuries. So if someone argues that humans have gotten taller over the years, this data definitely supports that idea."
  },
  {
    "objectID": "weeks/w05/polish_visualization.html#visualization-1-gun-deaths-by-intent",
    "href": "weeks/w05/polish_visualization.html#visualization-1-gun-deaths-by-intent",
    "title": "polish_visualization",
    "section": "Visualization 1 — Gun Deaths by Intent",
    "text": "Visualization 1 — Gun Deaths by Intent\n\n\nCode\ngun_data %>%\n  count(intent) %>%\n  ggplot(aes(x = reorder(intent, -n), y = n, fill = intent)) +\n  geom_col(show.legend = FALSE) +\n  labs(\n    title = \"Gun Deaths by Intent\",\n    x = \"Intent\",\n    y = \"Number of Deaths\"\n  ) +\n  new_theme\n\n\n\n\n\nSuicides clearly make up the majority of deaths. The theme simply bolds the title and angles the axis labels so that the bar names are easier to read on a slide."
  },
  {
    "objectID": "weeks/w05/polish_visualization.html#visualization-2-gun-deaths-by-month",
    "href": "weeks/w05/polish_visualization.html#visualization-2-gun-deaths-by-month",
    "title": "polish_visualization",
    "section": "Visualization 2 — Gun Deaths by Month",
    "text": "Visualization 2 — Gun Deaths by Month\n\n\nCode\ngun_data %>%\n  mutate(month = factor(month, levels = month.abb)) %>%\n  count(month) %>%\n  drop_na(month) %>%\n  ggplot(aes(x = month, y = n, group = 1)) +\n  geom_line(color = \"steelblue\") +\n  geom_point(color = \"steelblue\", size = 2) +\n  labs(\n    title = \"Seasonal Pattern of Gun Deaths\",\n    x = \"Month\",\n    y = \"Number of Deaths\"\n  ) +\n  new_theme\n\n\n\n\n\nCounts creep up during the summer months. That small bump is a natural place to schedule awareness campaigns that focus on conflict resolution and safe storage."
  },
  {
    "objectID": "weeks/w05/polish_visualization.html#visualization-3-gun-deaths-by-age-group",
    "href": "weeks/w05/polish_visualization.html#visualization-3-gun-deaths-by-age-group",
    "title": "polish_visualization",
    "section": "Visualization 3 — Gun Deaths by Age Group",
    "text": "Visualization 3 — Gun Deaths by Age Group\n\n\nCode\ngun_data %>%\n  mutate(\n    age_group = cut(\n      age,\n      breaks = c(0, 20, 40, 60, 80, 100),\n      labels = c(\"0–19\", \"20–39\", \"40–59\", \"60–79\", \"80+\"),\n      right = FALSE\n    )\n  ) %>%\n  count(age_group) %>%\n  drop_na(age_group) %>%\n  ggplot(aes(x = age_group, y = n, fill = age_group)) +\n  geom_col(show.legend = FALSE) +\n  labs(\n    title = \"Gun Deaths by Age Group\",\n    x = \"Age Group\",\n    y = \"Number of Deaths\"\n  ) +\n  new_theme\n\n\n\n\n\nPeople between 20 and 59 years old account for most deaths. Highlighting this range makes it simple to describe a priority audience."
  },
  {
    "objectID": "weeks/w05/polish_visualization.html#visualization-4-gun-deaths-by-gender-and-intent",
    "href": "weeks/w05/polish_visualization.html#visualization-4-gun-deaths-by-gender-and-intent",
    "title": "polish_visualization",
    "section": "Visualization 4 — Gun Deaths by Gender and Intent",
    "text": "Visualization 4 — Gun Deaths by Gender and Intent\n\n\nCode\ngun_data %>%\n  count(sex, intent) %>%\n  drop_na(sex) %>%\n  ggplot(aes(x = sex, y = n, fill = intent)) +\n  geom_col(position = \"dodge\") +\n  labs(\n    title = \"Gender Differences by Intent\",\n    x = \"Gender\",\n    y = \"Number of Deaths\",\n    fill = \"Intent\"\n  ) +\n  new_theme\n\n\n\n\n\nMales account for most deaths overall, especially suicides. Females have fewer total deaths but a relatively higher share of homicides. That information helps the client tailor messages for each audience."
  },
  {
    "objectID": "case_studies/savior_names.html",
    "href": "case_studies/savior_names.html",
    "title": "savior_names",
    "section": "",
    "text": "Code\npacman::p_load(readr, tidyverse, stringr, stringi, rio, pander, ggplot2, dplyr)\n\nsavior_names <- read_rds(\"https://byuistats.github.io/M335/data/BoM_SaviorNames.rds\")\nglimpse(savior_names)\n\n\nRows: 112\nColumns: 6\n$ Book          <chr> \"Ether\", \"Mosiah\", \"Mormon\", \"3 Nephi\", \"Mosiah\", \"Mosia…\n$ chapter_verse <chr> \"4:7\", \"3:8\", \"9:29\", \"10:10\", \"7:19\", \"15:4\", \"7:27\", \"…\n$ name          <chr> \"the Father of the heavens and of the earth, and all thi…\n$ reference     <chr> \"Ether 4:7\", \"Mosiah 3:8\", \"Mormon 9:29\", \"3 Nephi 10:10…\n$ nchar         <int> 75, 40, 39, 37, 36, 34, 29, 29, 28, 28, 28, 27, 27, 26, …\n$ words         <int> 16, 7, 8, 6, 7, 6, 6, 6, 6, 6, 5, 4, 5, 5, 5, 4, 5, 5, 5…\n\n\nCode\nscriptures <- read_csv(\"https://github.com/beandog/lds-scriptures/raw/master/csv/lds-scriptures.csv\")\n\nbom_nt <- scriptures %>% \n  filter(volume_title %in% c(\"New Testament\", \"Book of Mormon\")) %>% \n  mutate(num_words = str_count(scripture_text, \" \") + 1)\n\n# Only Book of Mormon verses\nbom_text <- bom_nt %>% filter(volume_title == \"Book of Mormon\")"
  },
  {
    "objectID": "case_studies/savior_names.html#analysis-of-words-between-savior-names",
    "href": "case_studies/savior_names.html#analysis-of-words-between-savior-names",
    "title": "savior_names",
    "section": "Analysis of Words Between Savior Names",
    "text": "Analysis of Words Between Savior Names\nThe histogram above shows the distribution of the number of words between mentions of Savior names in the Book of Mormon. Most Savior names occur fairly close together, indicating clusters of references in key chapters. There are occasional long stretches without a Savior name, which contribute to the long tail of the distribution. On average, there are 24.7962406 words between mentions, suggesting that readers encounter references to the Savior regularly throughout the text."
  },
  {
    "objectID": "weeks/w03/wings_to_fly.html",
    "href": "weeks/w03/wings_to_fly.html",
    "title": "wings_to_fly",
    "section": "",
    "text": "Code\npacman::p_load(nycflights13, ggplot2, pander, tidyverse)\n\nflights_boss <- flights %>% filter(origin %in% c(\"JFK\", \"EWR\", \"LGA\"))\n\n\n\n\nCode\nquantiles <- flights_boss %>% \n  group_by(origin) %>% \n  summarise(delay = quantile(dep_delay, 0.75, na.rm=TRUE))\n\npander(quantiles)\n\n\n\n\n\n\n\n\n\norigin\ndelay\n\n\n\n\nEWR\n15\n\n\nJFK\n10\n\n\nLGA\n7\n\n\n\n\n\nCode\nggplot(quantiles, aes(origin, delay, fill = origin)) +\n  geom_col()+\n  labs(\n    title = \"Airlines by Lowest 75th Percentile of Departure Delay\",\n    y=\"Delay time in Minutes of Departure\",\n    x=\"Airline\"\n  )\n\n\n\n\n\n\n\n\nOverall, LaGuardia (LGA) has the lowest 75th percentile delay at 7 minutes, meaning flights before noon at LGA tend to run smoother compared to JFK and EWR. JFK is next best at 10 minutes, while EWR is the worst of the three with 15 minutes.\nSo, if you want to minimize the chances of getting a “bad” delay before lunch, LGA is the strongest choice.\n\n\n\n\n\nCode\ndelta_data <- flights_boss %>% \n  filter(carrier == \"DL\") %>% \n  group_by(origin) %>% \n  summarise(mean = mean(arr_delay, na.rm=TRUE))\n\npander(delta_data)\n\n\n\n\n\n\n\n\n\norigin\nmean\n\n\n\n\nEWR\n8.78\n\n\nJFK\n-2.379\n\n\nLGA\n3.928\n\n\n\n\n\nCode\nggplot(delta_data, aes(origin, mean, fill=origin)) +\n  geom_col() +\n  labs(\n    title = \"Don't Wanna Be Late? \\n Choose JFK\",\n    y=\"Delay time in Minutes of Arrival\",\n    x=\"Airline\"\n  )\n\n\n\n\n\n\n\n\nAmong the three NYC airports, JFK performs the best for Delta arrivals, with an average arrival time of 2.38 minutes early. LGA averages about 4 minutes late, and EWR is the worst performer at almost 9 minutes late on average.\nSo, if I want to minimize the chances of arriving late on a Delta flight, JFK is the best origin airport.\n\n\n\n\n\nCode\nworst_dest <- flights_boss %>% \n  group_by(dest) %>% \n  summarise(mean = mean(arr_delay, na.rm=TRUE)) %>% \n  filter(mean > 20) %>% \n  arrange(desc(mean))\n\npander(worst_dest)\n\n\n\n\n\n\n\n\n\ndest\nmean\n\n\n\n\nCAE\n41.76\n\n\nTUL\n33.66\n\n\nOKC\n30.62\n\n\nJAC\n28.1\n\n\nTYS\n24.07\n\n\nMSN\n20.2\n\n\nRIC\n20.11\n\n\n\n\n\nCode\nggplot(worst_dest, aes(dest, mean, fill=dest)) +\n  geom_col() +\n  coord_cartesian(ylim = c(20, 40)) +\n  labs(\n    title = \"Worst Arrival Delays for each Destination\",\n    y=\"Delay time in Minutes of Arrival\",\n    x=\"Destination Airport\"\n  ) \n\n\n\n\n\n\n\n\nThe worst-performing destination is CAE (Columbia Metropolitan Airport) with an average arrival delay of almost 42 minutes, which is significantly higher than the others. TUL and OKC also stand out with delays above 30 minutes.\nSo based on average delays, CAE is the destination where you are most likely to arrive late, by a pretty wide margin."
  },
  {
    "objectID": "weeks/w05/visualization_for_presentation.html",
    "href": "weeks/w05/visualization_for_presentation.html",
    "title": "visualization_for_presentation",
    "section": "",
    "text": "Fruit consumption vs. GDP per capita, 2022\nAverage per capita fruit supply, measured in kilograms per year versus gross domestic product (GDP) per capita, adjusted for inflation and differences in living costs between countries.\n\n\nCode\nggplot(fruit, aes(x = gdp, y = kilo, color = regions)) +\n  geom_point(size = 3, shape = 21, stroke = 1, fill = \"white\") +\n  geom_text_repel(aes(label = Entity), size = 3, max.overlaps = 20) +\n    scale_x_log10(labels = scales::dollar_format(prefix = \"$\", big.mark = \",\")) +\n    scale_y_continuous(labels = function(x) paste0(x, \" kg\")) +\n  scale_color_manual(\n    values = c(\n      \"North America\" = \"#a2559c\",\n      \"South America\" = \"#0b847e\",\n      \"Africa\" = \"#4c6a9d\",\n      \"Europe\" = \"#e56e5a\",\n      \"Asia\" = \"#38aabb\",\n      \"Oceania\" = \"#883039\"\n    )\n  ) +\n    labs(\n    title = \"Fruit Supply per Person vs. GDP per Capita\",\n    x = \"GDP per capita (log scale)\",\n    y = \"Fruit supply per person (kg per year)\",\n    color = \"Region\"\n  ) +\n  theme_minimal(base_size = 13) \n\n\nWarning: ggrepel: 132 unlabeled data points (too many overlaps). Consider\nincreasing max.overlaps\n\n\n\n\n\n\n\nCode\ncountries_to_highlight <- c(\"United States\", \"Ecuador\", \"Poland\")\n\nggplot(fruit, aes(x = gdp, y = kilo)) +\n  geom_point(\n    aes(color = ifelse(Entity %in% countries_to_highlight, \"highlight\", \"other\")),\n    size = 3, shape = 21, stroke = 1, fill = \"white\", alpha = 0.6\n  ) +\n  geom_text_repel(\n    data = fruit %>% filter(Entity %in% countries_to_highlight),\n    aes(label = Entity),\n    size = 4,\n    color = \"red\"\n  ) +\n  scale_color_manual(\n    values = c(\"highlight\" = \"red\", \"other\" = \"gray70\"),\n    guide = \"none\"\n  ) +\n  scale_x_log10(labels = scales::dollar_format()) +\n  scale_y_continuous(labels = ~paste0(.x, \" kg\")) +\n  labs(\n    title = \"Highlighting Selected Countries: GDP vs Fruit Supply\",\n    subtitle = \"United States, Ecuador, and Poland shown in red\",\n    x = \"GDP per capita (log scale)\",\n    y = \"Fruit supply per person (kg/year)\"\n  ) +\n  theme_minimal(base_size = 13)\n\n\n\n\n\nIn this task I learned how to use geom_text_repel() from the ggrepel package, which keeps labels from overlapping and makes scatterplots much cleaner. I used it to highlight three countries—United States, Ecuador, and Poland—while fading all the others into gray. This made the comparisons stand out clearly: the U.S. has high GDP but moderate fruit supply, Poland falls in the middle, and Ecuador has low GDP but surprisingly strong fruit supply. Highlighting these specific points helped the graph tell a clearer story than if all the countries were shown equally.\n\n\nCode\ncontinent_to_highlight <- \"Europe\"\n\nggplot(fruit, aes(x = gdp, y = kilo)) +\n  geom_point(\n    aes(color = ifelse(regions == continent_to_highlight, \"highlight\", \"other\")),\n    size = 3, shape = 21, stroke = 1, fill = \"white\", alpha = 0.7\n  ) +\n  geom_text_repel(\n    data = fruit %>% filter(regions == continent_to_highlight),\n    aes(label = Entity),\n    size = 3.5,\n    color = \"#e56e5a\"\n  ) +\n  scale_color_manual(\n    values = c(\"highlight\" = \"#e56e5a\", \"other\" = \"gray80\"),\n    guide = \"none\"\n  ) +\n  scale_x_log10(labels = scales::dollar_format()) +\n  scale_y_continuous(labels = ~paste0(.x, \" kg\")) +\n  labs(\n    title = \"Countries in Europe Highlighted\",\n    subtitle = \"European countries shown in red-orange; others muted\",\n    x = \"GDP per capita (log scale)\",\n    y = \"Fruit supply per person (kg/year)\"\n  ) +\n  theme_minimal(base_size = 13)\n\n\nWarning: ggrepel: 29 unlabeled data points (too many overlaps). Consider\nincreasing max.overlaps\n\n\n\n\n\nThis plot highlights all the countries in Europe while muting the rest of the world. Doing this makes it easy to compare European nations to each other without the distraction of the hundreds of dots in the full dataset. Most European countries cluster together with relatively high GDP and moderately high fruit consumption. Highlighting a single continent helps reveal regional patterns that disappear when every country is shown at the same visual weight."
  },
  {
    "objectID": "case_studies/world_data.html",
    "href": "case_studies/world_data.html",
    "title": "world_data",
    "section": "",
    "text": "Code\nggplot(obesity, aes(calorie, weight, color = regions)) +\n  geom_point(aes(size = population), alpha = 0.8) +\n  geom_text_repel(aes(label = Entity), size = 4, max.overlaps = 20) +\n  scale_size(range = c(2, 12), guide = \"none\") +\n  scale_color_manual(\n    values = c(\n      \"North America\" = \"#a2559c\",\n      \"South America\" = \"#0b847e\",\n      \"Africa\" = \"#4c6a9d\",\n      \"Europe\" = \"#e56e5a\",\n      \"Asia\" = \"#38aabb\",\n      \"Oceania\" = \"#883039\"\n    )\n  ) +\n  scale_x_continuous(\n    breaks = seq(1500, 4000, 500),\n    labels = function(x) paste(x, \"kcal\")\n  ) +\n  scale_y_continuous(\n    labels = scales::percent_format(scale = 1)\n  ) +\n    labs(\n    title = \"Overweight or Obese\",\n    x = \"Caloric supply (kilocalories per day)\",\n    y = \"\",\n    color = \"Region\"\n  ) +\n  theme_minimal(base_size = 13) \n\n\n\n\n\n\nReference Picture\n\n\nCode\nlibrary(magick)\n\nimg <- image_read(\"data/men_overweight_or_obese.jpeg\")\nplot(img)"
  },
  {
    "objectID": "weeks/w06/age_of_planes.html",
    "href": "weeks/w06/age_of_planes.html",
    "title": "age_of_planes",
    "section": "",
    "text": "Code\nggplot(flights_planes, aes(plane_age, dep_delay)) +\n  geom_point(alpha = 0.15, color = \"#2A6F97\") +\n  geom_smooth(color = \"red\", fill = \"#2A6F9730\") +\n  labs(\n    title = \"Departure Delay vs Plane Age\",\n    x = \"Plane Age (years)\",\n    y = \"Departure Delay (minutes)\"\n  ) +\n  theme_minimal(base_size = 14)\n\n\n\n\n\nThere is no strong relationship between plane age and departure delay. The trend line is nearly flat, and planes of all ages experience similar delay patterns. This suggests that departure delays are caused more by airport congestion, scheduling, or weather rather than the age of the airplane.\n\n\nCode\nflights_planes <- flights_planes %>%\n  mutate(\n    age_group = case_when(\n      plane_age <= 10 ~ \"0–10\",\n      plane_age <= 20 ~ \"11–20\",\n      plane_age <= 30 ~ \"21–30\",\n      TRUE ~ \">30\"\n    )\n  )\nflights_planes <- flights_planes %>%\n  left_join(airlines, by = \"carrier\")\n\n\n\n\nCode\nggplot(flights_planes, aes(name, plane_age)) +\n  geom_boxplot(fill = \"#2A6F97\", color = \"black\", alpha = 0.5) +\n  coord_flip() +\n  labs(\n    title = \"Distribution of Plane Age by Airline\",\n    x = \"Airline\",\n    y = \"Plane Age (years)\"\n  ) +\n  theme_minimal(base_size = 14)\n\n\n\n\n\nDifferent airlines operate fleets of different ages. Some carriers, such as Virgin America or JetBlue, have newer fleets, while others, like American and Delta, operate a wider range of aircraft ages, including some older models. This reflects differences in fleet renewal strategies and the variety of aircraft used by each airline. Even within a single airline, there is a mix of new and older planes.\n\n\nCode\nggplot(flights_planes, aes(plane_age, arr_delay)) +\n  geom_point(alpha = 0.15, color = \"#2A6F97\") +\n  geom_smooth(color = \"red\", fill = \"#2A6F9730\") +\n  labs(\n    title = \"Arrival Delay vs Plane Age\",\n    x = \"Plane Age (years)\",\n    y = \"Arrival Delay (minutes)\"\n  ) +\n  theme_minimal(base_size = 14)\n\n\n\n\n\nThe arrival delay plot looks nearly identical to the departure delay plot. This makes sense because arrival delays are highly influenced by departure delays. The trend remains essentially flat, and planes of all ages experience similar delay patterns. There is no strong evidence that older planes arrive later or earlier than newer planes. This suggests that arrival delays are primarily driven by operational factors (airport congestion, weather, scheduling, departure delay) rather than aircraft age."
  },
  {
    "objectID": "case_studies/ballgame.html",
    "href": "case_studies/ballgame.html",
    "title": "ballgame",
    "section": "",
    "text": "Code\n# data(package = \"Lahman\")\n\nbaseball <- CollegePlaying %>%\n  left_join(Schools, by = \"schoolID\") %>% \n  left_join(\n    People %>% \n      select(playerID, nameFirst, nameLast), by = \"playerID\") %>% \n  left_join(Salaries, by = \"playerID\")\n\najdusted_baseball <- baseball %>%\n  filter(state == \"UT\") %>% \n  mutate(\n    salary_2021 = adjust_for_inflation(\n      price = salary,\n      from_date = yearID.y,\n      country = \"US\",\n      to_date = 2021\n    )\n  )\n\n\nGenerating URL to request all 296 results\nRetrieving inflation data for US \nGenerating URL to request all 65 results\n\n\nCode\nutah_players <- ajdusted_baseball %>%\n  left_join(Batting %>% group_by(playerID) %>% \n              summarize(batting_total = sum(H, na.rm = TRUE)), by = \"playerID\") %>%\n  left_join(Pitching %>% group_by(playerID) %>% \n              summarize(pitching_total = sum(SO, na.rm = TRUE)), by = \"playerID\") %>%\n  left_join(Fielding %>% group_by(playerID) %>% \n              summarize(fielding_total = sum(PO + A, na.rm = TRUE)), by = \"playerID\")\n\n\n\n\nCode\nschool_stats <- utah_players %>%\n  group_by(name_full) %>%\n  summarize(\n    batting = mean(batting_total, na.rm = TRUE),\n    pitching = mean(pitching_total, na.rm = TRUE),\n    fielding = mean(fielding_total, na.rm = TRUE)\n  ) %>%\n  pivot_longer(cols = c(batting, pitching, fielding),\n               names_to = \"stat\",\n               values_to = \"value\")\n\n\n\n\nCode\nggplot(school_stats, aes(x = value, y = name_full, fill = stat)) +\n  geom_col(position = \"stack\") +\n  coord_flip() +\n  labs(\n    title = \"Overall Batting, Pitching, and Fielding Strength by Utah College\",\n    x = \"College\",\n    y = \"Total Performance\",\n    fill = \"Stat Type\"\n  ) +\n  scale_fill_manual(values = c(\n    batting = \"#2A6F97\",\n    pitching = \"#a2559c\",\n    fielding = \"#e56e5a\"\n  )) +\n  theme(\n    axis.text.x = element_text(angle = 30, hjust = 1)\n  )\n\n\n\n\n\n\nInterpretation\nWhen I looked at the average batting, pitching, and fielding stats for each Utah school, BYU clearly stood out. Even when you average the performance per player (so BYU isn’t getting credit just for having more MLB guys), the BYU players still come out ahead in basically every category. Some of the other schools have a few solid players, but overall BYU is producing the most well-rounded MLB athletes. It’s actually pretty cool to see how consistent that pattern is across all three stats.\n\n\nCode\nbubble_data <- utah_players %>%\n  filter(!is.na(salary_2021), salary_2021 > 0) %>%     # keep players with salary data\n  mutate(\n    total_performance = batting_total + pitching_total + fielding_total\n  ) %>%\n  group_by(playerID, name_full) %>%                      # one row per player\n  summarize(\n    avg_total_perf = mean(total_performance, na.rm = TRUE),\n    avg_salary_2021 = mean(salary_2021, na.rm = TRUE)\n  )\n\n\n\n\nCode\nschool_colors <- c(\n  \"Brigham Young University\" = \"#022e59\",   # BYU blue\n  \"College of Eastern Utah\" = \"#DAA520\",\n  \"Dixie State College of Utah\" = \"#228B22\",\n  \"Salt Lake Community College\" = \"#38aabb\",\n  \"Snow College\" = \"#87CEEB\",\n  \"Southern Utah University\" = \"#e56e5a\",\n  \"University of Utah\" = \"#883039\",\n  \"Utah Valley State University\" = \"#a2559c\"\n)\n\nggplot(bubble_data, aes(\n  x = name_full,\n  y = avg_total_perf,\n  size = avg_salary_2021,\n  color = name_full\n)) +\n  geom_point(alpha = 0.65) +\n  scale_size_continuous(labels = scales::dollar) +\n  labs(\n    title = \"MLB Player Performance vs Salary by Utah College\",\n    subtitle = \"Bubble size represents average career salary (2021 dollars)\",\n    x = \"College\",\n    y = \"Average MLB Player Performance\\n(Batting + Pitching + Fielding)\",\n    size = \"Avg Salary (2021 USD)\",\n    color = \"College\"\n  ) + scale_color_manual(values = school_colors) +\n  coord_flip() +\n  theme(\n    axis.text.y = element_text(angle = 30, hjust = 1)\n  )\n\n\n\n\n\nThe bubble chart tells a similar story but from a different angle. Schools with higher-performing players also tend to have higher average salaries, and BYU players are again clustered toward the top with some of the bigger salary bubbles. A lot of the smaller schools barely register because they’ve only had one or two MLB players, and those players didn’t stick around long. BYU not only sends more players to the pros, but their players typically perform better and end up earning more. So overall, both graphs point to the same conclusion, BYU is easily the strongest, MLB pipeline out of all the Utah colleges."
  },
  {
    "objectID": "weeks/w07/about_time.html",
    "href": "weeks/w07/about_time.html",
    "title": "about_time",
    "section": "",
    "text": "Code\nclean_time <- about_time %>%\n  mutate(\n    datetime_utc   = ymd_hms(Time, tz = \"UTC\"),\n    datetime_local = with_tz(datetime_utc, \"America/Boise\")\n  ) %>% \n  filter(Amount > 0)\n\n\n\n\nCode\nclean_day <- clean_time %>%\n  mutate(day = date(datetime_local)) %>%\n  group_by(Name, day) %>%\n  summarise(\n    revenue = sum(Amount),\n    transactions = n(),\n    .groups = \"drop\"\n  )\n\nclean_weekly <- clean_time %>%\n  mutate(week = floor_date(datetime_local, \"week\")) %>%\n  group_by(Name, week) %>%\n  summarise(\n    revenue = sum(Amount),\n    transactions = n(),\n    .groups = \"drop\"\n  )\n\nclean_monthly <- clean_time %>%\n  mutate(month = floor_date(datetime_local, \"month\")) %>%\n  group_by(Name, month) %>%\n  summarise(\n    revenue = sum(Amount),\n    transactions = n(),\n    .groups = \"drop\"\n  )\n\n\n\n\nCode\nggplot(clean_day, aes(day, revenue, color = Name)) +\n  geom_line() +\n  labs(title = \"Daily Revenue by Company\", x = \"Date\", y = \"Revenue\") +\n  theme_minimal()\n\n\n\n\n\n\n\nCode\nggplot(clean_weekly, aes(week, revenue, color = Name)) +\n  geom_line(size = 1.1) +\n  labs(title = \"Weekly Revenue by Company\", x = \"Week\", y = \"Revenue\") +\n  theme_minimal()\n\n\n\n\n\n\n\nCode\nggplot(clean_monthly, aes(month, revenue, color = Name)) +\n  geom_line(size = 1.2) +\n  labs(title = \"Monthly Revenue by Company\", x = \"Month\", y = \"Revenue\") +\n  theme_minimal()\n\n\n\n\n\n\nCustomer Traffic\n\n\nCode\nggplot(clean_day, aes(day, transactions, color = Name)) +\n  geom_line(size = 1) +\n  labs(title = \"Daily Customer Traffic\", y = \"# Transactions\") +\n  theme_minimal()\n\n\n\n\n\n\n\nHours of Operation\n\n\nCode\nhourly_sales <- clean_time %>%\n  mutate(hour = hour(datetime_local)) %>%\n  group_by(Name, hour) %>%\n  summarise(transactions = n(), .groups = \"drop\")\n\nggplot(hourly_sales, aes(hour, transactions, color = Name)) +\n  geom_line(size = 1.1) +\n  scale_x_continuous(breaks = 0:23) +\n  labs(title = \"Business Hours & Customer Activity\", x = \"Hour of Day\") +\n  theme_minimal()\n\n\n\n\n\nEven though LeBelle doesn’t have the highest customer traffic, they’re showing the clearest upward trend in revenue across the three months. Their growth is strong and consistent, with June and July both outperforming earlier months. Because they’re expanding quickly and bringing in more money over time, I’d choose LeBelle for the loan and witht that they can get more traction and attention."
  },
  {
    "objectID": "case_studies/stocks_tidyquant.html",
    "href": "case_studies/stocks_tidyquant.html",
    "title": "stocks_tidyquant",
    "section": "",
    "text": "Code\nstart_date <- ymd(\"2023-9-01\") # the start of my mission\nend_date   <- today()\n\nprices <- tq_get(stocks, from = start_date, to = end_date) %>%\n  select(symbol, date, adjusted)\n\n\n\n\nCode\npurchased <- prices %>%\n  filter(date == min(date)) %>%\n  group_by(symbol) %>%\n  slice(1) %>%\n  mutate(shares = 1000/3 / adjusted)\n\n\n\n\nCode\nprice_value <- prices %>%\n  left_join(purchased %>% select(symbol, shares),\n            by = \"symbol\") %>%\n  mutate(value = shares * adjusted)\n\n\n\n\nCode\nprice_totals <- price_value %>%\n  mutate(owner = if_else(symbol %in% friend_stocks, \"Friend\", \"Me\")) %>%\n  group_by(owner, date) %>%\n  summarise(total_value = sum(value), .groups = \"drop\")\n\n\n\n\nCode\n# Step 6: Plot who is winning each day\nggplot(price_totals,\n       aes(date, total_value, color = owner)) +\n  geom_line(size = 1.2) +\n  labs(\n    title = \"Portfolio Value Over Time\",\n    x = \"Date\",\n    y = \"Total Value ($)\",\n    color = \"Investor\"\n  ) +\n  theme_minimal(base_size = 14)\n\n\n\n\n\nHonestly, I thought my portfolio would fall far behind my friend’s… but no. We basically traded punches the whole time, and somehow we ended up pretty close. I chose stocks that are still big but not as big as the companies my friend did. My friend’s tech stocks kept randomly jumping like they drank a Monster Energy, which pushed him ahead at a few points. I guess thats the big companies for some seasons. In the end he still beat me by a bit, but I was closer than I thought.\n\n\nCode\nstock_contributions <- price_value %>%\n  mutate(owner = if_else(symbol %in% friend_stocks, \"Friend\", \"Me\")) %>%\n  group_by(owner, symbol, date) %>%\n  summarise(value = sum(value), .groups = \"drop\")\n\nggplot(stock_contributions,\n       aes(x = date, y = value, fill = symbol)) +\n  geom_area(alpha = 0.7) +\n  facet_wrap(~ owner, scales = \"free_y\") +\n  labs(\n    title = \"Which Stocks Contribute to Each Portfolio?\",\n    x = \"Date\",\n    y = \"Value ($)\",\n    fill = \"Stock\"\n  ) +\n  theme_minimal(base_size = 14)\n\n\n\n\n\nFor the stock-breakdown graph, the story becomes pretty obvious. My friend’s portfolio was basically being dragged to victory by Apple and Amazon like they were unstoppable gym bros. Every time those two flexed, his whole portfolio shot up. Meanwhile, my stocks were doing the “stable and sensible adult” thing. SPY and JPM showed up consistently, and EWY tried its best but I had to represent KOREA. So, his huge tech companies did all the heavy lifting, while mine just jogged steadily along."
  },
  {
    "objectID": "weeks/w07/car_wash.html",
    "href": "weeks/w07/car_wash.html",
    "title": "car_wash",
    "section": "",
    "text": "Code\n# Convert the time\ncarwash_clean <- carwash %>%\n  mutate(\n    datetime_utc = ymd_hms(time, tz = \"UTC\"),\n    datetime_local = with_tz(datetime_utc, tzone = \"America/Boise\")\n  )\n\n\n\n\nCode\n# Unusual data\ncarwash_clean <- carwash_clean %>%\n  filter(amount > 1) \n\n\n\n\nCode\n# Grouping the time\ncarwash_clean <- carwash_clean %>%\n  mutate(\n    hour = ceiling_date(datetime_local, unit = \"hour\")\n  )\n\n\n\n\nCode\n# Aggregate into hours\nsales_hourly <- carwash_clean %>%\n  group_by(hour) %>%\n  summarise(\n    total_sales = sum(amount),\n    n_transactions = n()\n  )\n\n\n\n\nCode\n# Weather\nstart <- min(carwash_clean$datetime_local)\nend   <- max(carwash_clean$datetime_local)\n\nweather <- riem_measures(\n  station = \"RXE\",\n  date_start = as.Date(start),\n  date_end = as.Date(end)\n)\n\n\n\n\nCode\n# Hourly variable\nweather_clean <- weather %>%\n  mutate(\n    hour = floor_date(ymd_hms(valid), unit = \"hour\")\n  ) %>%\n  select(hour, tmpf)\n\n\n\n\nCode\n# Join\ncarwash_weather <- sales_hourly %>%\n  left_join(weather_clean, by = \"hour\")\n\n\n\n\nCode\n# Plots!\n\n\nggplot(carwash_weather, aes(tmpf, total_sales, color = tmpf)) +\n  geom_point(size = 2) +\n  geom_smooth(se = TRUE, color = \"#8a6240\", linewidth = 2) +  \n  scale_color_viridis_c(option = \"viridis\") +   # <-- EXACT palette in your image\n  labs(\n    title = \"Sales vs Temperature\",\n    x = \"Temperature (F)\",\n    y = \"Total Sales\",\n    color = \"Temp (F)\"\n  ) +\n  theme_minimal(base_size = 14)\n\n\n\n\n\n\n\nCode\nggplot(carwash_weather, aes(hour, total_sales, color = tmpf)) +\n  geom_line() +\n  scale_color_viridis_c() +\n  labs(title = \"Hourly Sales with Temperature Gradient\")\n\n\n\n\n\nFrom the “Sales vs Temperature” graph, the relationship forms a curve that looks almost like a mustache. Sales increase as temperatures rise from the 40s into the 60s, flatten out, then rise again near the 80s before declining when it gets very hot. This suggests two “sweet spots” where customers are most likely to wash their cars.\nOne possible reason is comfort behavior:\nAround 60°F, the weather becomes pleasant after cold days, so people start doing outdoor tasks like car washing. There’s no risk of it snowing and wasting money on a car wash!\nAround 80°F, it’s warm and sunny — ideal “weekend chore” weather — so sales rise again.\nThe drop above 85–90°F could be because it’s too hot, and people avoid being outside for long. I get it, I never go outside after it 90°F\nAnother factor is weather patterns not included in this dataset. Rainy or stormy hours would reduce sales dramatically, and extremely sunny/dry stretches might boost them. Since we don’t have precipitation data here, temperature alone gives us an incomplete but still meaningful picture. We’d need more data to know this for sure though."
  },
  {
    "objectID": "weeks/w08/US_cities.html",
    "href": "weeks/w08/US_cities.html",
    "title": "US_cities",
    "section": "",
    "text": "Code\nstates <- us_states() %>%\n  filter(!state_name %in% c(\"Alaska\", \"Hawaii\", \"Puerto Rico\"))\n\n\n\n\nCode\nidaho_counties <- us_counties() %>%\n  filter(state_name == \"Idaho\")\n\n\n\n\nCode\ncities <- us_cities()\n\n\n\n\nCode\ncities48 <- cities %>%\n  filter(state_name %in% states$state_name)\n\n\n\n\nCode\ntop3 <- cities48 %>%\n  group_by(state_name) %>%\n  slice_max(order_by = population, n = 3) %>%\n  mutate(rank = row_number()) %>%\n  ungroup()\n\n\n\n\nCode\ncoords <- st_coordinates(top3)\ntop3$lng <- coords[,1]\ntop3$lat <- coords[,2]\n\n\n\n\nCode\nrank_colors <- c(\n  \"1\" = \"#003f88\",\n  \"2\" = \"#4ea8de\",\n  \"3\" = \"#ade8f4\"\n)\n\n\n\n\nCode\ntop3 <- top3 %>%\n  mutate(pop_thousands = population / 1000)\n\nggp <- ggplot() +\n  geom_sf(data = states, fill = NA, color = \"black\", size = 0.3) +\n  geom_sf(data = idaho_counties, fill = NA, color = \"gray30\", size = 0.4) +\n  geom_point(\n    data = top3,\n    aes(\n      x = lng,\n      y = lat,\n      size = pop_thousands,\n      fill = as.factor(rank)\n    ),\n    color = \"#003f88\",\n    alpha = 0.7,\n    shape = 21\n  ) +\n  scale_fill_manual(values = rank_colors) +\n  scale_size_continuous(\n    range = c(1, 6),\n    breaks = c(2000, 4000, 6000, 8000),\n    name = \"Population\\n(1,000)\"\n  ) +\n  geom_label_repel(\n    data = top3,\n    aes(x = lng, y = lat, label = city),\n    size = 3,\n    colour = \"#5e4fa2\",\n    fill = \"white\",\n    label.size = 0.35,\n    label.r = unit(0.1, \"lines\"),\n    label.padding = unit(0.15, \"lines\"),\n    box.padding = unit(0.25, \"lines\"),\n    point.padding = unit(0.1, \"lines\"),\n    segment.color = NA,\n    label.color = \"#5e4fa2\"\n  ) +\n  guides(\n    fill = \"none\"\n  ) +\n  theme_minimal(base_size = 14) +\n  theme(\n    panel.grid.major = element_blank(),\n    panel.grid.minor = element_blank(),\n    legend.position = \"right\",\n    axis.title = element_blank(),\n    axis.text = element_blank()\n  )\n\nggp\n\n\n\n\n\n\n\nCode\nggsave(\"us_cities2.png\", plot = ggp, width = 15, height = 10, dpi = 300)\n\n\n\n\n\nThe copied Picture"
  },
  {
    "objectID": "weeks/w08/idaho_water.html",
    "href": "weeks/w08/idaho_water.html",
    "title": "idaho_water",
    "section": "",
    "text": "Code\nwells_clean <- wells %>%\n  filter(Production > 5000)\n\ndams_clean <- dam %>%\n  filter(SurfaceAre > 50)\n\nwater_clean <- water %>%\n  filter(FEAT_NAME %in% c(\"Snake River\", \"Henrys Fork\"))\n\nidaho_crs <- 32611\nshape_proj <- st_transform(shape, idaho_crs)\nwells_proj <- st_transform(wells_clean, idaho_crs)\ndams_proj  <- st_transform(dams_clean,  idaho_crs)\nwater_proj <- st_transform(water_clean, idaho_crs)\n\n\n\n\nCode\nggmap <- ggplot() +\n  geom_sf(data = shape_proj, fill = \"gray90\", color = \"gray60\") +\n  geom_sf(data = water_proj, color = \"blue\", size = 0.9) +\n  geom_sf(data = dams_proj,  color = \"red\", size = 2) +\n  geom_sf(data = wells_proj, color = \"darkgreen\", size = 1) +\n  scale_color_manual(\n    name = \"Map Features\",\n    values = c(\n      \"Rivers\" = \"blue\",\n      \"Dams\"   = \"red\",\n      \"Wells\"  = \"darkgreen\"\n    )\n  ) +\n  theme_minimal() +\n  labs(\n    title = \"Idaho Water Map\",\n    subtitle = \"Filtered wells, dams, and river segments\",\n    caption = \"Wells: Production > 5000 gallons | Dams: Surface Area > 50 acres\"\n  )\n\nggmap\n\n\n\n\n\n\n\nCode\nggsave(\"idaho_water.png\", plot = ggmap, width = 15, height = 10, dpi = 300)\n\n\n\n\n\nHeres Our Map"
  },
  {
    "objectID": "weeks/w09/maps_leaflets.html",
    "href": "weeks/w09/maps_leaflets.html",
    "title": "maps_leaflets",
    "section": "",
    "text": "Code\nmy_colors <- as.character(paletteer::paletteer_c(\"ggthemes::Blue-Green Sequential\", 30))\n\naccess_pal <- colorFactor(\n  palette = my_colors,\n  domain = hot_springs$access_type\n)\n\nleaflet(hot_springs) %>%\n  addProviderTiles(\"Esri.WorldImagery\", group = \"Satellite\") %>%\n  addProviderTiles(\"OpenTopoMap\", group = \"Topo\") %>%\n  addProviderTiles(\"Stadia.StamenWatercolor\", group = \"Watercolor\") %>%\n  \n  addCircleMarkers(lng = ~lng, lat = ~lat, radius = ~case_when(\n  difficulty %in% c(\"Easy\") ~ 6,\n  difficulty %in% c(\"Easy-Moderate\",\"Moderate\") ~ 7, TRUE ~ 8), \n  color = \"white\", weight = 1, fillColor = ~access_pal(access_type), fillOpacity = 0.9, popup = ~paste(\n  \"<b>\", name, \"</b><br>\",\n  state, \"<br>\",\n  \"Elevation:\", elevation_ft, \"ft<br>\",\n  \"Temp (approx):\", temperature_F, \"°F<br>\",\n  \"Access:\", access_type, \"<br>\",\n  \"Hike distance:\", hike_distance_miles, \"miles<br>\",\n  \"Difficulty:\", difficulty, \"<br>\",\n  \"<em>\", notes, \"</em>\")\n  ) %>%\n  addLayersControl(\n  baseGroups = c(\"Satellite\", \"Topo\", \"Watercolor\"),\n  options = layersControlOptions(collapsed = FALSE)\n)"
  },
  {
    "objectID": "case_studies/leaflet_layers.html",
    "href": "case_studies/leaflet_layers.html",
    "title": "leaflet_layers",
    "section": "",
    "text": "Code\nstates <- us_states() %>%\n  filter(!state_name %in% c(\"Alaska\", \"Hawaii\", \"Puerto Rico\"))\n\n\n\n\nCode\nidaho_counties <- us_counties() %>%\n  filter(state_name == \"Idaho\")\n\n\n\n\nCode\ncities <- us_cities()\n\n\n\n\nCode\ncities48 <- cities %>%\n  filter(state_name %in% states$state_name)\n\n\n\n\nCode\ntop3 <- cities48 %>%\n  group_by(state_name) %>%\n  slice_max(order_by = population, n = 3) %>%\n  mutate(rank = row_number()) %>%\n  ungroup()\n\n\n\n\nCode\ncoords <- st_coordinates(top3)\ntop3$lng <- coords[,1]\ntop3$lat <- coords[,2]\n\n\n\n\nCode\ntop3 <- top3 %>%\n  mutate(pop_thousands = population / 1000)\n\nrank_pal <- colorFactor(\n  palette = c(\n    \"1\" = \"#003f88\",\n    \"2\" = \"#4ea8de\",\n    \"3\" = \"#ade8f4\"\n  ),\n  domain = top3$rank\n)\n\n\n\n\nCode\nggp <- leaflet() %>% \n  addProviderTiles(\"Esri.WorldImagery\", group = \"Satellite\") %>%\n  addProviderTiles(\"OpenTopoMap\", group = \"Topo\") %>%\n  addProviderTiles(\"Stadia.StamenWatercolor\", group = \"Watercolor\") %>%\n  addProviderTiles(\"USGS.USTopo\", group = \"USGS Topo\") %>%\n    addLayersControl(\n    baseGroups = c(\"Satellite\", \"Topo\", \"Watercolor\", \"USGS Topo\"),\n    overlayGroups = c(\"States\", \"Idaho Counties\", \"Top Cities\"),\n    options = layersControlOptions(collapsed = FALSE)\n  ) %>% \n  addPolygons(\n    data = states,\n    group = \"States\",\n    color = \"black\",\n    weight = 1,\n    fill = FALSE\n  ) %>%\n  addPolygons(\n    data = idaho_counties,\n    group = \"Idaho Counties\",\n    color = \"gray30\",\n    weight = 1.2,\n    fill = FALSE\n  ) %>%\n  addCircleMarkers(\n    data = top3,\n    group = \"Top Cities\",\n    lng = ~lng, lat = ~lat,\n    radius = ~scales::rescale(population, to = c(5,20)),\n    fillColor = ~rank_pal(rank),\n    fillOpacity = 0.8,\n    color = \"#003f88\", weight = 1,\n    popup = ~paste(\n      \"<b>\", city, \",\", state_name, \"</b><br>\",\n      \"Rank:\", rank, \"<br>\",\n      \"Population:\", format(population, big.mark = \",\")\n    )\n  ) %>%\n  addLegend(\n    position = \"bottomright\",\n    pal = rank_pal,\n    values = top3$rank,\n    title = \"City Rank in State\"\n  )\n\nggp\n\n\n\n\n\n\n\n\nCode\n# ggsave(\"us_cities2.png\", plot = ggp, width = 15, height = 10, dpi = 300)\n\n# ![The copied Picture](pictures/us_cities2.png)"
  },
  {
    "objectID": "weeks/w010/counting_words.html",
    "href": "weeks/w010/counting_words.html",
    "title": "counting_words",
    "section": "",
    "text": "Code\npacman::p_load(readr, tidyverse, stringr, stringi, rio, pander, ggplot2)\n\nscriptures <- read_csv(\"https://github.com/beandog/lds-scriptures/raw/master/csv/lds-scriptures.csv\")\n\n\nRows: 41995 Columns: 19\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (13): volume_title, book_title, volume_long_title, book_long_title, volu...\ndbl  (6): volume_id, book_id, chapter_id, verse_id, chapter_number, verse_nu...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\nCode\nbom_nt <- scriptures %>% \n  filter(volume_title %in% c(\"New Testament\", \"Book of Mormon\")) %>% \n  mutate(\n    num_words = str_count(scripture_text, \" \") + 1\n  )\n\nglimpse(bom_nt)\n\n\nRows: 14,561\nColumns: 20\n$ volume_id          <dbl> 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, …\n$ book_id            <dbl> 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40,…\n$ chapter_id         <dbl> 930, 930, 930, 930, 930, 930, 930, 930, 930, 930, 9…\n$ verse_id           <dbl> 23146, 23147, 23148, 23149, 23150, 23151, 23152, 23…\n$ volume_title       <chr> \"New Testament\", \"New Testament\", \"New Testament\", …\n$ book_title         <chr> \"Matthew\", \"Matthew\", \"Matthew\", \"Matthew\", \"Matthe…\n$ volume_long_title  <chr> \"The New Testament\", \"The New Testament\", \"The New …\n$ book_long_title    <chr> \"The Gospel According to St Matthew\", \"The Gospel A…\n$ volume_subtitle    <chr> \"Of our Lord and Saviour Jesus Christ\", \"Of our Lor…\n$ book_subtitle      <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ volume_short_title <chr> \"NT\", \"NT\", \"NT\", \"NT\", \"NT\", \"NT\", \"NT\", \"NT\", \"NT…\n$ book_short_title   <chr> \"Matt.\", \"Matt.\", \"Matt.\", \"Matt.\", \"Matt.\", \"Matt.…\n$ volume_lds_url     <chr> \"nt\", \"nt\", \"nt\", \"nt\", \"nt\", \"nt\", \"nt\", \"nt\", \"nt…\n$ book_lds_url       <chr> \"matt\", \"matt\", \"matt\", \"matt\", \"matt\", \"matt\", \"ma…\n$ chapter_number     <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ verse_number       <dbl> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, …\n$ scripture_text     <chr> \"The book of the generation of Jesus Christ, the so…\n$ verse_title        <chr> \"Matthew 1:1\", \"Matthew 1:2\", \"Matthew 1:3\", \"Matth…\n$ verse_short_title  <chr> \"Matt. 1:1\", \"Matt. 1:2\", \"Matt. 1:3\", \"Matt. 1:4\",…\n$ num_words          <dbl> 16, 14, 16, 12, 16, 21, 12, 12, 12, 12, 16, 14, 12,…\n\n\n\n\n\n\nCode\n# Average verse length\navg_verse_length <- bom_nt %>%\n  group_by(volume_title) %>%\n  summarise(avg_words = mean(num_words, na.rm = TRUE))\n\npander(avg_verse_length) \n\n\n\n\n\n\n\n\n\nvolume_title\navg_words\n\n\n\n\nBook of Mormon\n40.42\n\n\nNew Testament\n22.67\n\n\n\n\n\n\n\n\n\n\nCode\n# Count \"Jesus\" mentions per volume\njesus_count <- bom_nt %>%\n  mutate(jesus_mentions = str_count(scripture_text, regex(\"Jesus\", ignore_case = TRUE))) %>%\n  group_by(volume_title) %>%\n  summarise(total_mentions = sum(jesus_mentions, na.rm = TRUE))\n\njesus_count\n\n\n# A tibble: 2 × 2\n  volume_title   total_mentions\n  <chr>                   <int>\n1 Book of Mormon            184\n2 New Testament             984\n\n\n\n\n\n\n\nCode\nbom <- bom_nt %>%\n  filter(volume_title == \"Book of Mormon\")\n\n# Optional glimpse of word counts per verse\nbom %>% select(book_title, num_words) %>% glimpse()\n\n\nRows: 6,604\nColumns: 2\n$ book_title <chr> \"1 Nephi\", \"1 Nephi\", \"1 Nephi\", \"1 Nephi\", \"1 Nephi\", \"1 N…\n$ num_words  <dbl> 68, 25, 27, 56, 28, 46, 33, 46, 30, 20, 35, 18, 47, 78, 43,…\n\n\n\n\n\n\n\nCode\n# Visualize\nggplot(bom, aes(x = fct_reorder(book_title, num_words, .fun = median), y = num_words)) +\n  geom_boxplot(fill = \"skyblue\") +\n  labs(\n    title = \"Distribution of Verse Lengths in the Book of Mormon\",\n    x = \"Book\",\n    y = \"Number of Words per Verse\"\n  ) +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))"
  },
  {
    "objectID": "weeks/w010/string_task.html",
    "href": "weeks/w010/string_task.html",
    "title": "string_task",
    "section": "",
    "text": "Code\npacman::p_load(tidyverse, stringr, stringi, rio)\n\n\nQuestion 2 (With the randomletters.txt file, pull out every 1700 letter (for example, 1, 1700, 3400, 5100, …) and find the quote that is hidden—the quote ends with a period.)\n\n\nCode\nchars <- readr::read_file('https://byuistats.github.io/M335/data/randomletters.txt')\ninculde_all <- paste(chars, collapse = \"\")\nbreak_string <- str_split(inculde_all, \"\")[[1]]\n\nposition <- c(1,seq(0, length(break_string), by = 1700))\n\nbreak_string[position]\n\n\n [1] \"t\" \"h\" \"e\" \" \" \"p\" \"l\" \"u\" \"r\" \"a\" \"l\" \" \" \"o\" \"f\" \" \" \"a\" \"n\" \"e\" \"c\" \"d\"\n[20] \"o\" \"t\" \"e\" \" \" \"i\" \"s\" \" \" \"n\" \"o\" \"t\" \" \" \"d\" \"a\" \"t\" \"a\" \".\" \"z\" \" \" \"a\"\n[39] \"n\" \"f\" \"r\" \"a\"\n\n\nQuestion 3 (With the randomletters_wnumbers.txt file, find all the numbers hidden, and convert those numbers to letters using the letters order in the alphabet to decipher the message. For example, a 1=a, 2=b,…, 26=z (Hint: the message starts with “experts”).)\n\n\nCode\nchars_num <- readr::read_lines(\"https://byuistats.github.io/M335/data/randomletters_wnumbers.txt\")\n\nnum_words <- str_extract_all(chars_num, \"[:digit:]+\")[[1]]\nnum_words <- as.integer(num_words)\nnums <- letters[num_words]\npaste(nums, collapse = \"\")\n\n\n[1] \"expertsoftenpossessmoredatathanjudgment\"\n\n\nQuestion 4\nWith the randomletters.txt file, remove all the spaces and periods from the string then find the longest sequence of vowels.\n\n\nCode\ncleaned <- str_remove_all(chars, \"[ \\\\.]\")\nvowel_sequences <- str_extract_all(cleaned, \"[aeiouy]+\")[[1]]\nlongest_vowel_sequence <- vowel_sequences[which.max(nchar(vowel_sequences))]\nlongest_vowel_sequence\n\n\n[1] \"ayeyeya\""
  },
  {
    "objectID": "weeks/w10/counting_words.html",
    "href": "weeks/w10/counting_words.html",
    "title": "counting_words",
    "section": "",
    "text": "Code\npacman::p_load(readr, tidyverse, stringr, stringi, rio, pander, ggplot2)\n\nscriptures <- read_csv(\"https://github.com/beandog/lds-scriptures/raw/master/csv/lds-scriptures.csv\")\n\n\nRows: 41995 Columns: 19\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (13): volume_title, book_title, volume_long_title, book_long_title, volu...\ndbl  (6): volume_id, book_id, chapter_id, verse_id, chapter_number, verse_nu...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\nCode\nbom_nt <- scriptures %>% \n  filter(volume_title %in% c(\"New Testament\", \"Book of Mormon\")) %>% \n  mutate(\n    num_words = str_count(scripture_text, \" \") + 1\n  )\n\nglimpse(bom_nt)\n\n\nRows: 14,561\nColumns: 20\n$ volume_id          <dbl> 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, …\n$ book_id            <dbl> 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40,…\n$ chapter_id         <dbl> 930, 930, 930, 930, 930, 930, 930, 930, 930, 930, 9…\n$ verse_id           <dbl> 23146, 23147, 23148, 23149, 23150, 23151, 23152, 23…\n$ volume_title       <chr> \"New Testament\", \"New Testament\", \"New Testament\", …\n$ book_title         <chr> \"Matthew\", \"Matthew\", \"Matthew\", \"Matthew\", \"Matthe…\n$ volume_long_title  <chr> \"The New Testament\", \"The New Testament\", \"The New …\n$ book_long_title    <chr> \"The Gospel According to St Matthew\", \"The Gospel A…\n$ volume_subtitle    <chr> \"Of our Lord and Saviour Jesus Christ\", \"Of our Lor…\n$ book_subtitle      <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ volume_short_title <chr> \"NT\", \"NT\", \"NT\", \"NT\", \"NT\", \"NT\", \"NT\", \"NT\", \"NT…\n$ book_short_title   <chr> \"Matt.\", \"Matt.\", \"Matt.\", \"Matt.\", \"Matt.\", \"Matt.…\n$ volume_lds_url     <chr> \"nt\", \"nt\", \"nt\", \"nt\", \"nt\", \"nt\", \"nt\", \"nt\", \"nt…\n$ book_lds_url       <chr> \"matt\", \"matt\", \"matt\", \"matt\", \"matt\", \"matt\", \"ma…\n$ chapter_number     <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ verse_number       <dbl> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, …\n$ scripture_text     <chr> \"The book of the generation of Jesus Christ, the so…\n$ verse_title        <chr> \"Matthew 1:1\", \"Matthew 1:2\", \"Matthew 1:3\", \"Matth…\n$ verse_short_title  <chr> \"Matt. 1:1\", \"Matt. 1:2\", \"Matt. 1:3\", \"Matt. 1:4\",…\n$ num_words          <dbl> 16, 14, 16, 12, 16, 21, 12, 12, 12, 12, 16, 14, 12,…\n\n\n\n\n\n\nCode\n# Average verse length\navg_verse_length <- bom_nt %>%\n  group_by(volume_title) %>%\n  summarise(avg_words = mean(num_words, na.rm = TRUE))\n\npander(avg_verse_length) \n\n\n\n\n\n\n\n\n\nvolume_title\navg_words\n\n\n\n\nBook of Mormon\n40.42\n\n\nNew Testament\n22.67\n\n\n\n\n\n\n\n\n\n\nCode\n# Count \"Jesus\" mentions per volume\njesus_count <- bom_nt %>%\n  mutate(jesus_mentions = str_count(scripture_text, regex(\"Jesus\", ignore_case = TRUE))) %>%\n  group_by(volume_title) %>%\n  summarise(total_mentions = sum(jesus_mentions, na.rm = TRUE))\n\njesus_count\n\n\n# A tibble: 2 × 2\n  volume_title   total_mentions\n  <chr>                   <int>\n1 Book of Mormon            184\n2 New Testament             984\n\n\n\n\n\n\n\nCode\nbom <- bom_nt %>%\n  filter(volume_title == \"Book of Mormon\")\n\n# Optional glimpse of word counts per verse\nbom %>% select(book_title, num_words) %>% glimpse()\n\n\nRows: 6,604\nColumns: 2\n$ book_title <chr> \"1 Nephi\", \"1 Nephi\", \"1 Nephi\", \"1 Nephi\", \"1 Nephi\", \"1 N…\n$ num_words  <dbl> 68, 25, 27, 56, 28, 46, 33, 46, 30, 20, 35, 18, 47, 78, 43,…\n\n\n\n\n\n\n\nCode\n# Visualize\nggplot(bom, aes(x = fct_reorder(book_title, num_words, .fun = median), y = num_words)) +\n  geom_boxplot(fill = \"skyblue\") +\n  labs(\n    title = \"Distribution of Verse Lengths in the Book of Mormon\",\n    x = \"Book\",\n    y = \"Number of Words per Verse\"\n  ) +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))"
  },
  {
    "objectID": "classmates/baseball.html",
    "href": "classmates/baseball.html",
    "title": "baseball",
    "section": "",
    "text": "Code\n#Gathering Data\n\nplayers <- Salaries %>%\n  mutate(salary = salary/1000) %>% \n  left_join(People, by=\"playerID\") %>% \n  select(playerID, nameFirst, nameLast, salary, yearID)\n\ncollege <- CollegePlaying %>% \n  group_by(playerID) %>% \n  summarise(schoolID = last(schoolID, order_by = yearID)) %>% \n  left_join(Schools, by = \"schoolID\") %>% \n  select(playerID, schoolID, name_full, state)\n\nfull <- players %>% \n  left_join(college, by = \"playerID\") %>%\n  filter(state == \"UT\") %>%\n  group_by(playerID) %>% \n  mutate(year = rank(as.numeric(yearID))-1) %>% \n  ungroup()\n\n\ngrouped_full <- full%>% \n  group_by(name_full, year) %>% \n  summarise(\n    salary = mean(salary),\n    player_cnt = n()\n  )\n\ntop2 <- grouped_full %>% \n  mutate(salary = salary/1000) %>% \n  filter(name_full %in% c(\"Brigham Young University\", \"Dixie State College of Utah\"))\n\n\nWhen it comes to Professional Baseball, only two Utah colleges have been truly successful in delivering players: Dixie State and BYU. Below is a graph showing how the two compare to each other\n\n\nCode\nggplot(top2, aes(year, salary, color=name_full)) +\n  geom_line(linewidth = 1) +\n  theme_minimal()+\n  geom_text_repel(data = top2 %>% filter(year %% 4 == 0), aes(label = player_cnt), nudge_x = -.3, nudge_y = .4)+\n  theme(\n    legend.position = \"none\",\n    plot.title = element_text(size =23),\n    plot.subtitle = element_text(color = \"grey30\", size = 12),\n    panel.grid = element_blank(),\n    panel.grid.major.y = element_line(linetype = \"dashed\", linewidth = .75, color = \"grey80\"),\n    axis.line.x.bottom = element_line(linewidth = 1, color = \"grey80\"),\n    axis.title = element_text(color = \"grey25\"),\n    axis.text = element_text(color = \"grey40\")\n  )+\n  labs(\n    title = \"BYU vs Dixie State - Baseball Careers\",\n    subtitle = \"avg salary and count of players\",\n    y = \"\", x = \"Years Played\"\n  ) +\n  scale_x_continuous(\n    breaks = seq(0, 17, 2)\n  ) +\n  scale_y_continuous(\n    breaks = c(1, 3, 5),\n    labels = scales::label_dollar(prefix = \"$\", suffix = \"M\")\n  )+\n  scale_color_manual(values = c(\"#0072CE\", \"#C21B32\"))\n\n\n\n\n\nOne could say, “look! Dixie has the highest paid baseball players on average!”. They would be right, but they would have to ignore the amount of players that came from that college. Dixie has had two players, one of which had had a career that didn’t even last 4 years. Sure that other player was paid more on average then the average of the BYU players, but BYU has had 8 alumni play professionally, dropping down to 5 when Dixie only had 1. BYU players come very close to performing similarly with those from Dixie, but have a 4 to 1 chance of making it to the big leagues, and staying.\n\nTo be complete, I have included a similar graph showing the players from other colleges, all of whom only had 1-2 players compete professionally.\n\n\nCode\nggplot(grouped_full %>% filter(year <= 3), aes(year, salary, color=name_full)) +\n  geom_line(linewidth = 1) +\n  geom_point()+\n  theme_minimal()+\n  geom_text_repel(aes(label = player_cnt))+\n  theme(\n    legend.position = \"none\",\n    plot.title = element_text(size =23),\n    plot.subtitle = element_text(color = \"grey40\"),\n    panel.grid = element_blank(),\n    panel.grid.major.y = element_line(linetype = \"dashed\", linewidth = .75, color = \"grey80\"),\n    axis.line.x.bottom = element_line(linewidth = 1, color = \"grey80\"),\n    axis.title = element_text(color = \"grey25\"),\n    axis.text = element_text(color = \"grey40\")\n  )+\n  coord_cartesian(xlim = c(-.25,1.25), ylim = c(100,500))+\n  scale_y_continuous(\n    breaks = c(200, 400),\n    labels = scales::label_dollar(prefix = \"$\", suffix = \"K\")\n  )+\n  labs(\n    title = \"Utah Colleges - Baseball Careers\",\n    subtitle = \"avg salary and count of players\",\n    y = \"Salary\", x = \"Years Played\"\n  )"
  },
  {
    "objectID": "classmates/OWD_Dup.html",
    "href": "classmates/OWD_Dup.html",
    "title": "OWD_Dup",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(rio)\nlibrary(ggrepel)\n\nrobots <- import(\"https://ourworldindata.org/grapher/annual-industrial-robots-installed.csv?v=1&csvType=full&useColumnShortNames=true\")\n\nrobots <- robots %>% \n  filter(Entity != \"World\")"
  },
  {
    "objectID": "classmates/OWD_Dup.html#original",
    "href": "classmates/OWD_Dup.html#original",
    "title": "OWD_Dup",
    "section": "Original",
    "text": "Original\n\n\n\noriginal graph\n\n\n\n\nCode\nggplot(robots, aes(Year, installations, color=Entity)) +\n  geom_line(size = .8) + \n  geom_point() + \n  labs(\n    title = \"Annual industrial robots installed in top five countries\",\n    subtitle = \"Industrial robots are automated, reprogrammable machines that perfor a variety of tasks in industrial settings.\",\n    x=\"\", y=\"\"\n  )+\n  scale_x_continuous(\n    breaks = c(2011, 2014, 2016, 2018, 2020, 2022, 2023)\n  )+\n  scale_y_continuous(\n    breaks = seq(0, 300000, 50000),\n    labels = scales::label_number(big.mark = \",\")\n  )+\n  theme(\n    plot.title = element_text(size = 18, family = \"serif\"),\n    plot.subtitle = element_text(size = 9, family = \"sans\", color = \"grey35\"),\n    \n    panel.background = element_rect(fill=\"white\"),\n    panel.grid = element_blank(),\n    panel.grid.major.y = element_line(linetype = \"dashed\", \n                                      color = \"grey80\",\n                                      linewidth = .5),\n    axis.ticks.y = element_blank(),\n    axis.ticks.x = element_line(color = \"grey70\", linewidth = .6),\n    axis.line.x.bottom = element_line(color = \"grey70\", linewidth = .6),\n    axis.text = element_text(color = \"grey40\"),\n    legend.position = \"none\"\n  ) + \n  scale_color_manual(values = c(\"#771199\", \"#E74C3C\", \"#8B4513\", \"#1C4587\", \"#487654\"))\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead."
  }
]