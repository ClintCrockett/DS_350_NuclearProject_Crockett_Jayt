[
  {
    "objectID": "w02/Second_Quarto_File.html",
    "href": "w02/Second_Quarto_File.html",
    "title": "Second_Quarto_File",
    "section": "",
    "text": "Code\n# Working with the Flights Data Set\navg_delay <- flights %>%\n  filter(arr_delay > 0) %>%\n  group_by(origin) %>% \n  summarise(avg_delay_total = mean(arr_delay, na.rm = TRUE)) %>% \n  ungroup()\n\nplot_ly(avg_delay, type='barpolar', r=~avg_delay_total, theta=~origin, color=~origin) %>%  \n  layout(\n    title = \"2013 NYC Flight Average Arrival Delays\",\n    polar = list(\n      radialaxis = list(title = \"Number of Delays\"),\n      angularaxis = list(categoryarray = flights$origin)\n    )\n )\n\n\n\n\n\n\n\nOkay technically this isn’t a pie chart so I should be good… right? Originally I wanted to to do a racial bar chart but I could not find anything on plotly and ggplot didn’t work like I wanted it to. I’d love to learn if there is a way to create it if you have any tips."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "about",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "w01/First_Quarto_File.html",
    "href": "w01/First_Quarto_File.html",
    "title": "First_Quarto_File",
    "section": "",
    "text": "Code\n## Sourced Articles\n\n### Article 1: Top 10 Movies on Netflix Right Now(https://www.netflix.com/tudum/top10)\n\n### First we have Netflix showing the top viewed movies on their streaming service. It looks visually nice when you get to see the movie and the ranking on the top left corner. It is clean, however, when you scroll down to look at why the data looks very strange. We have Kpop Demon Hunters sitting at first followed by the rest of the movies. Next to the titles is a column of weeks and views. All the weeks shown are different most at 1 which is unknown if it is current or of all time. The visual isn't very pleasing either. Just straight forward with no interesting or stand out visuals. Just basic, almost like an excel sheet. It looks clean but maybe something a little more fun to look at could go a long way.\n\nplot(1:20)\nFirst we have Netflix showing the top viewed movies on their streaming service. At first it looks visually nice when you get to see the movie and the ranking on the top left corner. However, when you scroll down to look at why the data looks very strange. We have Kpop Demon Hunters sitting at first followed by the rest of the movies. Next to the titles is a column of weeks and views. All the weeks shown are different most at 1 which is unknown if it is current or of all time. The visual isn’t very pleasing either. Just straight forward with no interesting or stand out visuals. Just basic, almost like an excel sheet. It looks clean but maybe something a little more fun to look at could go a long way."
  },
  {
    "objectID": "w01/First_Quarto_File.html#article-1-top-10-movies-on-netflix-right-nowhttpswww.netflix.comtudumtop10",
    "href": "w01/First_Quarto_File.html#article-1-top-10-movies-on-netflix-right-nowhttpswww.netflix.comtudumtop10",
    "title": "First_Quarto_File",
    "section": "Article 1: Top 10 Movies on Netflix Right Now(https://www.netflix.com/tudum/top10)",
    "text": "Article 1: Top 10 Movies on Netflix Right Now(https://www.netflix.com/tudum/top10)\nNext we have Anychart. Any chart looks like a place just to post what the best looking data visualizations there are. It’s a great website but it uses none of them itself, maybe to avoid bias. Many of the graphs created are very unique but some are a little too busy to look at."
  },
  {
    "objectID": "w01/First_Quarto_File.html#article-2-top-data-visualizations-on-travel-burgers-shootings-and-light",
    "href": "w01/First_Quarto_File.html#article-2-top-data-visualizations-on-travel-burgers-shootings-and-light",
    "title": "First_Quarto_File",
    "section": "Article 2: Top Data Visualizations on Travel, Burgers, Shootings, and Light",
    "text": "Article 2: Top Data Visualizations on Travel, Burgers, Shootings, and Light"
  },
  {
    "objectID": "w01/First_Quarto_File.html#httpswww.anychart.comblog20190809top-data-visualizations-dataviz-weekly",
    "href": "w01/First_Quarto_File.html#httpswww.anychart.comblog20190809top-data-visualizations-dataviz-weekly",
    "title": "First_Quarto_File",
    "section": "(https://www.anychart.com/blog/2019/08/09/top-data-visualizations-dataviz-weekly/)",
    "text": "(https://www.anychart.com/blog/2019/08/09/top-data-visualizations-dataviz-weekly/)\nLastly we have Tastewise which is a website looking at the popularity of a culture’s food. Showing trends somewhere, maybe globally, ingredients and flavors, and some links that don’t work… They used a variety of different graphs and visuals but some look like completely different designs almost as if they don’t go together. Most of the data is not fully understandable because the labels aren’t very strong to explain what it’s showing."
  },
  {
    "objectID": "w01/First_Quarto_File.html#article-3-korean-food-trend-overview",
    "href": "w01/First_Quarto_File.html#article-3-korean-food-trend-overview",
    "title": "First_Quarto_File",
    "section": "Article 3: Korean Food trend overview",
    "text": "Article 3: Korean Food trend overview"
  },
  {
    "objectID": "w01/First_Quarto_File.html#httpstastewise.iofoodtrendskoreandishes",
    "href": "w01/First_Quarto_File.html#httpstastewise.iofoodtrendskoreandishes",
    "title": "First_Quarto_File",
    "section": "(https://tastewise.io/foodtrends/korean#dishes)",
    "text": "(https://tastewise.io/foodtrends/korean#dishes)"
  },
  {
    "objectID": "w03/readme.html",
    "href": "w03/readme.html",
    "title": "DS350_website_template",
    "section": "",
    "text": "What are the top 20 movies and what streaming service offers the most?\nWhat streaming service would be the best for a child?\nWhat streaming service should I buy for the next month that will have the most popular movies?"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DS350 Nuclear Project",
    "section": "",
    "text": "To learn more about Quarto websites visit https://quarto.org/docs/websites.\n\n\nCode\n1 + 1\n\n\n[1] 2"
  },
  {
    "objectID": "project_page.html",
    "href": "project_page.html",
    "title": "project_page",
    "section": "",
    "text": "Here’s an overview …\n\n\nProject 1 Project 2 Project 3"
  },
  {
    "objectID": "case_studies/gapminder1.html",
    "href": "case_studies/gapminder1.html",
    "title": "gapminder1",
    "section": "",
    "text": "Code\nggplot(gapminder2, aes(x = lifeExp, y = gdpPercap, size = pop, color = continent)) + \n  geom_point(alpha = 0.7) +\n  scale_x_continuous(breaks = seq(40, 80, by = 20)) +\n  scale_size_continuous(range = c(1,12), breaks = c(2.5e5, 5e5, 7.5e5, 1e6, 1.25e6)/100) +\n  scale_y_continuous(trans = \"sqrt\", breaks = seq(0, 50000, by = 10000)) +\n  facet_wrap(~ year, nrow=1) +\n  labs(\n    x = \"Life Expectancy\",\n    y = \"GDP per Capita\",\n    size = \"Population (×100k)\",\n    color = \"Continent\") +\n  theme_bw() +\n  theme(\n    legend.position = \"right\",\n#    panel.spacing.x = unit(10, \"cm\")\n  )"
  },
  {
    "objectID": "case_studies.html",
    "href": "case_studies.html",
    "title": "project_page",
    "section": "",
    "text": "Here’s an overview …\n\n\nGapminder Part 1\nGapminder Part 2\nWings to Fly\nCombining Height Files\nWorld Data Investigations\nTake Me Out to the Ballgame\nLeaflet Layers\nDistance Between Savior Names, Part 1\nDistance Between Savior Names, Part 1"
  },
  {
    "objectID": "case_studies/gapminder2.html",
    "href": "case_studies/gapminder2.html",
    "title": "gapminder2",
    "section": "",
    "text": "Code\nggplot(gapminder2, aes(x = year, y = gdpPercap, color = continent)) +\n  geom_line(aes(group = country), alpha = 0.3) +\n    geom_point(aes(size = pop / 100000), alpha = 0.5) + \n  geom_line(data = gap_summary, \n            aes(x = year, y = weighted_gdp),\n            linewidth = 0.7,\n            color = \"black\") +\n  geom_point(data = gap_summary,\n             aes(x = year, y = weighted_gdp, size = total_pop / 100000), color = \"black\") +\n  \n#  geom_point(data = gapminder2,\n#             aes(x = year, y = weighted_gdp, size = total_pop / 100000)) +  # dots sized by continent population\n  facet_wrap(~ continent, nrow = 1) +\n  scale_size_continuous(range = c(.5, 3)) +  # adjust range as needed\n  labs(\n    y = \"GDP per Capita\",\n    size = \"Population (100k)\",\n    color = \"Continent\"\n  ) +\n  theme_minimal()+\n  theme(strip.background = element_rect(fill = \"gray90\", color = \"gray50\"),\n        panel.border = element_rect(color = \"black\", fill = NA, linewidth = 0.5)\n)"
  },
  {
    "objectID": "w04/gun_deaths.html",
    "href": "w04/gun_deaths.html",
    "title": "gun_deaths",
    "section": "",
    "text": "The interactive visualization created by Periscopic on U.S. Gun Deaths in 2018 presents an emotional and data-driven exploration of every gun death that year.\nEach line represents a life cut short, with its length showing the number of years of life lost. After exploring the filters for age, gender, race, and intent, several clear patterns emerge.\nMost gun deaths in 2018 were suicides, particularly among middle-aged white males. In contrast, homicides were more common among younger individuals. The visualization also makes it clear that suicides, though often less publicized, far outnumber homicides in the United States.\nThis data tells a story not only about how people die but also about who they were — and what could have been. Understanding these patterns helps us think about how to target prevention and awareness efforts throughout the year."
  },
  {
    "objectID": "w04/gun_deaths.html#research-question",
    "href": "w04/gun_deaths.html#research-question",
    "title": "gun_deaths",
    "section": "Research Question",
    "text": "Research Question\nWhich demographic and intent-based groups experienced the highest numbers of gun deaths in 2018, and how might these patterns vary by season?\nThis question will guide the following visualizations, which aim to help a public-awareness client plan campaigns emphasizing different groups or issues in different seasons."
  },
  {
    "objectID": "w04/gun_deaths.html#data-loading",
    "href": "w04/gun_deaths.html#data-loading",
    "title": "gun_deaths",
    "section": "Data Loading",
    "text": "Data Loading\n\n\nCode\nlibrary(tidyverse)\ngun_data <- read_csv(\"https://raw.githubusercontent.com/fivethirtyeight/guns-data/master/full_data.csv\")\n\n## glimpse(gun_data)\n\n## head(gun_data)\n\n\nSummary: This dataset contains over 33,000 records representing individual gun deaths in 2018. It includes columns such as age, sex, race, month, intent, and geographic information, allowing for analysis of who was affected and how circumstances vary.\nVisualization 1 — Gun Deaths by Intent\nThis first visualization provides a general overview of the types of gun deaths recorded in 2018.\n\n\nCode\ngun_data %>%\ncount(intent) %>%\nggplot(aes(x = reorder(intent, -n), y = n, fill = intent)) +\ngeom_col(show.legend = FALSE) +\nlabs(title = \"Gun Deaths by Intent (2018)\",\nx = \"Intent\",\ny = \"Number of Deaths\") +\ntheme_minimal() +\ntheme(axis.text.x = element_text(angle = 20, hjust = 1))\n\n\n\n\n\nDescription: This plot shows that suicides account for the vast majority of gun deaths in 2018, followed by homicides. This highlights how prevention efforts could focus more on mental health resources and suicide awareness campaigns, especially targeting high-risk groups.\nVisualization 2 — Gun Deaths by Month\nTo address the client’s request for identifying seasonal emphasis areas, this chart examines monthly variations.\n\n\nCode\ngun_data %>%\ncount(month) %>%\nggplot(aes(x = month, y = n, group = 1)) +\ngeom_line(color = \"steelblue\", size = 1) +\ngeom_point(color = \"steelblue\", size = 2) +\nlabs(title = \"Gun Deaths by Month (2018)\",\nx = \"Month\",\ny = \"Number of Deaths\") +\ntheme_minimal()\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\nDescription: There appears to be a slight increase in gun deaths during the summer months (June–August), suggesting that awareness campaigns focusing on safe conflict resolution and stress management might be most effective during this time. Winter months generally show fewer deaths, possibly due to seasonal behaviors or reduced outdoor activity.\nVisualization 3 — Gun Deaths by Age Group\nNext, we group individuals by age to understand which life stages are most affected.\n\n\nCode\ngun_data %>%\nmutate(age_group = cut(age, breaks = c(0, 20, 40, 60, 80, 100),\nlabels = c(\"0–19\", \"20–39\", \"40–59\", \"60–79\", \"80+\"))) %>%\ncount(age_group) %>%\nggplot(aes(x = age_group, y = n, fill = age_group)) +\ngeom_col(show.legend = FALSE) +\nlabs(title = \"Gun Deaths by Age Group (2018)\",\nx = \"Age Group\",\ny = \"Number of Deaths\") +\ntheme_minimal()\n\n\n\n\n\nDescription: Gun deaths are most common among individuals aged 20–39, followed by those aged 40–59. Younger adults appear to be at the highest risk, indicating that outreach efforts and educational campaigns targeting this demographic could have the greatest impact.\nVisualization 4 — Gun Deaths by Gender and Intent\nFinally, this chart explores gender differences in types of gun deaths.\n\n\nCode\ngun_data %>%\ncount(sex, intent) %>%\nggplot(aes(x = sex, y = n, fill = intent)) +\ngeom_col(position = \"dodge\") +\nlabs(title = \"Gun Deaths by Gender and Intent (2018)\",\nx = \"Gender\",\ny = \"Number of Deaths\",\nfill = \"Intent\") +\ntheme_minimal()\n\n\n\n\n\nDescription: Males represent the overwhelming majority of gun deaths, particularly in suicides. Female deaths occur far less frequently but are more likely to be homicides. This suggests that messaging for male audiences could focus on mental health and suicide prevention, while female-focused campaigns could emphasize domestic violence prevention and safety resources.\nConclusion\nThe 2018 data on gun deaths reveal significant trends across intent, age, gender, and season. Most deaths result from suicide, primarily among men aged 20–59. Seasonal patterns indicate slightly higher rates during the summer months, suggesting an opportunity to time prevention campaigns accordingly.\nBy tailoring campaigns to highlight mental health awareness in men, safety in younger adults, and prevention messaging during the summer, organizations can maximize their impact in reducing gun-related deaths in the U.S."
  },
  {
    "objectID": "w10/string_task.html",
    "href": "w10/string_task.html",
    "title": "string_task",
    "section": "",
    "text": "Code\npacman::p_load(tidyverse, stringr, stringi, rio)\n\n\nQuestion 2 (With the randomletters.txt file, pull out every 1700 letter (for example, 1, 1700, 3400, 5100, …) and find the quote that is hidden—the quote ends with a period.)\n\n\nCode\nchars <- readr::read_file('https://byuistats.github.io/M335/data/randomletters.txt')\ninculde_all <- paste(chars, collapse = \"\")\nbreak_string <- str_split(inculde_all, \"\")[[1]]\n\nposition <- c(1,seq(0, length(break_string), by = 1700))\n\nbreak_string[position]\n\n\n [1] \"t\" \"h\" \"e\" \" \" \"p\" \"l\" \"u\" \"r\" \"a\" \"l\" \" \" \"o\" \"f\" \" \" \"a\" \"n\" \"e\" \"c\" \"d\"\n[20] \"o\" \"t\" \"e\" \" \" \"i\" \"s\" \" \" \"n\" \"o\" \"t\" \" \" \"d\" \"a\" \"t\" \"a\" \".\" \"z\" \" \" \"a\"\n[39] \"n\" \"f\" \"r\" \"a\"\n\n\nQuestion 3 (With the randomletters_wnumbers.txt file, find all the numbers hidden, and convert those numbers to letters using the letters order in the alphabet to decipher the message. For example, a 1=a, 2=b,…, 26=z (Hint: the message starts with “experts”).)\n\n\nCode\nchars_num <- readr::read_lines(\"https://byuistats.github.io/M335/data/randomletters_wnumbers.txt\")\n\nnum_words <- str_extract_all(chars_num, \"[:digit:]+\")[[1]]\nnum_words <- as.integer(num_words)\nnums <- letters[num_words]\npaste(nums, collapse = \"\")\n\n\n[1] \"expertsoftenpossessmoredatathanjudgment\"\n\n\nQuestion 4\nWith the randomletters.txt file, remove all the spaces and periods from the string then find the longest sequence of vowels.\n\n\nCode\ncleaned <- str_remove_all(chars, \"[ \\\\.]\")\nvowel_sequences <- str_extract_all(cleaned, \"[aeiouy]+\")[[1]]\nlongest_vowel_sequence <- vowel_sequences[which.max(nchar(vowel_sequences))]\nlongest_vowel_sequence\n\n\n[1] \"ayeyeya\""
  },
  {
    "objectID": "w10/counting_words_task.html",
    "href": "w10/counting_words_task.html",
    "title": "counting_words_task",
    "section": "",
    "text": "Code\npacman::p_load(readr, tidyverse, stringr, stringi, rio, pander, ggplot2)\n\nscriptures <- read_csv(\"https://github.com/beandog/lds-scriptures/raw/master/csv/lds-scriptures.csv\")\n\n\nRows: 41995 Columns: 19\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (13): volume_title, book_title, volume_long_title, book_long_title, volu...\ndbl  (6): volume_id, book_id, chapter_id, verse_id, chapter_number, verse_nu...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\nCode\nbom_nt <- scriptures %>% \n  filter(volume_title %in% c(\"New Testament\", \"Book of Mormon\")) %>% \n  mutate(\n    num_words = str_count(scripture_text, \" \") + 1\n  )\n\nglimpse(bom_nt)\n\n\nRows: 14,561\nColumns: 20\n$ volume_id          <dbl> 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, …\n$ book_id            <dbl> 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40,…\n$ chapter_id         <dbl> 930, 930, 930, 930, 930, 930, 930, 930, 930, 930, 9…\n$ verse_id           <dbl> 23146, 23147, 23148, 23149, 23150, 23151, 23152, 23…\n$ volume_title       <chr> \"New Testament\", \"New Testament\", \"New Testament\", …\n$ book_title         <chr> \"Matthew\", \"Matthew\", \"Matthew\", \"Matthew\", \"Matthe…\n$ volume_long_title  <chr> \"The New Testament\", \"The New Testament\", \"The New …\n$ book_long_title    <chr> \"The Gospel According to St Matthew\", \"The Gospel A…\n$ volume_subtitle    <chr> \"Of our Lord and Saviour Jesus Christ\", \"Of our Lor…\n$ book_subtitle      <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ volume_short_title <chr> \"NT\", \"NT\", \"NT\", \"NT\", \"NT\", \"NT\", \"NT\", \"NT\", \"NT…\n$ book_short_title   <chr> \"Matt.\", \"Matt.\", \"Matt.\", \"Matt.\", \"Matt.\", \"Matt.…\n$ volume_lds_url     <chr> \"nt\", \"nt\", \"nt\", \"nt\", \"nt\", \"nt\", \"nt\", \"nt\", \"nt…\n$ book_lds_url       <chr> \"matt\", \"matt\", \"matt\", \"matt\", \"matt\", \"matt\", \"ma…\n$ chapter_number     <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ verse_number       <dbl> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, …\n$ scripture_text     <chr> \"The book of the generation of Jesus Christ, the so…\n$ verse_title        <chr> \"Matthew 1:1\", \"Matthew 1:2\", \"Matthew 1:3\", \"Matth…\n$ verse_short_title  <chr> \"Matt. 1:1\", \"Matt. 1:2\", \"Matt. 1:3\", \"Matt. 1:4\",…\n$ num_words          <dbl> 16, 14, 16, 12, 16, 21, 12, 12, 12, 12, 16, 14, 12,…\n\n\n\n\n\n\nCode\n# Average verse length\navg_verse_length <- bom_nt %>%\n  group_by(volume_title) %>%\n  summarise(avg_words = mean(num_words, na.rm = TRUE))\n\npander(avg_verse_length) \n\n\n\n\n\n\n\n\n\nvolume_title\navg_words\n\n\n\n\nBook of Mormon\n40.42\n\n\nNew Testament\n22.67\n\n\n\n\n\n\n\n\n\n\nCode\n# Count \"Jesus\" mentions per volume\njesus_count <- bom_nt %>%\n  mutate(jesus_mentions = str_count(scripture_text, regex(\"Jesus\", ignore_case = TRUE))) %>%\n  group_by(volume_title) %>%\n  summarise(total_mentions = sum(jesus_mentions, na.rm = TRUE))\n\njesus_count\n\n\n# A tibble: 2 × 2\n  volume_title   total_mentions\n  <chr>                   <int>\n1 Book of Mormon            184\n2 New Testament             984\n\n\n\n\n\n\n\nCode\nbom <- bom_nt %>%\n  filter(volume_title == \"Book of Mormon\")\n\n# Optional glimpse of word counts per verse\nbom %>% select(book_title, num_words) %>% glimpse()\n\n\nRows: 6,604\nColumns: 2\n$ book_title <chr> \"1 Nephi\", \"1 Nephi\", \"1 Nephi\", \"1 Nephi\", \"1 Nephi\", \"1 N…\n$ num_words  <dbl> 68, 25, 27, 56, 28, 46, 33, 46, 30, 20, 35, 18, 47, 78, 43,…\n\n\n\n\n\n\n\nCode\n# Visualize\nggplot(bom, aes(x = fct_reorder(book_title, num_words, .fun = median), y = num_words)) +\n  geom_boxplot(fill = \"skyblue\") +\n  labs(\n    title = \"Distribution of Verse Lengths in the Book of Mormon\",\n    x = \"Book\",\n    y = \"Number of Words per Verse\"\n  ) +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))"
  },
  {
    "objectID": "Notes/string_notes.html",
    "href": "Notes/string_notes.html",
    "title": "string_notes",
    "section": "",
    "text": "Code\nchars <- readr::read_lines('https://byuistats.github.io/M335/data/randomletters.txt')\n\nchars_num <- readr::read_lines(\"https://byuistats.github.io/M335/data/randomletters_wnumbers.txt\")\n\n\nTask 1:\n\nWhat is the length of the randomletters string? ﻿﻿\nRemove all the ‘e’ and ‘a’ letters and then tell me how long the string is. and ‘a’ letters ﻿﻿\nHow many times is the name ‘jim’ in the string?\n﻿﻿Tell me which character locations have three “a”’s in a row. ﻿﻿\nShow all the sequences with 3 of the same letter in a row. (hardest one?) ﻿﻿\nSplit the string so that instead of a stzing of length 70000, it is a vector of character strings that eacl have length 1.{r} str_length(chars)\n\n\n\nCode\nstr_extract_all(chars, \"(.)\\\\1{2}\")[[1]]\n\n\n [1] \"nnn\" \"zzz\" \"qqq\" \"ccc\" \"jjj\" \"zzz\" \"...\" \"ooo\" \"vvv\" \"lll\" \"vvv\" \"yyy\"\n[13] \"vvv\" \"xxx\" \"fff\" \"vvv\" \"xxx\" \"lll\" \"yyy\" \"nnn\" \"fff\" \"ccc\" \"kkk\" \"zzz\"\n[25] \"nnn\" \"aaa\" \"kkk\" \"hhh\" \"uuu\" \"ccc\" \"bbb\" \"rrr\" \"qqq\" \"nnn\" \"ccc\" \"nnn\"\n[37] \"kkk\" \"ggg\" \"lll\" \"xxx\" \"ddd\" \"...\" \"ooo\" \"...\" \"ggg\" \"uuu\" \"hhh\" \"...\"\n[49] \"lll\" \"fff\" \"yyy\" \"qqq\" \"ttt\" \"www\" \"xxx\" \"ddd\" \"qqq\" \"...\" \"ggg\" \"iii\"\n[61] \"lll\" \"ggg\" \"hhh\" \"vvv\" \"vvv\" \"vvv\" \"yyy\" \"rrr\" \"rrr\" \"mmm\" \"   \" \"mmm\"\n[73] \"hhh\" \"rrr\" \"zzz\" \"www\" \"iii\" \"   \" \"yyy\" \"rrr\" \"ttt\" \"aaa\" \"mmm\""
  },
  {
    "objectID": "task_manager.html",
    "href": "task_manager.html",
    "title": "project_page",
    "section": "",
    "text": "Project 1\n\n\nCounting Words\nStrings and Regex"
  },
  {
    "objectID": "w10/savior_names.html",
    "href": "w10/savior_names.html",
    "title": "savior_names",
    "section": "",
    "text": "Code\npacman::p_load(readr, tidyverse, stringr, stringi, rio, pander, ggplot2, dplyr)\n\nsavior_names <- read_rds(\"https://byuistats.github.io/M335/data/BoM_SaviorNames.rds\")\nglimpse(savior_names)\n\n\nRows: 112\nColumns: 6\n$ Book          <chr> \"Ether\", \"Mosiah\", \"Mormon\", \"3 Nephi\", \"Mosiah\", \"Mosia…\n$ chapter_verse <chr> \"4:7\", \"3:8\", \"9:29\", \"10:10\", \"7:19\", \"15:4\", \"7:27\", \"…\n$ name          <chr> \"the Father of the heavens and of the earth, and all thi…\n$ reference     <chr> \"Ether 4:7\", \"Mosiah 3:8\", \"Mormon 9:29\", \"3 Nephi 10:10…\n$ nchar         <int> 75, 40, 39, 37, 36, 34, 29, 29, 28, 28, 28, 27, 27, 26, …\n$ words         <int> 16, 7, 8, 6, 7, 6, 6, 6, 6, 6, 5, 4, 5, 5, 5, 4, 5, 5, 5…\n\n\nCode\nscriptures <- read_csv(\"https://github.com/beandog/lds-scriptures/raw/master/csv/lds-scriptures.csv\")\n\nbom_nt <- scriptures %>% \n  filter(volume_title %in% c(\"New Testament\", \"Book of Mormon\")) %>% \n  mutate(num_words = str_count(scripture_text, \" \") + 1)\n\n# Only Book of Mormon verses\nbom_text <- bom_nt %>% filter(volume_title == \"Book of Mormon\")"
  },
  {
    "objectID": "w10/savior_names.html#analysis-of-words-between-savior-names",
    "href": "w10/savior_names.html#analysis-of-words-between-savior-names",
    "title": "savior_names",
    "section": "Analysis of Words Between Savior Names",
    "text": "Analysis of Words Between Savior Names\nThe histogram above shows the distribution of the number of words between mentions of Savior names in the Book of Mormon. Most Savior names occur fairly close together, indicating clusters of references in key chapters. There are occasional long stretches without a Savior name, which contribute to the long tail of the distribution. On average, there are 24.7962406 words between mentions, suggesting that readers encounter references to the Savior regularly throughout the text."
  },
  {
    "objectID": "case_studies/wings_to_fly.html",
    "href": "case_studies/wings_to_fly.html",
    "title": "wings_to_fly",
    "section": "",
    "text": "Code\n# glimpse(flights)\n\n\n\n\nCode\nfl_before_noon <- flights %>%\n  filter(!is.na(sched_dep_time), sched_dep_time < 1200)\n\ndep75 <- fl_before_noon %>%\n  filter(!is.na(dep_delay)) %>%\n  group_by(origin, carrier) %>%\n  summarise(\n    n = n(),\n    p75 = quantile(dep_delay, probs = 0.75, na.rm = TRUE),\n    median = median(dep_delay, na.rm = TRUE),\n    mean = mean(dep_delay, na.rm = TRUE),\n    .groups = \"drop\"\n  ) %>%\n  filter(n >= 50) %>%\n  left_join(airlines, by = \"carrier\") %>%\n  arrange(origin, p75)\n\nbest_by_origin <- dep75 %>%\n  group_by(origin) %>%\n  slice_min(order_by = p75, n = 1, with_ties = FALSE) %>%\n  ungroup()\n\n\n# dep75 %>% print(n = 200)\n# best_by_origin\n\ntop_carriers <- fl_before_noon %>%\n  count(origin, carrier) %>%\n  group_by(origin) %>%\n  slice_max(n, n = 8) %>%\n  ungroup() %>%\n  distinct(origin, carrier)\n\nplot_data <- fl_before_noon %>%\n  inner_join(top_carriers, by = c(\"origin\", \"carrier\")) %>%\n  left_join(airlines, by = \"carrier\")\n\n\n\n\nCode\np1 <- ggplot(plot_data, aes(x = fct_reorder(name, dep_delay, .fun = median, .desc = FALSE),\n                            y = dep_delay)) +\n  geom_boxplot(outlier.shape = NA, alpha = 0.6) +\n  geom_jitter(width = 0.25, alpha = 0.12, size = 0.6) +\n  facet_wrap(~origin, scales = \"free_x\") +\n  labs(x = \"Airline\", y = \"Departure delay (minutes)\",\n       title = \"Departure delays for flights scheduled before noon (jitter = individual flights)\") +\n  coord_flip() +\n  theme_minimal()\n\nprint(p1)\n\n\n\n\n\n\n\nCode\np2 <- ggplot(dep75, aes(x = reorder(name, p75), y = p75, fill = origin)) +\n  geom_col() +\n  facet_wrap(~origin, scales = \"free\") +\n  geom_text(aes(label = paste0(\"n=\", n)), hjust = -0.05, size = 3) +\n  labs(x = \"Airline\", y = \"75th percentile of departure delay (minutes)\",\n       title = \"75th percentile of departure delay for flights scheduled before noon\") +\n  coord_flip() +\n  theme_minimal()\n\nprint(p2)\n\n\n\n\n\n\n\nCode\ndelta_flights <- flights %>%\n  filter(carrier == \"DL\") %>%\n  filter(!is.na(arr_delay), !is.na(origin))\n\ndelta_late_summary <- delta_flights %>%\n  group_by(origin) %>%\n  summarise(\n    n = n(),\n    prop_any_delay = mean(arr_delay > 0, na.rm = TRUE),\n    prop_15plus = mean(arr_delay > 15, na.rm = TRUE),\n    avg_delay = mean(arr_delay, na.rm = TRUE),\n    median_delay = median(arr_delay, na.rm = TRUE),\n    p75 = quantile(arr_delay, 0.75, na.rm = TRUE),\n    .groups = \"drop\"\n  ) %>%\n  arrange(prop_any_delay)\n\npander(delta_late_summary)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\norigin\nn\nprop_any_delay\nprop_15plus\navg_delay\nmedian_delay\np75\n\n\n\n\nJFK\n20559\n0.309\n0.1662\n-2.379\n-11\n5\n\n\nLGA\n22804\n0.3655\n0.1926\n3.928\n-7\n9\n\n\nEWR\n4295\n0.4016\n0.2049\n8.78\n-4\n10\n\n\n\n\n\nCode\n# Q2 plots: ECDF (shows individual-flight complexity with rugs) and bar chart of proportions\np3 <- ggplot(delta_flights, aes(x = arr_delay, colour = origin)) +\n  stat_ecdf(size = 1) +\n  geom_jitter(aes(y = 0), height = 0.02, alpha = 0.2, show.legend = FALSE) +\n  labs(x = \"Arrival delay (minutes)\", y = \"ECDF\",\n       title = \"Delta (DL) arrival delay ECDF by origin\",\n       subtitle = \"Rugs show individual flights\") +\n  xlim(-60, 180) +\n  theme_minimal()\n\n\nprint(p3)\n\n\n\n\n\n\n\nCode\np4 <- delta_late_summary %>%\n  pivot_longer(cols = c(prop_any_delay, prop_15plus),\n               names_to = \"threshold\", values_to = \"proportion\") %>%\n  mutate(threshold = recode(threshold,\n                            prop_any_delay = \"Any delay (>0 min)\",\n                            prop_15plus = \"Significant delay (>15 min)\")) %>%\n  ggplot(aes(x = origin, y = proportion, fill = origin)) +\n  geom_col() +\n  geom_text(aes(label = scales::percent(proportion, accuracy = 0.1)), vjust = -0.5) +\n  facet_wrap(~threshold) +\n  labs(y = \"Proportion of flights late\", title = \"Proportion of Delta flights arriving late by origin\") +\n  theme_minimal()\nprint(p4)"
  },
  {
    "objectID": "w05/stock_task.html",
    "href": "w05/stock_task.html",
    "title": "stock_task",
    "section": "",
    "text": "Code\nstock_raw <- read_csv(\n  \"https://raw.githubusercontent.com/byuistats/data/master/Dart_Expert_Dow_6month_anova/Dart_Expert_Dow_6month_anova.csv\"\n) %>%\n  clean_names()\n\n\nRows: 300 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): contest_period, variable\ndbl (1): value\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\n# glimpse(stock_raw)\n\n\n\n\nCode\nstock_tidy <- stock_raw %>%\n  clean_names() %>%\n  mutate(\n    month_end = str_extract(contest_period, \"^[A-Za-z]+\"),\n    year_end = str_extract(contest_period, \"[0-9]{4}\") %>% as.integer()\n  )\n\n# glimpse(stock_tidy)\n\n\n\n\nCode\nstock_tidy <- stock_tidy %>%\n  mutate(\n    month_end = str_sub(month_end, 1, 3),\n    month_end = factor(month_end,\n         levels = c(\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\",\"Jun\",\"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\")))\nwrite_rds(stock_tidy, \"stock_tidy.rds\")\n\ntest_read <- read_rds(\"stock_tidy.rds\")\n# glimpse(test_read)\n\n\n\n\nCode\ndjia_table <- stock_tidy %>%\n  filter(variable == \"DJIA\") %>%\n  select(month_end, year_end, value) %>%\n  pivot_wider(\n    names_from = year_end,\n    values_from = value\n  ) %>%\n  arrange(month_end)   # ensures Jan → Dec in rows\n\n\npander(djia_table)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmonth_end\n1990\n1991\n1992\n1993\n1994\n1995\n1996\n1997\n1998\n\n\n\n\nJan\n2.5\n17.7\n3.6\n7.7\n-6.2\n16\n10.2\n16.2\n15\n\n\nFeb\n11.5\n7.6\n4.2\n3.7\n-5.3\n19.6\n1.3\n20.8\n7.1\n\n\nMar\n-2.3\n4.4\n-0.3\n7.3\n1.5\n15.3\n0.6\n8.3\n-13.1\n\n\nApr\n-9.2\n3.4\n-0.1\n5.2\n4.4\n14\n5.8\n20.2\n-11.8\n\n\nMay\n-8.5\n4.4\n-5\n5.7\n6.9\n8.2\n7.2\n3\nNA\n\n\nJun\n-12.8\n-3.3\n-2.8\n4.9\n-0.3\n13.1\n15.1\n3.8\nNA\n\n\nJul\n-9.3\n6.6\n0.2\n8\n3.6\n9.3\n15.5\n-0.7\nNA\n\n\nAug\n-0.8\n6.5\n-0.8\n11.2\n1.8\n15\n19.6\n-0.3\nNA\n\n\nSep\n11\n8.6\n2.5\n5.5\n3.2\n15.6\n20.1\n10.7\nNA\n\n\nOct\n15.8\n7.2\n9\n1.6\n7.3\n18.4\n9.6\n7.6\nNA\n\n\nNov\n16.2\n10.6\n5.8\n0.5\n12.8\n14.8\n15.3\n22.5\nNA\n\n\nDec\n17.3\n17.6\n6.7\n1.3\n19.5\n9\n13.3\n10.6\nNA"
  },
  {
    "objectID": "case_studies/height_files.html",
    "href": "case_studies/height_files.html",
    "title": "height_flies",
    "section": "",
    "text": "Code\nbavaria_19th <- read_dta(\"data/germanconscr.dta\")\n\nbls <- read_dta(\"data/germanprison.dta\") \n\nsoldiers <- read.dbf(\"../case_studies/data/B6090.DBF\", as.is = TRUE)\n\nDart_Expert <- read_csv(\"../case_studies/data/Dart_Expert.csv\")\n\n# bls_height <- read_csv(\"https://raw.githubusercontent.com/hadley/r4ds/main/data/heights.csv\") %>%\n# mutate(birth_year = 1950) # mid-20th century\n\nwisconsin <- read_sav(\"data/main05022005.sav\")\n\n\n\n\nCode\n# head(soldiers)\n# str(soldiers)\n\n\n\n\nCode\nbavaria_clean <- bavaria_19th %>%\nmutate(\n  birth_year = bdec,\n  height.cm = height,\n  height.in = height / 2.54,\n  study = \"Bavarian Conscripts 19th Century\"\n) %>%\n  select(birth_year, height.cm, height.in, study)\n\n\n\n\nCode\nbls_clean <- bls %>%\nmutate(\n  birth_year = bdec,\n  height.cm = height,\n  height.in = height / 2.54,\n  study = \"Bureau of Labor Statistics\"\n) %>%\n  select(birth_year, height.cm, height.in, study)\n\n\n\n\nCode\nsoldiers_clean <- soldiers %>%\nmutate(\n  birth_year = GEBJZ, \n  height.cm = CMETER,\n  height.in = CMETER / 2.54,\n  study = \"18th Century German Soldiers\"\n) %>%\n  select(birth_year, height.cm, height.in, study)\n\n\n\n\nCode\nwisconsin_clean <- wisconsin %>%\nmutate(\n  birth_year = 18 + DOBY, \n  height.cm = RT216F * 2.54,\n  height.in = RT216F,\n  study = \"Wisconsin Survey\"\n) %>% \n  select(birth_year, height.cm, height.in, study)\n\n\n\n\nCode\nall_data <- bind_rows(\n  bavaria_clean,\n  bls_clean,\n  soldiers_clean,\n  wisconsin_clean\n) %>% \n  filter(!is.na(birth_year)) %>%\n  filter(birth_year > 1700, height.in > 45, height.in < 85)\n\n\n\n\nCode\nggplot(all_data, aes(birth_year, height.in)) +\n  geom_point(alpha = 0.1) +\n  geom_smooth(method = \"gam\", formula = y ~ s(x, k = 3),   # chat gave me this to add in the line\n    se = FALSE,\n    color = \"red\",\n    linewidth = 1.2\n  ) +\n  labs(\n    title = \"Trend in Male Height\",\n    x = \"Birth Year\",\n    y = \"Height (inches)\"\n  ) +\n  theme_minimal(base_size = 14)\n\n\n\n\n\n\nInterpretation and Conclusion\nTo create my tidy dataset, I combined four separate historical height datasets—Bavarian conscripts, German soldiers, BLS prisoners, and the Wisconsin survey. Each source used different variable names and measurement units. I standardized everything into consistent columns for birth year, height in centimeters, height in inches, and a study label. After binding the datasets together, I removed rows with missing birth years and excluded impossible height values such as 0 inches, which indicated coding errors. I also filtered the data to include people born after 1700, resulting in a clean dataset focused on the primary historical window of interest.\nCompared to the previous height project using the worldwide historical dataset, the story told here is very different. The global data showed a clear long-term increase in average human height over many decades and across many populations. However, the datasets in this assignment show almost no change. Instead of a smooth timeline, the birth years appear as isolated clusters—1850, 1860, 1870, and a few others—making the smoothed trend essentially flat. At first glance this seems to contradict the earlier finding, but the contradiction disappears once we recognize that these datasets cover only a narrow range of years and represent very specific subpopulations, not broad national averages.\nGiven this, I would respond to the claim that “humans are getting taller over time” by saying that the long-term trend is real, but my combined dataset here is not designed to show it. The global dataset demonstrates height increases over centuries, but the smaller, clustered datasets used in this task simply do not include the continuous historical coverage needed to reveal that pattern. My final conclusion is that humans have generally gotten taller over the long run, but based on the data in this assignment, we can only say that within the limited birth years represented here, height remained essentially unchanged."
  },
  {
    "objectID": "week_one.html",
    "href": "week_one.html",
    "title": "project_page",
    "section": "",
    "text": "Case Study 1 Case Study 2"
  },
  {
    "objectID": "w05/country_height.html",
    "href": "w05/country_height.html",
    "title": "country_height",
    "section": "",
    "text": "Code\nggplot(heights, aes(x = year_decade, y = height_in, group = country)) +\n  geom_line(data = heights %>% filter(country != \"Germany\"), color = \"grey80\") + # all other countries muted\n  geom_line(data = heights %>% filter(country == \"Germany\"), color = \"red\", size = 1.2) + # Germany bold\n  geom_point(data = heights %>% filter(country == \"Germany\"), color = \"red\", size = 2) +\n  labs(\n    title = \"Average Heights Over Time by Country\",\n    x = \"Year / Decade\",\n    y = \"Height (inches)\"\n  ) +\n  theme_minimal()\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\nBased on this dataset, there is a pretty clear upward trend in average human height over time. Even though the countries start at different baseline heights, most of them show gradual increases across the decades. The pattern is very noticeable when you look at the highlighted Germany line, which steadily rises over the 19th and 20th centuries. So if someone argues that humans have gotten taller over the years, this data definitely supports that idea."
  },
  {
    "objectID": "weeks/week_ten.html",
    "href": "weeks/week_ten.html",
    "title": "project_page",
    "section": "",
    "text": "Counting Words\nSavior Names\nString Task"
  },
  {
    "objectID": "weeks/week_four.html",
    "href": "weeks/week_four.html",
    "title": "project_page",
    "section": "",
    "text": "Country Height\nStock Task"
  },
  {
    "objectID": "weeks/week_eleven.html",
    "href": "weeks/week_eleven.html",
    "title": "project_page",
    "section": "",
    "text": "First Task"
  },
  {
    "objectID": "weeks/week_two.html",
    "href": "weeks/week_two.html",
    "title": "project_page",
    "section": "",
    "text": "First Task"
  },
  {
    "objectID": "weeks/week_six.html",
    "href": "weeks/week_six.html",
    "title": "project_page",
    "section": "",
    "text": "First Task"
  },
  {
    "objectID": "weeks/week_three.html",
    "href": "weeks/week_three.html",
    "title": "project_page",
    "section": "",
    "text": "Gun Deaths\nWings to Fly"
  },
  {
    "objectID": "weeks/week_seven.html",
    "href": "weeks/week_seven.html",
    "title": "project_page",
    "section": "",
    "text": "First Task"
  },
  {
    "objectID": "weeks/week_twelve.html",
    "href": "weeks/week_twelve.html",
    "title": "project_page",
    "section": "",
    "text": "First Task"
  },
  {
    "objectID": "weeks/week_nine.html",
    "href": "weeks/week_nine.html",
    "title": "project_page",
    "section": "",
    "text": "First Task"
  },
  {
    "objectID": "weeks/week_eight.html",
    "href": "weeks/week_eight.html",
    "title": "project_page",
    "section": "",
    "text": "First Task"
  },
  {
    "objectID": "weeks/week_one.html",
    "href": "weeks/week_one.html",
    "title": "project_page",
    "section": "",
    "text": "First Task"
  },
  {
    "objectID": "weeks/week_five.html",
    "href": "weeks/week_five.html",
    "title": "project_page",
    "section": "",
    "text": "Polish Visualization\nVisualization for Presentation"
  },
  {
    "objectID": "weeks/w03/readme.html",
    "href": "weeks/w03/readme.html",
    "title": "DS350_website_template",
    "section": "",
    "text": "What are the top 20 movies and what streaming service offers the most?\nWhat streaming service would be the best for a child?\nWhat streaming service should I buy for the next month that will have the most popular movies?"
  },
  {
    "objectID": "weeks/w04/gun_deaths.html",
    "href": "weeks/w04/gun_deaths.html",
    "title": "gun_deaths",
    "section": "",
    "text": "The interactive visualization created by Periscopic on U.S. Gun Deaths in 2018 presents an emotional and data-driven exploration of every gun death that year.\nEach line represents a life cut short, with its length showing the number of years of life lost. After exploring the filters for age, gender, race, and intent, several clear patterns emerge.\nMost gun deaths in 2018 were suicides, particularly among middle-aged white males. In contrast, homicides were more common among younger individuals. The visualization also makes it clear that suicides, though often less publicized, far outnumber homicides in the United States.\nThis data tells a story not only about how people die but also about who they were — and what could have been. Understanding these patterns helps us think about how to target prevention and awareness efforts throughout the year."
  },
  {
    "objectID": "weeks/w04/gun_deaths.html#research-question",
    "href": "weeks/w04/gun_deaths.html#research-question",
    "title": "gun_deaths",
    "section": "Research Question",
    "text": "Research Question\nWhich demographic and intent-based groups experienced the highest numbers of gun deaths in 2018, and how might these patterns vary by season?\nThis question will guide the following visualizations, which aim to help a public-awareness client plan campaigns emphasizing different groups or issues in different seasons."
  },
  {
    "objectID": "weeks/w04/gun_deaths.html#data-loading",
    "href": "weeks/w04/gun_deaths.html#data-loading",
    "title": "gun_deaths",
    "section": "Data Loading",
    "text": "Data Loading\n\n\nCode\nlibrary(tidyverse)\ngun_data <- read_csv(\"https://raw.githubusercontent.com/fivethirtyeight/guns-data/master/full_data.csv\")\n\n## glimpse(gun_data)\n\n## head(gun_data)\n\n\nSummary: This dataset contains over 33,000 records representing individual gun deaths in 2018. It includes columns such as age, sex, race, month, intent, and geographic information, allowing for analysis of who was affected and how circumstances vary.\nVisualization 1 — Gun Deaths by Intent\nThis first visualization provides a general overview of the types of gun deaths recorded in 2018.\n\n\nCode\ngun_data %>%\ncount(intent) %>%\nggplot(aes(x = reorder(intent, -n), y = n, fill = intent)) +\ngeom_col(show.legend = FALSE) +\nlabs(title = \"Gun Deaths by Intent (2018)\",\nx = \"Intent\",\ny = \"Number of Deaths\") +\ntheme_minimal() +\ntheme(axis.text.x = element_text(angle = 20, hjust = 1))\n\n\n\n\n\nDescription: This plot shows that suicides account for the vast majority of gun deaths in 2018, followed by homicides. This highlights how prevention efforts could focus more on mental health resources and suicide awareness campaigns, especially targeting high-risk groups.\nVisualization 2 — Gun Deaths by Month\nTo address the client’s request for identifying seasonal emphasis areas, this chart examines monthly variations.\n\n\nCode\ngun_data %>%\ncount(month) %>%\nggplot(aes(x = month, y = n, group = 1)) +\ngeom_line(color = \"steelblue\", size = 1) +\ngeom_point(color = \"steelblue\", size = 2) +\nlabs(title = \"Gun Deaths by Month (2018)\",\nx = \"Month\",\ny = \"Number of Deaths\") +\ntheme_minimal()\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\nDescription: There appears to be a slight increase in gun deaths during the summer months (June–August), suggesting that awareness campaigns focusing on safe conflict resolution and stress management might be most effective during this time. Winter months generally show fewer deaths, possibly due to seasonal behaviors or reduced outdoor activity.\nVisualization 3 — Gun Deaths by Age Group\nNext, we group individuals by age to understand which life stages are most affected.\n\n\nCode\ngun_data %>%\nmutate(age_group = cut(age, breaks = c(0, 20, 40, 60, 80, 100),\nlabels = c(\"0–19\", \"20–39\", \"40–59\", \"60–79\", \"80+\"))) %>%\ncount(age_group) %>%\nggplot(aes(x = age_group, y = n, fill = age_group)) +\ngeom_col(show.legend = FALSE) +\nlabs(title = \"Gun Deaths by Age Group (2018)\",\nx = \"Age Group\",\ny = \"Number of Deaths\") +\ntheme_minimal()\n\n\n\n\n\nDescription: Gun deaths are most common among individuals aged 20–39, followed by those aged 40–59. Younger adults appear to be at the highest risk, indicating that outreach efforts and educational campaigns targeting this demographic could have the greatest impact.\nVisualization 4 — Gun Deaths by Gender and Intent\nFinally, this chart explores gender differences in types of gun deaths.\n\n\nCode\ngun_data %>%\ncount(sex, intent) %>%\nggplot(aes(x = sex, y = n, fill = intent)) +\ngeom_col(position = \"dodge\") +\nlabs(title = \"Gun Deaths by Gender and Intent (2018)\",\nx = \"Gender\",\ny = \"Number of Deaths\",\nfill = \"Intent\") +\ntheme_minimal()\n\n\n\n\n\nDescription: Males represent the overwhelming majority of gun deaths, particularly in suicides. Female deaths occur far less frequently but are more likely to be homicides. This suggests that messaging for male audiences could focus on mental health and suicide prevention, while female-focused campaigns could emphasize domestic violence prevention and safety resources.\nConclusion\nThe 2018 data on gun deaths reveal significant trends across intent, age, gender, and season. Most deaths result from suicide, primarily among men aged 20–59. Seasonal patterns indicate slightly higher rates during the summer months, suggesting an opportunity to time prevention campaigns accordingly.\nBy tailoring campaigns to highlight mental health awareness in men, safety in younger adults, and prevention messaging during the summer, organizations can maximize their impact in reducing gun-related deaths in the U.S."
  },
  {
    "objectID": "weeks/w05/stock_task.html",
    "href": "weeks/w05/stock_task.html",
    "title": "stock_task",
    "section": "",
    "text": "Code\nstock_raw <- read_csv(\n  \"https://raw.githubusercontent.com/byuistats/data/master/Dart_Expert_Dow_6month_anova/Dart_Expert_Dow_6month_anova.csv\"\n) %>%\n  clean_names()\n\n\nRows: 300 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): contest_period, variable\ndbl (1): value\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\n# pander(stock_raw)\n\n\n\n\nCode\nstock_tidy <- stock_raw %>%\n  clean_names() %>%\n  mutate(\n    month_end = str_extract(contest_period, \"^[A-Za-z]+\"),\n    year_end = str_extract(contest_period, \"[0-9]{4}\") %>% as.integer()\n  )\n\npander(stock_tidy)\n\n\n\n\n\n\n\n\n\n\n\n\ncontest_period\nvariable\nvalue\nmonth_end\nyear_end\n\n\n\n\nJanuary-June1990\nPROS\n12.7\nJanuary\n1990\n\n\nFebruary-July1990\nPROS\n26.4\nFebruary\n1990\n\n\nMarch-August1990\nPROS\n2.5\nMarch\n1990\n\n\nApril-September1990\nPROS\n-20\nApril\n1990\n\n\nMay-October1990\nPROS\n-37.8\nMay\n1990\n\n\nJune-November1990\nPROS\n-33.3\nJune\n1990\n\n\nJuly-December1990\nPROS\n-10.2\nJuly\n1990\n\n\nAugust1990-January1991\nPROS\n-20.3\nAugust\n1990\n\n\nSeptember1990-February1991\nPROS\n38.9\nSeptember\n1990\n\n\nOctober1990-March1991\nPROS\n20.2\nOctober\n1990\n\n\nNovember1990-April1991\nPROS\n50.6\nNovember\n1990\n\n\nDecember1990-May1991\nPROS\n66.9\nDecember\n1990\n\n\nJanuary-June1991\nPROS\n7.5\nJanuary\n1991\n\n\nFebruary-July1991\nPROS\n17.5\nFebruary\n1991\n\n\nMarch-August1991\nPROS\n39.6\nMarch\n1991\n\n\nApril-September1991\nPROS\n15.6\nApril\n1991\n\n\nMay-October1991\nPROS\n12.4\nMay\n1991\n\n\nJune-November1991\nPROS\n3\nJune\n1991\n\n\nJuly-December1991\nPROS\n12.3\nJuly\n1991\n\n\nAugust1991-January1992\nPROS\n39.3\nAugust\n1991\n\n\nSeptember1991-February1992\nPROS\n51.2\nSeptember\n1991\n\n\nOctober1991-March1992\nPROS\n25.2\nOctober\n1991\n\n\nNovember1991-April1992\nPROS\n-3.3\nNovember\n1991\n\n\nDecember1991-May1992\nPROS\n7.7\nDecember\n1991\n\n\nJanuary-June1992\nPROS\n-21\nJanuary\n1992\n\n\nFebruary-July1992\nPROS\n-13\nFebruary\n1992\n\n\nMarch-August1992\nPROS\n-2.5\nMarch\n1992\n\n\nApril-September1992\nPROS\n-19.6\nApril\n1992\n\n\nMay-October1992\nPROS\n6.3\nMay\n1992\n\n\nJune-November1992\nPROS\n-5.1\nJune\n1992\n\n\nJuly-December1992\nPROS\n14.1\nJuly\n1992\n\n\nAugust1992-January1993\nPROS\n15.6\nAugust\n1992\n\n\nSeptember1992-February1993\nPROS\n-26.7\nSeptember\n1992\n\n\nOctober1992-March1993\nPROS\n25.2\nOctober\n1992\n\n\nNovember1992-April1993\nPROS\n-13.9\nNovember\n1992\n\n\nDecember1992-May1993\nPROS\n27.9\nDecember\n1992\n\n\nJanuary1993-June1993\nPROS\n-6.6\nJanuary\n1993\n\n\nFebruary1993-July1993\nPROS\n29.1\nFebruary\n1993\n\n\nMarch1993-August1993\nPROS\n0.3\nMarch\n1993\n\n\nApril1993-September1993\nPROS\n2.6\nApril\n1993\n\n\nMay1993-October1993\nPROS\n5\nMay\n1993\n\n\nJune1993-November1993\nPROS\n-7.4\nJune\n1993\n\n\nJuly1993-Dec.1993\nPROS\n2.2\nJuly\n1993\n\n\nAugust1993-January1994\nPROS\n27.8\nAugust\n1993\n\n\nSeptember1993-February1994\nPROS\n3.7\nSeptember\n1993\n\n\nOctober1993-March1994\nPROS\n4.7\nOctober\n1993\n\n\nNovember1993-April1994\nPROS\n5.4\nNovember\n1993\n\n\nDecember1993-May1994\nPROS\n-9.5\nDecember\n1993\n\n\nJanuary-June1994\nPROS\n-13.1\nJanuary\n1994\n\n\nFebruary-July1994\nPROS\n-10\nFebruary\n1994\n\n\nMarch-August1994\nPROS\n28.4\nMarch\n1994\n\n\nApril-September1994\nPROS\n10.6\nApril\n1994\n\n\nMay-October1994\nPROS\n27.2\nMay\n1994\n\n\nJune-November1994\nPROS\n37\nJune\n1994\n\n\nJuly-December1994\nPROS\n-15.8\nJuly\n1994\n\n\nAugust1994-January1995\nPROS\n20.4\nAugust\n1994\n\n\nSeptember1994-Febuary1995\nPROS\n5.4\nSeptember\n1994\n\n\nOctober1994-March1995\nPROS\n-14.8\nOctober\n1994\n\n\nNovember1994-April1995\nPROS\n12.1\nNovember\n1994\n\n\nDecember1994-May1995\nPROS\n10.8\nDecember\n1994\n\n\nJanuary1995-June1995\nPROS\n72.7\nJanuary\n1995\n\n\nFebuary-July1995\nPROS\n30.5\nFebuary\n1995\n\n\nMarch-August1995\nPROS\n26.7\nMarch\n1995\n\n\nApril1995-September1995\nPROS\n75\nApril\n1995\n\n\nMay-October1995\nPROS\n12.6\nMay\n1995\n\n\nJune-November1995\nPROS\n31\nJune\n1995\n\n\nJuly1995-December1995\nPROS\n-11\nJuly\n1995\n\n\nAugust1995-January1996\nPROS\n28.1\nAugust\n1995\n\n\nSeptember1995-February1996\nPROS\n15.1\nSeptember\n1995\n\n\nOctober1995-March1996\nPROS\n1.5\nOctober\n1995\n\n\nNovember1995-April1996\nPROS\n10.8\nNovember\n1995\n\n\nDecember1995-May1996\nPROS\n2\nDecember\n1995\n\n\nJanuary-June1996\nPROS\n-9.2\nJanuary\n1996\n\n\nFebruary-July1996\nPROS\n-8.6\nFebruary\n1996\n\n\nMarch-August1996\nPROS\n31.7\nMarch\n1996\n\n\nApril-September1996\nPROS\n8.7\nApril\n1996\n\n\nMay-October1996\nPROS\n7\nMay\n1996\n\n\nJune-November1996\nPROS\n5.1\nJune\n1996\n\n\nJuly-December1996\nPROS\n41.2\nJuly\n1996\n\n\nAugust1996-January1997\nPROS\n7.7\nAugust\n1996\n\n\nSeptember1996-February1997\nPROS\n47.6\nSeptember\n1996\n\n\nOctober1996-March1997\nPROS\n-10\nOctober\n1996\n\n\nNovember1996-April1997\nPROS\n-13.6\nNovember\n1996\n\n\nDecember1996-May1997\nPROS\n10.5\nDecember\n1996\n\n\nJanuary-June1997\nPROS\n20.2\nJanuary\n1997\n\n\nFebruary-July1997\nPROS\n29.3\nFebruary\n1997\n\n\nMarch-August1997\nPROS\n20.7\nMarch\n1997\n\n\nApril-September1997\nPROS\n50.3\nApril\n1997\n\n\nMay-October1997\nPROS\n38.4\nMay\n1997\n\n\nJune-November1997\nPROS\n-3.5\nJune\n1997\n\n\nJuly-December1997\nPROS\n-14.1\nJuly\n1997\n\n\nAugust1997-January1998\nPROS\n14.3\nAugust\n1997\n\n\nSeptember1997-February1998\nPROS\n10.9\nSeptember\n1997\n\n\nOctober1997-March1998\nPROS\n5.5\nOctober\n1997\n\n\nNovember1997-April1998\nPROS\n17.4\nNovember\n1997\n\n\nDecember1997-May1998\nPROS\n0\nDecember\n1997\n\n\nJanuary-June1998\nPROS\n24.4\nJanuary\n1998\n\n\nFebruary-July1998\nPROS\n39.3\nFebruary\n1998\n\n\nMarch-August1998\nPROS\n-18.8\nMarch\n1998\n\n\nApril-September1998\nPROS\n-20.1\nApril\n1998\n\n\nJanuary-June1990\nDARTS\n0\nJanuary\n1990\n\n\nFebruary-July1990\nDARTS\n1.8\nFebruary\n1990\n\n\nMarch-August1990\nDARTS\n-14.3\nMarch\n1990\n\n\nApril-September1990\nDARTS\n-7.2\nApril\n1990\n\n\nMay-October1990\nDARTS\n-16.3\nMay\n1990\n\n\nJune-November1990\nDARTS\n-27.4\nJune\n1990\n\n\nJuly-December1990\nDARTS\n-22.5\nJuly\n1990\n\n\nAugust1990-January1991\nDARTS\n-37.3\nAugust\n1990\n\n\nSeptember1990-February1991\nDARTS\n-2.5\nSeptember\n1990\n\n\nOctober1990-March1991\nDARTS\n11.2\nOctober\n1990\n\n\nNovember1990-April1991\nDARTS\n72.9\nNovember\n1990\n\n\nDecember1990-May1991\nDARTS\n16.6\nDecember\n1990\n\n\nJanuary-June1991\nDARTS\n28.7\nJanuary\n1991\n\n\nFebruary-July1991\nDARTS\n44.8\nFebruary\n1991\n\n\nMarch-August1991\nDARTS\n71.3\nMarch\n1991\n\n\nApril-September1991\nDARTS\n2.8\nApril\n1991\n\n\nMay-October1991\nDARTS\n38\nMay\n1991\n\n\nJune-November1991\nDARTS\n-23.2\nJune\n1991\n\n\nJuly-December1991\nDARTS\n4.1\nJuly\n1991\n\n\nAugust1991-January1992\nDARTS\n-14\nAugust\n1991\n\n\nSeptember1991-February1992\nDARTS\n11.7\nSeptember\n1991\n\n\nOctober1991-March1992\nDARTS\n1.1\nOctober\n1991\n\n\nNovember1991-April1992\nDARTS\n-3.1\nNovember\n1991\n\n\nDecember1991-May1992\nDARTS\n-1.4\nDecember\n1991\n\n\nJanuary-June1992\nDARTS\n7.7\nJanuary\n1992\n\n\nFebruary-July1992\nDARTS\n15.4\nFebruary\n1992\n\n\nMarch-August1992\nDARTS\n3.6\nMarch\n1992\n\n\nApril-September1992\nDARTS\n5.7\nApril\n1992\n\n\nMay-October1992\nDARTS\n-5.7\nMay\n1992\n\n\nJune-November1992\nDARTS\n6.9\nJune\n1992\n\n\nJuly-December1992\nDARTS\n1.8\nJuly\n1992\n\n\nAugust1992-January1993\nDARTS\n-13.9\nAugust\n1992\n\n\nSeptember1992-February1993\nDARTS\n15.6\nSeptember\n1992\n\n\nOctober1992-March1993\nDARTS\n18.7\nOctober\n1992\n\n\nNovember1992-April1993\nDARTS\n-3.6\nNovember\n1992\n\n\nDecember1992-May1993\nDARTS\n6.6\nDecember\n1992\n\n\nJanuary1993-June1993\nDARTS\n4.7\nJanuary\n1993\n\n\nFebruary1993-July1993\nDARTS\n-43\nFebruary\n1993\n\n\nMarch1993-August1993\nDARTS\n-5.6\nMarch\n1993\n\n\nApril1993-September1993\nDARTS\n-17.7\nApril\n1993\n\n\nMay1993-October1993\nDARTS\n-4.9\nMay\n1993\n\n\nJune1993-November1993\nDARTS\n-21.4\nJune\n1993\n\n\nJuly1993-Dec.1993\nDARTS\n42.2\nJuly\n1993\n\n\nAugust1993-January1994\nDARTS\n18.5\nAugust\n1993\n\n\nSeptember1993-February1994\nDARTS\n1.5\nSeptember\n1993\n\n\nOctober1993-March1994\nDARTS\n-9.2\nOctober\n1993\n\n\nNovember1993-April1994\nDARTS\n-10.5\nNovember\n1993\n\n\nDecember1993-May1994\nDARTS\n1.4\nDecember\n1993\n\n\nJanuary-June1994\nDARTS\n-8.7\nJanuary\n1994\n\n\nFebruary-July1994\nDARTS\n16.9\nFebruary\n1994\n\n\nMarch-August1994\nDARTS\n-4.3\nMarch\n1994\n\n\nApril-September1994\nDARTS\n20.6\nApril\n1994\n\n\nMay-October1994\nDARTS\n10.3\nMay\n1994\n\n\nJune-November1994\nDARTS\n-6.8\nJune\n1994\n\n\nJuly-December1994\nDARTS\n5.3\nJuly\n1994\n\n\nAugust1994-January1995\nDARTS\n2.8\nAugust\n1994\n\n\nSeptember1994-Febuary1995\nDARTS\n11.9\nSeptember\n1994\n\n\nOctober1994-March1995\nDARTS\n3.8\nOctober\n1994\n\n\nNovember1994-April1995\nDARTS\n1.4\nNovember\n1994\n\n\nDecember1994-May1995\nDARTS\n9\nDecember\n1994\n\n\nJanuary1995-June1995\nDARTS\n11.8\nJanuary\n1995\n\n\nFebuary-July1995\nDARTS\n16.5\nFebuary\n1995\n\n\nMarch-August1995\nDARTS\n11.4\nMarch\n1995\n\n\nApril1995-September1995\nDARTS\n3.3\nApril\n1995\n\n\nMay-October1995\nDARTS\n17.6\nMay\n1995\n\n\nJune-November1995\nDARTS\n23.8\nJune\n1995\n\n\nJuly1995-December1995\nDARTS\n18.7\nJuly\n1995\n\n\nAugust1995-January1996\nDARTS\n-2.4\nAugust\n1995\n\n\nSeptember1995-February1996\nDARTS\n25.4\nSeptember\n1995\n\n\nOctober1995-March1996\nDARTS\n50.5\nOctober\n1995\n\n\nNovember1995-April1996\nDARTS\n24.4\nNovember\n1995\n\n\nDecember1995-May1996\nDARTS\n11.5\nDecember\n1995\n\n\nJanuary-June1996\nDARTS\n-5.3\nJanuary\n1996\n\n\nFebruary-July1996\nDARTS\n2.6\nFebruary\n1996\n\n\nMarch-August1996\nDARTS\n-5.7\nMarch\n1996\n\n\nApril-September1996\nDARTS\n7.8\nApril\n1996\n\n\nMay-October1996\nDARTS\n2\nMay\n1996\n\n\nJune-November1996\nDARTS\n6.2\nJune\n1996\n\n\nJuly-December1996\nDARTS\n6.9\nJuly\n1996\n\n\nAugust1996-January1997\nDARTS\n4.7\nAugust\n1996\n\n\nSeptember1996-February1997\nDARTS\n24.6\nSeptember\n1996\n\n\nOctober1996-March1997\nDARTS\n-16.9\nOctober\n1996\n\n\nNovember1996-April1997\nDARTS\n-9.7\nNovember\n1996\n\n\nDecember1996-May1997\nDARTS\n-21.4\nDecember\n1996\n\n\nJanuary-June1997\nDARTS\n18\nJanuary\n1997\n\n\nFebruary-July1997\nDARTS\n-13.9\nFebruary\n1997\n\n\nMarch-August1997\nDARTS\n0.1\nMarch\n1997\n\n\nApril-September1997\nDARTS\n35.6\nApril\n1997\n\n\nMay-October1997\nDARTS\n20.7\nMay\n1997\n\n\nJune-November1997\nDARTS\n6.5\nJune\n1997\n\n\nJuly-December1997\nDARTS\n6.5\nJuly\n1997\n\n\nAugust1997-January1998\nDARTS\n-9\nAugust\n1997\n\n\nSeptember1997-February1998\nDARTS\n-3.3\nSeptember\n1997\n\n\nOctober1997-March1998\nDARTS\n13.3\nOctober\n1997\n\n\nNovember1997-April1998\nDARTS\n-10.5\nNovember\n1997\n\n\nDecember1997-May1998\nDARTS\n28.5\nDecember\n1997\n\n\nJanuary-June1998\nDARTS\n3.2\nJanuary\n1998\n\n\nFebruary-July1998\nDARTS\n-10.1\nFebruary\n1998\n\n\nMarch-August1998\nDARTS\n-20.4\nMarch\n1998\n\n\nApril-September1998\nDARTS\n-34.2\nApril\n1998\n\n\nJanuary-June1990\nDJIA\n2.5\nJanuary\n1990\n\n\nFebruary-July1990\nDJIA\n11.5\nFebruary\n1990\n\n\nMarch-August1990\nDJIA\n-2.3\nMarch\n1990\n\n\nApril-September1990\nDJIA\n-9.2\nApril\n1990\n\n\nMay-October1990\nDJIA\n-8.5\nMay\n1990\n\n\nJune-November1990\nDJIA\n-12.8\nJune\n1990\n\n\nJuly-December1990\nDJIA\n-9.3\nJuly\n1990\n\n\nAugust1990-January1991\nDJIA\n-0.8\nAugust\n1990\n\n\nSeptember1990-February1991\nDJIA\n11\nSeptember\n1990\n\n\nOctober1990-March1991\nDJIA\n15.8\nOctober\n1990\n\n\nNovember1990-April1991\nDJIA\n16.2\nNovember\n1990\n\n\nDecember1990-May1991\nDJIA\n17.3\nDecember\n1990\n\n\nJanuary-June1991\nDJIA\n17.7\nJanuary\n1991\n\n\nFebruary-July1991\nDJIA\n7.6\nFebruary\n1991\n\n\nMarch-August1991\nDJIA\n4.4\nMarch\n1991\n\n\nApril-September1991\nDJIA\n3.4\nApril\n1991\n\n\nMay-October1991\nDJIA\n4.4\nMay\n1991\n\n\nJune-November1991\nDJIA\n-3.3\nJune\n1991\n\n\nJuly-December1991\nDJIA\n6.6\nJuly\n1991\n\n\nAugust1991-January1992\nDJIA\n6.5\nAugust\n1991\n\n\nSeptember1991-February1992\nDJIA\n8.6\nSeptember\n1991\n\n\nOctober1991-March1992\nDJIA\n7.2\nOctober\n1991\n\n\nNovember1991-April1992\nDJIA\n10.6\nNovember\n1991\n\n\nDecember1991-May1992\nDJIA\n17.6\nDecember\n1991\n\n\nJanuary-June1992\nDJIA\n3.6\nJanuary\n1992\n\n\nFebruary-July1992\nDJIA\n4.2\nFebruary\n1992\n\n\nMarch-August1992\nDJIA\n-0.3\nMarch\n1992\n\n\nApril-September1992\nDJIA\n-0.1\nApril\n1992\n\n\nMay-October1992\nDJIA\n-5\nMay\n1992\n\n\nJune-November1992\nDJIA\n-2.8\nJune\n1992\n\n\nJuly-December1992\nDJIA\n0.2\nJuly\n1992\n\n\nAugust1992-January1993\nDJIA\n-0.8\nAugust\n1992\n\n\nSeptember1992-February1993\nDJIA\n2.5\nSeptember\n1992\n\n\nOctober1992-March1993\nDJIA\n9\nOctober\n1992\n\n\nNovember1992-April1993\nDJIA\n5.8\nNovember\n1992\n\n\nDecember1992-May1993\nDJIA\n6.7\nDecember\n1992\n\n\nJanuary1993-June1993\nDJIA\n7.7\nJanuary\n1993\n\n\nFebruary1993-July1993\nDJIA\n3.7\nFebruary\n1993\n\n\nMarch1993-August1993\nDJIA\n7.3\nMarch\n1993\n\n\nApril1993-September1993\nDJIA\n5.2\nApril\n1993\n\n\nMay1993-October1993\nDJIA\n5.7\nMay\n1993\n\n\nJune1993-November1993\nDJIA\n4.9\nJune\n1993\n\n\nJuly1993-Dec.1993\nDJIA\n8\nJuly\n1993\n\n\nAugust1993-January1994\nDJIA\n11.2\nAugust\n1993\n\n\nSeptember1993-February1994\nDJIA\n5.5\nSeptember\n1993\n\n\nOctober1993-March1994\nDJIA\n1.6\nOctober\n1993\n\n\nNovember1993-April1994\nDJIA\n0.5\nNovember\n1993\n\n\nDecember1993-May1994\nDJIA\n1.3\nDecember\n1993\n\n\nJanuary-June1994\nDJIA\n-6.2\nJanuary\n1994\n\n\nFebruary-July1994\nDJIA\n-5.3\nFebruary\n1994\n\n\nMarch-August1994\nDJIA\n1.5\nMarch\n1994\n\n\nApril-September1994\nDJIA\n4.4\nApril\n1994\n\n\nMay-October1994\nDJIA\n6.9\nMay\n1994\n\n\nJune-November1994\nDJIA\n-0.3\nJune\n1994\n\n\nJuly-December1994\nDJIA\n3.6\nJuly\n1994\n\n\nAugust1994-January1995\nDJIA\n1.8\nAugust\n1994\n\n\nSeptember1994-Febuary1995\nDJIA\n3.2\nSeptember\n1994\n\n\nOctober1994-March1995\nDJIA\n7.3\nOctober\n1994\n\n\nNovember1994-April1995\nDJIA\n12.8\nNovember\n1994\n\n\nDecember1994-May1995\nDJIA\n19.5\nDecember\n1994\n\n\nJanuary1995-June1995\nDJIA\n16\nJanuary\n1995\n\n\nFebuary-July1995\nDJIA\n19.6\nFebuary\n1995\n\n\nMarch-August1995\nDJIA\n15.3\nMarch\n1995\n\n\nApril1995-September1995\nDJIA\n14\nApril\n1995\n\n\nMay-October1995\nDJIA\n8.2\nMay\n1995\n\n\nJune-November1995\nDJIA\n13.1\nJune\n1995\n\n\nJuly1995-December1995\nDJIA\n9.3\nJuly\n1995\n\n\nAugust1995-January1996\nDJIA\n15\nAugust\n1995\n\n\nSeptember1995-February1996\nDJIA\n15.6\nSeptember\n1995\n\n\nOctober1995-March1996\nDJIA\n18.4\nOctober\n1995\n\n\nNovember1995-April1996\nDJIA\n14.8\nNovember\n1995\n\n\nDecember1995-May1996\nDJIA\n9\nDecember\n1995\n\n\nJanuary-June1996\nDJIA\n10.2\nJanuary\n1996\n\n\nFebruary-July1996\nDJIA\n1.3\nFebruary\n1996\n\n\nMarch-August1996\nDJIA\n0.6\nMarch\n1996\n\n\nApril-September1996\nDJIA\n5.8\nApril\n1996\n\n\nMay-October1996\nDJIA\n7.2\nMay\n1996\n\n\nJune-November1996\nDJIA\n15.1\nJune\n1996\n\n\nJuly-December1996\nDJIA\n15.5\nJuly\n1996\n\n\nAugust1996-January1997\nDJIA\n19.6\nAugust\n1996\n\n\nSeptember1996-February1997\nDJIA\n20.1\nSeptember\n1996\n\n\nOctober1996-March1997\nDJIA\n9.6\nOctober\n1996\n\n\nNovember1996-April1997\nDJIA\n15.3\nNovember\n1996\n\n\nDecember1996-May1997\nDJIA\n13.3\nDecember\n1996\n\n\nJanuary-June1997\nDJIA\n16.2\nJanuary\n1997\n\n\nFebruary-July1997\nDJIA\n20.8\nFebruary\n1997\n\n\nMarch-August1997\nDJIA\n8.3\nMarch\n1997\n\n\nApril-September1997\nDJIA\n20.2\nApril\n1997\n\n\nMay-October1997\nDJIA\n3\nMay\n1997\n\n\nJune-November1997\nDJIA\n3.8\nJune\n1997\n\n\nJuly-December1997\nDJIA\n-0.7\nJuly\n1997\n\n\nAugust1997-January1998\nDJIA\n-0.3\nAugust\n1997\n\n\nSeptember1997-February1998\nDJIA\n10.7\nSeptember\n1997\n\n\nOctober1997-March1998\nDJIA\n7.6\nOctober\n1997\n\n\nNovember1997-April1998\nDJIA\n22.5\nNovember\n1997\n\n\nDecember1997-May1998\nDJIA\n10.6\nDecember\n1997\n\n\nJanuary-June1998\nDJIA\n15\nJanuary\n1998\n\n\nFebruary-July1998\nDJIA\n7.1\nFebruary\n1998\n\n\nMarch-August1998\nDJIA\n-13.1\nMarch\n1998\n\n\nApril-September1998\nDJIA\n-11.8\nApril\n1998\n\n\n\n\n\n\n\nCode\nstock_tidy <- stock_tidy %>%\n  mutate(\n    month_end = str_sub(month_end, 1, 3),\n    month_end = factor(month_end,\n         levels = c(\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\",\"Jun\",\"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\")))\nwrite_rds(stock_tidy, \"stock_tidy.rds\")\n\ntest_read <- read_rds(\"stock_tidy.rds\")\n# glimpse(test_read)\n\n\n\n\nCode\ndjia_table <- stock_tidy %>%\n  filter(variable == \"DJIA\") %>%\n  select(month_end, year_end, value) %>%\n  pivot_wider(\n    names_from = year_end,\n    values_from = value\n  ) %>%\n  arrange(month_end)   # ensures Jan → Dec in rows\n\n\npander(djia_table)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmonth_end\n1990\n1991\n1992\n1993\n1994\n1995\n1996\n1997\n1998\n\n\n\n\nJan\n2.5\n17.7\n3.6\n7.7\n-6.2\n16\n10.2\n16.2\n15\n\n\nFeb\n11.5\n7.6\n4.2\n3.7\n-5.3\n19.6\n1.3\n20.8\n7.1\n\n\nMar\n-2.3\n4.4\n-0.3\n7.3\n1.5\n15.3\n0.6\n8.3\n-13.1\n\n\nApr\n-9.2\n3.4\n-0.1\n5.2\n4.4\n14\n5.8\n20.2\n-11.8\n\n\nMay\n-8.5\n4.4\n-5\n5.7\n6.9\n8.2\n7.2\n3\nNA\n\n\nJun\n-12.8\n-3.3\n-2.8\n4.9\n-0.3\n13.1\n15.1\n3.8\nNA\n\n\nJul\n-9.3\n6.6\n0.2\n8\n3.6\n9.3\n15.5\n-0.7\nNA\n\n\nAug\n-0.8\n6.5\n-0.8\n11.2\n1.8\n15\n19.6\n-0.3\nNA\n\n\nSep\n11\n8.6\n2.5\n5.5\n3.2\n15.6\n20.1\n10.7\nNA\n\n\nOct\n15.8\n7.2\n9\n1.6\n7.3\n18.4\n9.6\n7.6\nNA\n\n\nNov\n16.2\n10.6\n5.8\n0.5\n12.8\n14.8\n15.3\n22.5\nNA\n\n\nDec\n17.3\n17.6\n6.7\n1.3\n19.5\n9\n13.3\n10.6\nNA"
  },
  {
    "objectID": "weeks/w05/country_height.html",
    "href": "weeks/w05/country_height.html",
    "title": "country_height",
    "section": "",
    "text": "Code\nggplot(heights, aes(x = year_decade, y = height_in, group = country)) +\n  geom_line(data = heights %>% filter(country != \"Germany\"), color = \"grey80\") + # all other countries muted\n  geom_line(data = heights %>% filter(country == \"Germany\"), color = \"red\", size = 1.2) + # Germany bold\n  geom_point(data = heights %>% filter(country == \"Germany\"), color = \"red\", size = 2) +\n  labs(\n    title = \"Average Heights Over Time by Country\",\n    x = \"Year / Decade\",\n    y = \"Height (inches)\"\n  ) +\n  theme_minimal()\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\nBased on this dataset, there is a pretty clear upward trend in average human height over time. Even though the countries start at different baseline heights, most of them show gradual increases across the decades. The pattern is very noticeable when you look at the highlighted Germany line, which steadily rises over the 19th and 20th centuries. So if someone argues that humans have gotten taller over the years, this data definitely supports that idea."
  },
  {
    "objectID": "weeks/w02/Second_Quarto_File.html",
    "href": "weeks/w02/Second_Quarto_File.html",
    "title": "Second_Quarto_File",
    "section": "",
    "text": "Code\n# Working with the Flights Data Set\navg_delay <- flights %>%\n  filter(arr_delay > 0) %>%\n  group_by(origin) %>% \n  summarise(avg_delay_total = mean(arr_delay, na.rm = TRUE)) %>% \n  ungroup()\n\nplot_ly(avg_delay, type='barpolar', r=~avg_delay_total, theta=~origin, color=~origin) %>%  \n  layout(\n    title = \"2013 NYC Flight Average Arrival Delays\",\n    polar = list(\n      radialaxis = list(title = \"Number of Delays\"),\n      angularaxis = list(categoryarray = flights$origin)\n    )\n )\n\n\n\n\n\n\n\nOkay technically this isn’t a pie chart so I should be good… right? Originally I wanted to to do a racial bar chart but I could not find anything on plotly and ggplot didn’t work like I wanted it to. I’d love to learn if there is a way to create it if you have any tips."
  },
  {
    "objectID": "weeks/w10/string_task.html",
    "href": "weeks/w10/string_task.html",
    "title": "string_task",
    "section": "",
    "text": "Code\npacman::p_load(tidyverse, stringr, stringi, rio)\n\n\nQuestion 2 (With the randomletters.txt file, pull out every 1700 letter (for example, 1, 1700, 3400, 5100, …) and find the quote that is hidden—the quote ends with a period.)\n\n\nCode\nchars <- readr::read_file('https://byuistats.github.io/M335/data/randomletters.txt')\ninculde_all <- paste(chars, collapse = \"\")\nbreak_string <- str_split(inculde_all, \"\")[[1]]\n\nposition <- c(1,seq(0, length(break_string), by = 1700))\n\nbreak_string[position]\n\n\n [1] \"t\" \"h\" \"e\" \" \" \"p\" \"l\" \"u\" \"r\" \"a\" \"l\" \" \" \"o\" \"f\" \" \" \"a\" \"n\" \"e\" \"c\" \"d\"\n[20] \"o\" \"t\" \"e\" \" \" \"i\" \"s\" \" \" \"n\" \"o\" \"t\" \" \" \"d\" \"a\" \"t\" \"a\" \".\" \"z\" \" \" \"a\"\n[39] \"n\" \"f\" \"r\" \"a\"\n\n\nQuestion 3 (With the randomletters_wnumbers.txt file, find all the numbers hidden, and convert those numbers to letters using the letters order in the alphabet to decipher the message. For example, a 1=a, 2=b,…, 26=z (Hint: the message starts with “experts”).)\n\n\nCode\nchars_num <- readr::read_lines(\"https://byuistats.github.io/M335/data/randomletters_wnumbers.txt\")\n\nnum_words <- str_extract_all(chars_num, \"[:digit:]+\")[[1]]\nnum_words <- as.integer(num_words)\nnums <- letters[num_words]\npaste(nums, collapse = \"\")\n\n\n[1] \"expertsoftenpossessmoredatathanjudgment\"\n\n\nQuestion 4\nWith the randomletters.txt file, remove all the spaces and periods from the string then find the longest sequence of vowels.\n\n\nCode\ncleaned <- str_remove_all(chars, \"[ \\\\.]\")\nvowel_sequences <- str_extract_all(cleaned, \"[aeiouy]+\")[[1]]\nlongest_vowel_sequence <- vowel_sequences[which.max(nchar(vowel_sequences))]\nlongest_vowel_sequence\n\n\n[1] \"ayeyeya\""
  },
  {
    "objectID": "weeks/w10/counting_words_task.html",
    "href": "weeks/w10/counting_words_task.html",
    "title": "counting_words_task",
    "section": "",
    "text": "Code\npacman::p_load(readr, tidyverse, stringr, stringi, rio, pander, ggplot2)\n\nscriptures <- read_csv(\"https://github.com/beandog/lds-scriptures/raw/master/csv/lds-scriptures.csv\")\n\n\nRows: 41995 Columns: 19\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (13): volume_title, book_title, volume_long_title, book_long_title, volu...\ndbl  (6): volume_id, book_id, chapter_id, verse_id, chapter_number, verse_nu...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\nCode\nbom_nt <- scriptures %>% \n  filter(volume_title %in% c(\"New Testament\", \"Book of Mormon\")) %>% \n  mutate(\n    num_words = str_count(scripture_text, \" \") + 1\n  )\n\nglimpse(bom_nt)\n\n\nRows: 14,561\nColumns: 20\n$ volume_id          <dbl> 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, …\n$ book_id            <dbl> 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40,…\n$ chapter_id         <dbl> 930, 930, 930, 930, 930, 930, 930, 930, 930, 930, 9…\n$ verse_id           <dbl> 23146, 23147, 23148, 23149, 23150, 23151, 23152, 23…\n$ volume_title       <chr> \"New Testament\", \"New Testament\", \"New Testament\", …\n$ book_title         <chr> \"Matthew\", \"Matthew\", \"Matthew\", \"Matthew\", \"Matthe…\n$ volume_long_title  <chr> \"The New Testament\", \"The New Testament\", \"The New …\n$ book_long_title    <chr> \"The Gospel According to St Matthew\", \"The Gospel A…\n$ volume_subtitle    <chr> \"Of our Lord and Saviour Jesus Christ\", \"Of our Lor…\n$ book_subtitle      <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ volume_short_title <chr> \"NT\", \"NT\", \"NT\", \"NT\", \"NT\", \"NT\", \"NT\", \"NT\", \"NT…\n$ book_short_title   <chr> \"Matt.\", \"Matt.\", \"Matt.\", \"Matt.\", \"Matt.\", \"Matt.…\n$ volume_lds_url     <chr> \"nt\", \"nt\", \"nt\", \"nt\", \"nt\", \"nt\", \"nt\", \"nt\", \"nt…\n$ book_lds_url       <chr> \"matt\", \"matt\", \"matt\", \"matt\", \"matt\", \"matt\", \"ma…\n$ chapter_number     <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ verse_number       <dbl> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, …\n$ scripture_text     <chr> \"The book of the generation of Jesus Christ, the so…\n$ verse_title        <chr> \"Matthew 1:1\", \"Matthew 1:2\", \"Matthew 1:3\", \"Matth…\n$ verse_short_title  <chr> \"Matt. 1:1\", \"Matt. 1:2\", \"Matt. 1:3\", \"Matt. 1:4\",…\n$ num_words          <dbl> 16, 14, 16, 12, 16, 21, 12, 12, 12, 12, 16, 14, 12,…\n\n\n\n\n\n\nCode\n# Average verse length\navg_verse_length <- bom_nt %>%\n  group_by(volume_title) %>%\n  summarise(avg_words = mean(num_words, na.rm = TRUE))\n\npander(avg_verse_length) \n\n\n\n\n\n\n\n\n\nvolume_title\navg_words\n\n\n\n\nBook of Mormon\n40.42\n\n\nNew Testament\n22.67\n\n\n\n\n\n\n\n\n\n\nCode\n# Count \"Jesus\" mentions per volume\njesus_count <- bom_nt %>%\n  mutate(jesus_mentions = str_count(scripture_text, regex(\"Jesus\", ignore_case = TRUE))) %>%\n  group_by(volume_title) %>%\n  summarise(total_mentions = sum(jesus_mentions, na.rm = TRUE))\n\njesus_count\n\n\n# A tibble: 2 × 2\n  volume_title   total_mentions\n  <chr>                   <int>\n1 Book of Mormon            184\n2 New Testament             984\n\n\n\n\n\n\n\nCode\nbom <- bom_nt %>%\n  filter(volume_title == \"Book of Mormon\")\n\n# Optional glimpse of word counts per verse\nbom %>% select(book_title, num_words) %>% glimpse()\n\n\nRows: 6,604\nColumns: 2\n$ book_title <chr> \"1 Nephi\", \"1 Nephi\", \"1 Nephi\", \"1 Nephi\", \"1 Nephi\", \"1 N…\n$ num_words  <dbl> 68, 25, 27, 56, 28, 46, 33, 46, 30, 20, 35, 18, 47, 78, 43,…\n\n\n\n\n\n\n\nCode\n# Visualize\nggplot(bom, aes(x = fct_reorder(book_title, num_words, .fun = median), y = num_words)) +\n  geom_boxplot(fill = \"skyblue\") +\n  labs(\n    title = \"Distribution of Verse Lengths in the Book of Mormon\",\n    x = \"Book\",\n    y = \"Number of Words per Verse\"\n  ) +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))"
  },
  {
    "objectID": "weeks/w10/savior_names.html",
    "href": "weeks/w10/savior_names.html",
    "title": "savior_names",
    "section": "",
    "text": "Code\npacman::p_load(readr, tidyverse, stringr, stringi, rio, pander, ggplot2, dplyr)\n\nsavior_names <- read_rds(\"https://byuistats.github.io/M335/data/BoM_SaviorNames.rds\")\nglimpse(savior_names)\n\n\nRows: 112\nColumns: 6\n$ Book          <chr> \"Ether\", \"Mosiah\", \"Mormon\", \"3 Nephi\", \"Mosiah\", \"Mosia…\n$ chapter_verse <chr> \"4:7\", \"3:8\", \"9:29\", \"10:10\", \"7:19\", \"15:4\", \"7:27\", \"…\n$ name          <chr> \"the Father of the heavens and of the earth, and all thi…\n$ reference     <chr> \"Ether 4:7\", \"Mosiah 3:8\", \"Mormon 9:29\", \"3 Nephi 10:10…\n$ nchar         <int> 75, 40, 39, 37, 36, 34, 29, 29, 28, 28, 28, 27, 27, 26, …\n$ words         <int> 16, 7, 8, 6, 7, 6, 6, 6, 6, 6, 5, 4, 5, 5, 5, 4, 5, 5, 5…\n\n\nCode\nscriptures <- read_csv(\"https://github.com/beandog/lds-scriptures/raw/master/csv/lds-scriptures.csv\")\n\nbom_nt <- scriptures %>% \n  filter(volume_title %in% c(\"New Testament\", \"Book of Mormon\")) %>% \n  mutate(num_words = str_count(scripture_text, \" \") + 1)\n\n# Only Book of Mormon verses\nbom_text <- bom_nt %>% filter(volume_title == \"Book of Mormon\")"
  },
  {
    "objectID": "weeks/w10/savior_names.html#analysis-of-words-between-savior-names",
    "href": "weeks/w10/savior_names.html#analysis-of-words-between-savior-names",
    "title": "savior_names",
    "section": "Analysis of Words Between Savior Names",
    "text": "Analysis of Words Between Savior Names\nThe histogram above shows the distribution of the number of words between mentions of Savior names in the Book of Mormon. Most Savior names occur fairly close together, indicating clusters of references in key chapters. There are occasional long stretches without a Savior name, which contribute to the long tail of the distribution. On average, there are 24.7962406 words between mentions, suggesting that readers encounter references to the Savior regularly throughout the text."
  },
  {
    "objectID": "weeks/w01/First_Quarto_File.html",
    "href": "weeks/w01/First_Quarto_File.html",
    "title": "First_Quarto_File",
    "section": "",
    "text": "Code\n## Sourced Articles\n\n### Article 1: Top 10 Movies on Netflix Right Now(https://www.netflix.com/tudum/top10)\n\n### First we have Netflix showing the top viewed movies on their streaming service. It looks visually nice when you get to see the movie and the ranking on the top left corner. It is clean, however, when you scroll down to look at why the data looks very strange. We have Kpop Demon Hunters sitting at first followed by the rest of the movies. Next to the titles is a column of weeks and views. All the weeks shown are different most at 1 which is unknown if it is current or of all time. The visual isn't very pleasing either. Just straight forward with no interesting or stand out visuals. Just basic, almost like an excel sheet. It looks clean but maybe something a little more fun to look at could go a long way.\n\nplot(1:20)\nFirst we have Netflix showing the top viewed movies on their streaming service. At first it looks visually nice when you get to see the movie and the ranking on the top left corner. However, when you scroll down to look at why the data looks very strange. We have Kpop Demon Hunters sitting at first followed by the rest of the movies. Next to the titles is a column of weeks and views. All the weeks shown are different most at 1 which is unknown if it is current or of all time. The visual isn’t very pleasing either. Just straight forward with no interesting or stand out visuals. Just basic, almost like an excel sheet. It looks clean but maybe something a little more fun to look at could go a long way."
  },
  {
    "objectID": "weeks/w01/First_Quarto_File.html#article-1-top-10-movies-on-netflix-right-nowhttpswww.netflix.comtudumtop10",
    "href": "weeks/w01/First_Quarto_File.html#article-1-top-10-movies-on-netflix-right-nowhttpswww.netflix.comtudumtop10",
    "title": "First_Quarto_File",
    "section": "Article 1: Top 10 Movies on Netflix Right Now(https://www.netflix.com/tudum/top10)",
    "text": "Article 1: Top 10 Movies on Netflix Right Now(https://www.netflix.com/tudum/top10)\nNext we have Anychart. Any chart looks like a place just to post what the best looking data visualizations there are. It’s a great website but it uses none of them itself, maybe to avoid bias. Many of the graphs created are very unique but some are a little too busy to look at."
  },
  {
    "objectID": "weeks/w01/First_Quarto_File.html#article-2-top-data-visualizations-on-travel-burgers-shootings-and-light",
    "href": "weeks/w01/First_Quarto_File.html#article-2-top-data-visualizations-on-travel-burgers-shootings-and-light",
    "title": "First_Quarto_File",
    "section": "Article 2: Top Data Visualizations on Travel, Burgers, Shootings, and Light",
    "text": "Article 2: Top Data Visualizations on Travel, Burgers, Shootings, and Light"
  },
  {
    "objectID": "weeks/w01/First_Quarto_File.html#httpswww.anychart.comblog20190809top-data-visualizations-dataviz-weekly",
    "href": "weeks/w01/First_Quarto_File.html#httpswww.anychart.comblog20190809top-data-visualizations-dataviz-weekly",
    "title": "First_Quarto_File",
    "section": "(https://www.anychart.com/blog/2019/08/09/top-data-visualizations-dataviz-weekly/)",
    "text": "(https://www.anychart.com/blog/2019/08/09/top-data-visualizations-dataviz-weekly/)\nLastly we have Tastewise which is a website looking at the popularity of a culture’s food. Showing trends somewhere, maybe globally, ingredients and flavors, and some links that don’t work… They used a variety of different graphs and visuals but some look like completely different designs almost as if they don’t go together. Most of the data is not fully understandable because the labels aren’t very strong to explain what it’s showing."
  },
  {
    "objectID": "weeks/w01/First_Quarto_File.html#article-3-korean-food-trend-overview",
    "href": "weeks/w01/First_Quarto_File.html#article-3-korean-food-trend-overview",
    "title": "First_Quarto_File",
    "section": "Article 3: Korean Food trend overview",
    "text": "Article 3: Korean Food trend overview"
  },
  {
    "objectID": "weeks/w01/First_Quarto_File.html#httpstastewise.iofoodtrendskoreandishes",
    "href": "weeks/w01/First_Quarto_File.html#httpstastewise.iofoodtrendskoreandishes",
    "title": "First_Quarto_File",
    "section": "(https://tastewise.io/foodtrends/korean#dishes)",
    "text": "(https://tastewise.io/foodtrends/korean#dishes)"
  },
  {
    "objectID": "weeks/w05/polish_visualization.html",
    "href": "weeks/w05/polish_visualization.html",
    "title": "polish_visualization",
    "section": "",
    "text": "Periscopic created a moving interactive visualization of U.S. gun deaths where each ribbon represents a life that ended early. My original analysis summarized the same data, and my client now wants a version that looks a little cleaner for a beginner-level presentation. I kept the structure from the first draft but nudged the plots with a consistent theme and clearer labels."
  },
  {
    "objectID": "weeks/w05/polish_visualization.html#research-question",
    "href": "weeks/w05/polish_visualization.html#research-question",
    "title": "polish_visualization",
    "section": "Research Question",
    "text": "Research Question\nWhich combinations of intent, age, month, and gender experienced the most gun deaths in the data? The answer helps schedule different prevention messages during the year."
  },
  {
    "objectID": "weeks/w05/polish_visualization.html#data-loading",
    "href": "weeks/w05/polish_visualization.html#data-loading",
    "title": "polish_visualization",
    "section": "Data Loading",
    "text": "Data Loading\n\n\nCode\nlibrary(tidyverse)\n\n# Download FiveThirtyEight gun data\ngun_data <- read_csv(\n  \"https://raw.githubusercontent.com/fivethirtyeight/guns-data/master/full_data.csv\",\n  show_col_types = FALSE\n)\n\nnew_theme <- theme_minimal(base_size = 14) +\n  theme(\n    plot.title = element_text(face = \"bold\"),\n    panel.grid.minor = element_blank(),\n    axis.text.x = element_text(angle = 25, hjust = 1)\n  )\n\n\nThe dataset has more than 33,000 rows with age, gender, race, month, intent, and location fields. That gives us many options to filter or group the statistics."
  },
  {
    "objectID": "weeks/w05/polish_visualization.html#visualization-1-intent-drives-the-story",
    "href": "weeks/w05/polish_visualization.html#visualization-1-intent-drives-the-story",
    "title": "Polished Gun Death Visualizations",
    "section": "Visualization 1 — Intent Drives the Story",
    "text": "Visualization 1 — Intent Drives the Story\nThe first visual establishes scale: intent types and their relative share of deaths.\n\n\nCode\nintent_summary <- gun_focus |>\n  mutate(intent = str_to_sentence(intent)) |>\n  count(intent, name = \"deaths\") |>\n  mutate(intent = fct_reorder(intent, deaths))\n\nggplot(intent_summary, aes(x = intent, y = deaths, fill = intent)) +\n  geom_col(width = 0.65, show.legend = FALSE) +\n  geom_text(\n    aes(label = comma(deaths)),\n    vjust = -0.4,\n    family = \"sans\",\n    size = 3.5\n  ) +\n  scale_y_continuous(labels = comma, expand = expansion(mult = c(0, 0.08))) +\n  scale_fill_manual(values = intent_palette) +\n  labs(\n    title = \"Suicides Dominate U.S. Gun Deaths\",\n    subtitle = glue::glue(\"Counts for {analysis_year} from CDC WONDER extracts\"),\n    x = \"Intent Type\",\n    y = \"Number of Deaths\",\n    caption = \"Source: FiveThirtyEight gun deaths data set\"\n  ) +\n  polished_theme\n\n\n\n\n\nSuicide dwarfs every other intent, accounting for roughly two-thirds of deaths. The subtitle explicitly cites the data year so stakeholders know the context immediately."
  },
  {
    "objectID": "weeks/w05/polish_visualization.html#visualization-2-seasonal-pulse-by-month",
    "href": "weeks/w05/polish_visualization.html#visualization-2-seasonal-pulse-by-month",
    "title": "Polished Gun Death Visualizations",
    "section": "Visualization 2 — Seasonal Pulse by Month",
    "text": "Visualization 2 — Seasonal Pulse by Month\nConference presentations usually want a hook for timing. This panel highlights the months with the highest counts and uses a consistent color language.\n\n\nCode\nmonthly_pattern <- gun_focus |>\n  mutate(\n    month_num = suppressWarnings(as.integer(as.character(month))),\n    month_label = case_when(\n      !is.na(month_num) ~ month.name[pmax(1, pmin(12, month_num))],\n      TRUE ~ str_to_title(month)\n    ),\n    month_label = factor(month_label, levels = month.name)\n  ) |>\n  count(month_label, name = \"deaths\") |>\n  drop_na(month_label) |>\n  arrange(month_label) |>\n  mutate(is_peak = deaths == max(deaths))\n\nggplot(monthly_pattern, aes(x = month_label, y = deaths, group = 1)) +\n  geom_area(fill = \"lightsteelblue1\", alpha = 0.8) +\n  geom_line(color = \"steelblue\", size = 1.1) +\n  geom_point(aes(color = is_peak), size = 3) +\n  scale_color_manual(values = c(\"FALSE\" = \"steelblue\", \"TRUE\" = \"firebrick\"), guide = \"none\") +\n  scale_y_continuous(labels = comma) +\n  labs(\n    title = \"Summer Shows a Measurable Uptick\",\n    subtitle = \"Peak months stand out for scheduling seasonal prevention messaging\",\n    x = \"Month\",\n    y = \"Number of Deaths\"\n  ) +\n  polished_theme +\n  theme(axis.text.x = element_text(angle = 35, hjust = 1))\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\nThe red point calls out the peak month and makes it easy for viewers to recall when planning campaigns."
  },
  {
    "objectID": "weeks/w05/polish_visualization.html#visualization-3-age-bands-emphasize-target-audiences",
    "href": "weeks/w05/polish_visualization.html#visualization-3-age-bands-emphasize-target-audiences",
    "title": "Polished Gun Death Visualizations",
    "section": "Visualization 3 — Age Bands Emphasize Target Audiences",
    "text": "Visualization 3 — Age Bands Emphasize Target Audiences\n\n\nCode\nage_summary <- gun_focus |>\n  filter(!is.na(age) & age < 100) |>\n  mutate(\n    age_band = cut(\n      age,\n      breaks = c(0, 18, 30, 45, 60, 75, 100),\n      labels = c(\"0–17\", \"18–29\", \"30–44\", \"45–59\", \"60–74\", \"75+\"),\n      right = FALSE\n    )\n  ) |>\n  count(age_band, name = \"deaths\") |>\n  mutate(age_band = fct_rev(age_band))\n\nggplot(age_summary, aes(x = deaths, y = age_band, fill = age_band)) +\n  geom_col(width = 0.7, show.legend = FALSE) +\n  geom_text(\n    aes(label = comma(deaths)),\n    hjust = -0.05,\n    size = 3.3\n  ) +\n  scale_x_continuous(labels = comma, expand = expansion(mult = c(0, 0.1))) +\n  scale_fill_manual(values = rep(\"seagreen3\", nrow(age_summary))) +\n  labs(\n    title = \"Young to Middle-Aged Adults Bear the Brunt\",\n    subtitle = \"Ages 18–44 represent the largest blocks of lost life\",\n    x = \"Number of Deaths\",\n    y = \"Age Band\"\n  ) +\n  polished_theme\n\n\n\n\n\nFlipping the axes enables long labels and a callout that the 18–44 brackets are the primary outreach target."
  },
  {
    "objectID": "weeks/w05/polish_visualization.html#visualization-4-gender-and-intent-facets",
    "href": "weeks/w05/polish_visualization.html#visualization-4-gender-and-intent-facets",
    "title": "Polished Gun Death Visualizations",
    "section": "Visualization 4 — Gender and Intent Facets",
    "text": "Visualization 4 — Gender and Intent Facets\n\n\nCode\ngender_intent <- gun_focus |>\n  mutate(intent = str_to_sentence(intent)) |>\n  count(sex, intent, name = \"deaths\") |>\n  group_by(intent) |>\n  mutate(percent = deaths / sum(deaths)) |>\n  ungroup() |>\n  drop_na(sex)\n\nggplot(gender_intent, aes(x = sex, y = percent, fill = sex)) +\n  geom_col(width = 0.55) +\n  facet_wrap(~intent, scales = \"free_y\") +\n  scale_y_continuous(labels = percent_format(accuracy = 1)) +\n  scale_fill_manual(values = sex_palette, guide = \"none\") +\n  labs(\n    title = \"Male Deaths Dominate, Especially for Suicide\",\n    subtitle = \"Facets make it simple to compare how intent splits between men and women\",\n    x = \"Gender\",\n    y = \"Percent within Intent\"\n  ) +\n  polished_theme\n\n\n\n\n\nFaceting separates the intents so that the suicide panel instantly communicates that men drive nearly nine out of ten cases, whereas the homicide panel shows a noticeably higher female share."
  },
  {
    "objectID": "weeks/w05/polish_visualization.html#conclusion",
    "href": "weeks/w05/polish_visualization.html#conclusion",
    "title": "polish_visualization",
    "section": "Conclusion",
    "text": "Conclusion\nThe refreshed graphics stay close to the original assignment but use a consistent minimalist theme to support conference slides. They show that suicides dominate the data, summer months are slightly higher, adults aged 20–59 face the greatest risk, and male suicide prevention should be a major talking point."
  },
  {
    "objectID": "weeks/w05/polish_visualization.html#book-exercises-28.3.1",
    "href": "weeks/w05/polish_visualization.html#book-exercises-28.3.1",
    "title": "polish_visualization",
    "section": "Book Exercises 28.3.1",
    "text": "Book Exercises 28.3.1\nI completed two short annotation exercises from the book section to practice labeling data.\n\nExercise 1\n\n\nCode\nbest_car <- mpg %>% slice_max(hwy, n = 1, with_ties = FALSE)\n\nggplot(mpg, aes(displ, hwy)) +\n  geom_point(alpha = 0.4) +\n  annotate(\n    \"label\",\n    x = best_car$displ,\n    y = best_car$hwy,\n    label = paste(best_car$manufacturer, best_car$model)\n  ) +\n  labs(title = \"Highlighting the Best Highway MPG\") +\n  new_theme\n\n\n\n\n\n\n\nExercise 2\n\n\nCode\nggplot(mpg, aes(displ, hwy)) +\n  geom_point(alpha = 0.4) +\n  annotate(\"rect\", xmin = 5, xmax = 7, ymin = 10, ymax = 20, alpha = 0.1, fill = \"red\") +\n  annotate(\"text\", x = 6, y = 22, label = \"Large engines\\nlow MPG\") +\n  labs(title = \"Marking a Low-Efficiency Zone\") +\n  new_theme\n\n\n\n\n\nThese exercises simply add labels and shaded regions, matching the beginner-level ideas from section 28.3.1."
  },
  {
    "objectID": "weeks/w03/gun_deaths.html",
    "href": "weeks/w03/gun_deaths.html",
    "title": "gun_deaths",
    "section": "",
    "text": "The interactive visualization created by Periscopic on U.S. Gun Deaths in 2018 presents an emotional and data-driven exploration of every gun death that year.\nEach line represents a life cut short, with its length showing the number of years of life lost. After exploring the filters for age, gender, race, and intent, several clear patterns emerge.\nMost gun deaths in 2018 were suicides, particularly among middle-aged white males. In contrast, homicides were more common among younger individuals. The visualization also makes it clear that suicides, though often less publicized, far outnumber homicides in the United States.\nThis data tells a story not only about how people die but also about who they were — and what could have been. Understanding these patterns helps us think about how to target prevention and awareness efforts throughout the year."
  },
  {
    "objectID": "weeks/w03/gun_deaths.html#research-question",
    "href": "weeks/w03/gun_deaths.html#research-question",
    "title": "gun_deaths",
    "section": "Research Question",
    "text": "Research Question\nWhich demographic and intent-based groups experienced the highest numbers of gun deaths in 2018, and how might these patterns vary by season?\nThis question will guide the following visualizations, which aim to help a public-awareness client plan campaigns emphasizing different groups or issues in different seasons."
  },
  {
    "objectID": "weeks/w03/gun_deaths.html#data-loading",
    "href": "weeks/w03/gun_deaths.html#data-loading",
    "title": "gun_deaths",
    "section": "Data Loading",
    "text": "Data Loading\n\n\nCode\nlibrary(tidyverse)\ngun_data <- read_csv(\"https://raw.githubusercontent.com/fivethirtyeight/guns-data/master/full_data.csv\")\n\n## glimpse(gun_data)\n\n## head(gun_data)\n\n\nSummary: This dataset contains over 33,000 records representing individual gun deaths in 2018. It includes columns such as age, sex, race, month, intent, and geographic information, allowing for analysis of who was affected and how circumstances vary.\nVisualization 1 — Gun Deaths by Intent\nThis first visualization provides a general overview of the types of gun deaths recorded in 2018.\n\n\nCode\ngun_data %>%\ncount(intent) %>%\nggplot(aes(x = reorder(intent, -n), y = n, fill = intent)) +\ngeom_col(show.legend = FALSE) +\nlabs(title = \"Gun Deaths by Intent (2018)\",\nx = \"Intent\",\ny = \"Number of Deaths\") +\ntheme_minimal() +\ntheme(axis.text.x = element_text(angle = 20, hjust = 1))\n\n\n\n\n\nDescription: This plot shows that suicides account for the vast majority of gun deaths in 2018, followed by homicides. This highlights how prevention efforts could focus more on mental health resources and suicide awareness campaigns, especially targeting high-risk groups.\nVisualization 2 — Gun Deaths by Month\nTo address the client’s request for identifying seasonal emphasis areas, this chart examines monthly variations.\n\n\nCode\ngun_data %>%\ncount(month) %>%\nggplot(aes(x = month, y = n, group = 1)) +\ngeom_line(color = \"steelblue\", size = 1) +\ngeom_point(color = \"steelblue\", size = 2) +\nlabs(title = \"Gun Deaths by Month (2018)\",\nx = \"Month\",\ny = \"Number of Deaths\") +\ntheme_minimal()\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\nDescription: There appears to be a slight increase in gun deaths during the summer months (June–August), suggesting that awareness campaigns focusing on safe conflict resolution and stress management might be most effective during this time. Winter months generally show fewer deaths, possibly due to seasonal behaviors or reduced outdoor activity.\nVisualization 3 — Gun Deaths by Age Group\nNext, we group individuals by age to understand which life stages are most affected.\n\n\nCode\ngun_data %>%\nmutate(age_group = cut(age, breaks = c(0, 20, 40, 60, 80, 100),\nlabels = c(\"0–19\", \"20–39\", \"40–59\", \"60–79\", \"80+\"))) %>%\ncount(age_group) %>%\nggplot(aes(x = age_group, y = n, fill = age_group)) +\ngeom_col(show.legend = FALSE) +\nlabs(title = \"Gun Deaths by Age Group (2018)\",\nx = \"Age Group\",\ny = \"Number of Deaths\") +\ntheme_minimal()\n\n\n\n\n\nDescription: Gun deaths are most common among individuals aged 20–39, followed by those aged 40–59. Younger adults appear to be at the highest risk, indicating that outreach efforts and educational campaigns targeting this demographic could have the greatest impact.\nVisualization 4 — Gun Deaths by Gender and Intent\nFinally, this chart explores gender differences in types of gun deaths.\n\n\nCode\ngun_data %>%\ncount(sex, intent) %>%\nggplot(aes(x = sex, y = n, fill = intent)) +\ngeom_col(position = \"dodge\") +\nlabs(title = \"Gun Deaths by Gender and Intent (2018)\",\nx = \"Gender\",\ny = \"Number of Deaths\",\nfill = \"Intent\") +\ntheme_minimal()\n\n\n\n\n\nDescription: Males represent the overwhelming majority of gun deaths, particularly in suicides. Female deaths occur far less frequently but are more likely to be homicides. This suggests that messaging for male audiences could focus on mental health and suicide prevention, while female-focused campaigns could emphasize domestic violence prevention and safety resources.\nConclusion\nThe 2018 data on gun deaths reveal significant trends across intent, age, gender, and season. Most deaths result from suicide, primarily among men aged 20–59. Seasonal patterns indicate slightly higher rates during the summer months, suggesting an opportunity to time prevention campaigns accordingly.\nBy tailoring campaigns to highlight mental health awareness in men, safety in younger adults, and prevention messaging during the summer, organizations can maximize their impact in reducing gun-related deaths in the U.S."
  },
  {
    "objectID": "weeks/w04/stock_task.html",
    "href": "weeks/w04/stock_task.html",
    "title": "stock_task",
    "section": "",
    "text": "The contestant_period column is not “tidy” we want to create a month_end and a year_end column from the information it contains.\n\n\nCode\nstock <- stock %>% \n  separate(contest_period, into = c(\"month_start\", \"end_part\"), sep = \"-\") %>% \n  mutate(\n    month_end = str_sub(end_part, 1, -5),\n    year_end = str_sub(end_part, -4)\n)\n\n\nSave your “tidy” data as an .rds object. (as an optional challenge, see if you can read in the saved file!)\n\n\nCode\nstock_rsd <- saveRDS(stock, \"stock.rsd\")\nstock_rsd <- readRDS(\"stock.rsd\")\n\nhead(stock_rsd)\n\n\n# A tibble: 6 × 6\n  month_start end_part      variable value month_end year_end\n  <chr>       <chr>         <chr>    <dbl> <chr>     <chr>   \n1 January     June1990      PROS      12.7 June      1990    \n2 February    July1990      PROS      26.4 July      1990    \n3 March       August1990    PROS       2.5 August    1990    \n4 April       September1990 PROS     -20   September 1990    \n5 May         October1990   PROS     -37.8 October   1990    \n6 June        November1990  PROS     -33.3 November  1990    \n\n\nUse code to create a table of the DJIA returns that matches the table shown below (apply pivot_wider() to the data). Pay attention to detail.\n\n\nCode\nstock_order <- stock_rsd %>%\n  mutate(\n    month_end = factor(\n      month_end,\n      levels = c(\"January\", \"February\", \"March\", \"April\", \"May\", \"June\", \"July\", \"August\", \"September\", \"October\", \"November\", \"December\")\n    )\n  ) %>% \n  filter(variable == \"DJIA\") %>% \n  select(month_end, year_end, value) %>%\n  pivot_wider(\n    names_from = year_end,\n    values_from = value\n  ) %>%\n  arrange(month_end) %>%\n  mutate(across(\n    .cols = -month_end,\n    .fns = ~ ifelse(is.na(.x), \"-\", .x)\n  ))\n\npander(stock_order)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmonth_end\n1990\n1991\n1992\n1993\n1994\n1995\n1996\n1997\n1998\n\n\n\n\nJanuary\n-\n-0.8\n6.5\n-0.8\n11.2\n1.8\n15\n19.6\n-0.3\n\n\nFebruary\n-\n11\n8.6\n2.5\n5.5\n-\n15.6\n20.1\n10.7\n\n\nMarch\n-\n15.8\n7.2\n9\n1.6\n7.3\n18.4\n9.6\n7.6\n\n\nApril\n-\n16.2\n10.6\n5.8\n0.5\n12.8\n14.8\n15.3\n22.5\n\n\nMay\n-\n17.3\n17.6\n6.7\n1.3\n19.5\n9\n13.3\n10.6\n\n\nJune\n2.5\n17.7\n3.6\n7.7\n-6.2\n16\n10.2\n16.2\n15\n\n\nJuly\n11.5\n7.6\n4.2\n3.7\n-5.3\n19.6\n1.3\n20.8\n7.1\n\n\nAugust\n-2.3\n4.4\n-0.3\n7.3\n1.5\n15.3\n0.6\n8.3\n-13.1\n\n\nSeptember\n-9.2\n3.4\n-0.1\n5.2\n4.4\n14\n5.8\n20.2\n-11.8\n\n\nOctober\n-8.5\n4.4\n-5\n5.7\n6.9\n8.2\n7.2\n3\n-\n\n\nNovember\n-12.8\n-3.3\n-2.8\n4.9\n-0.3\n13.1\n15.1\n3.8\n-\n\n\nDecember\n-9.3\n6.6\n0.2\n-\n3.6\n9.3\n15.5\n-0.7\n-\n\n\nNA\n-\n-\n-\n8\n-\n3.2\n-\n-\n-"
  },
  {
    "objectID": "weeks/w04/country_height.html",
    "href": "weeks/w04/country_height.html",
    "title": "country_height",
    "section": "",
    "text": "Code\nggplot(heights, aes(x = year_decade, y = height_in, group = country)) +\n  geom_line(data = heights %>% filter(country != \"Germany\"), color = \"grey80\") + \n  geom_line(data = heights %>% filter(country == \"Germany\"), color = \"red\", size = 1.2) + # Germany bold\n  geom_point(data = heights %>% filter(country == \"Germany\"), color = \"red\", size = 2) +\n  labs(\n    title = \"Average Heights Over Time by Country\",\n    x = \"Year / Decade\",\n    y = \"Height (inches)\"\n  ) +\n  theme_minimal()\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\nBased on this dataset, there is a pretty clear upward trend in average human height over time. Even though the countries start at different baseline heights, most of them show gradual increases across the decades. The pattern is very noticeable when you look at the highlighted Germany line, which steadily rises over the 19th and 20th centuries. So if someone argues that humans have gotten taller over the years, this data definitely supports that idea."
  },
  {
    "objectID": "weeks/w05/polish_visualization.html#visualization-1-gun-deaths-by-intent",
    "href": "weeks/w05/polish_visualization.html#visualization-1-gun-deaths-by-intent",
    "title": "polish_visualization",
    "section": "Visualization 1 — Gun Deaths by Intent",
    "text": "Visualization 1 — Gun Deaths by Intent\n\n\nCode\ngun_data %>%\n  count(intent) %>%\n  ggplot(aes(x = reorder(intent, -n), y = n, fill = intent)) +\n  geom_col(show.legend = FALSE) +\n  labs(\n    title = \"Gun Deaths by Intent\",\n    x = \"Intent\",\n    y = \"Number of Deaths\"\n  ) +\n  new_theme\n\n\n\n\n\nSuicides clearly make up the majority of deaths. The theme simply bolds the title and angles the axis labels so that the bar names are easier to read on a slide."
  },
  {
    "objectID": "weeks/w05/polish_visualization.html#visualization-2-gun-deaths-by-month",
    "href": "weeks/w05/polish_visualization.html#visualization-2-gun-deaths-by-month",
    "title": "polish_visualization",
    "section": "Visualization 2 — Gun Deaths by Month",
    "text": "Visualization 2 — Gun Deaths by Month\n\n\nCode\ngun_data %>%\n  mutate(month = factor(month, levels = month.abb)) %>%\n  count(month) %>%\n  drop_na(month) %>%\n  ggplot(aes(x = month, y = n, group = 1)) +\n  geom_line(color = \"steelblue\") +\n  geom_point(color = \"steelblue\", size = 2) +\n  labs(\n    title = \"Seasonal Pattern of Gun Deaths\",\n    x = \"Month\",\n    y = \"Number of Deaths\"\n  ) +\n  new_theme\n\n\n\n\n\nCounts creep up during the summer months. That small bump is a natural place to schedule awareness campaigns that focus on conflict resolution and safe storage."
  },
  {
    "objectID": "weeks/w05/polish_visualization.html#visualization-3-gun-deaths-by-age-group",
    "href": "weeks/w05/polish_visualization.html#visualization-3-gun-deaths-by-age-group",
    "title": "polish_visualization",
    "section": "Visualization 3 — Gun Deaths by Age Group",
    "text": "Visualization 3 — Gun Deaths by Age Group\n\n\nCode\ngun_data %>%\n  mutate(\n    age_group = cut(\n      age,\n      breaks = c(0, 20, 40, 60, 80, 100),\n      labels = c(\"0–19\", \"20–39\", \"40–59\", \"60–79\", \"80+\"),\n      right = FALSE\n    )\n  ) %>%\n  count(age_group) %>%\n  drop_na(age_group) %>%\n  ggplot(aes(x = age_group, y = n, fill = age_group)) +\n  geom_col(show.legend = FALSE) +\n  labs(\n    title = \"Gun Deaths by Age Group\",\n    x = \"Age Group\",\n    y = \"Number of Deaths\"\n  ) +\n  new_theme\n\n\n\n\n\nPeople between 20 and 59 years old account for most deaths. Highlighting this range makes it simple to describe a priority audience."
  },
  {
    "objectID": "weeks/w05/polish_visualization.html#visualization-4-gun-deaths-by-gender-and-intent",
    "href": "weeks/w05/polish_visualization.html#visualization-4-gun-deaths-by-gender-and-intent",
    "title": "polish_visualization",
    "section": "Visualization 4 — Gun Deaths by Gender and Intent",
    "text": "Visualization 4 — Gun Deaths by Gender and Intent\n\n\nCode\ngun_data %>%\n  count(sex, intent) %>%\n  drop_na(sex) %>%\n  ggplot(aes(x = sex, y = n, fill = intent)) +\n  geom_col(position = \"dodge\") +\n  labs(\n    title = \"Gender Differences by Intent\",\n    x = \"Gender\",\n    y = \"Number of Deaths\",\n    fill = \"Intent\"\n  ) +\n  new_theme\n\n\n\n\n\nMales account for most deaths overall, especially suicides. Females have fewer total deaths but a relatively higher share of homicides. That information helps the client tailor messages for each audience."
  },
  {
    "objectID": "case_studies/savior_names.html",
    "href": "case_studies/savior_names.html",
    "title": "savior_names",
    "section": "",
    "text": "Code\npacman::p_load(readr, tidyverse, stringr, stringi, rio, pander, ggplot2, dplyr)\n\nsavior_names <- read_rds(\"https://byuistats.github.io/M335/data/BoM_SaviorNames.rds\")\nglimpse(savior_names)\n\n\nRows: 112\nColumns: 6\n$ Book          <chr> \"Ether\", \"Mosiah\", \"Mormon\", \"3 Nephi\", \"Mosiah\", \"Mosia…\n$ chapter_verse <chr> \"4:7\", \"3:8\", \"9:29\", \"10:10\", \"7:19\", \"15:4\", \"7:27\", \"…\n$ name          <chr> \"the Father of the heavens and of the earth, and all thi…\n$ reference     <chr> \"Ether 4:7\", \"Mosiah 3:8\", \"Mormon 9:29\", \"3 Nephi 10:10…\n$ nchar         <int> 75, 40, 39, 37, 36, 34, 29, 29, 28, 28, 28, 27, 27, 26, …\n$ words         <int> 16, 7, 8, 6, 7, 6, 6, 6, 6, 6, 5, 4, 5, 5, 5, 4, 5, 5, 5…\n\n\nCode\nscriptures <- read_csv(\"https://github.com/beandog/lds-scriptures/raw/master/csv/lds-scriptures.csv\")\n\nbom_nt <- scriptures %>% \n  filter(volume_title %in% c(\"New Testament\", \"Book of Mormon\")) %>% \n  mutate(num_words = str_count(scripture_text, \" \") + 1)\n\n# Only Book of Mormon verses\nbom_text <- bom_nt %>% filter(volume_title == \"Book of Mormon\")"
  },
  {
    "objectID": "case_studies/savior_names.html#analysis-of-words-between-savior-names",
    "href": "case_studies/savior_names.html#analysis-of-words-between-savior-names",
    "title": "savior_names",
    "section": "Analysis of Words Between Savior Names",
    "text": "Analysis of Words Between Savior Names\nThe histogram above shows the distribution of the number of words between mentions of Savior names in the Book of Mormon. Most Savior names occur fairly close together, indicating clusters of references in key chapters. There are occasional long stretches without a Savior name, which contribute to the long tail of the distribution. On average, there are 24.7962406 words between mentions, suggesting that readers encounter references to the Savior regularly throughout the text."
  },
  {
    "objectID": "weeks/w03/wings_to_fly.html",
    "href": "weeks/w03/wings_to_fly.html",
    "title": "wings_to_fly",
    "section": "",
    "text": "Code\npacman::p_load(nycflights13, ggplot2, pander, tidyverse)\n\nflights_boss <- flights %>% filter(origin %in% c(\"JFK\", \"EWR\", \"LGA\"))\n\n\n\n\nCode\nquantiles <- flights_boss %>% \n  group_by(origin) %>% \n  summarise(delay = quantile(dep_delay, 0.75, na.rm=TRUE))\n\npander(quantiles)\n\n\n\n\n\n\n\n\n\norigin\ndelay\n\n\n\n\nEWR\n15\n\n\nJFK\n10\n\n\nLGA\n7\n\n\n\n\n\nCode\nggplot(quantiles, aes(origin, delay, fill = origin)) +\n  geom_col()+\n  labs(\n    title = \"Airlines by Lowest 75th Percentile of Departure Delay\",\n    y=\"Delay time in Minutes of Departure\",\n    x=\"Airline\"\n  )\n\n\n\n\n\n\n\n\nOverall, LaGuardia (LGA) has the lowest 75th percentile delay at 7 minutes, meaning flights before noon at LGA tend to run smoother compared to JFK and EWR. JFK is next best at 10 minutes, while EWR is the worst of the three with 15 minutes.\nSo, if you want to minimize the chances of getting a “bad” delay before lunch, LGA is the strongest choice.\n\n\n\n\n\nCode\ndelta_data <- flights_boss %>% \n  filter(carrier == \"DL\") %>% \n  group_by(origin) %>% \n  summarise(mean = mean(arr_delay, na.rm=TRUE))\n\npander(delta_data)\n\n\n\n\n\n\n\n\n\norigin\nmean\n\n\n\n\nEWR\n8.78\n\n\nJFK\n-2.379\n\n\nLGA\n3.928\n\n\n\n\n\nCode\nggplot(delta_data, aes(origin, mean, fill=origin)) +\n  geom_col() +\n  labs(\n    title = \"Don't Wanna Be Late? \\n Choose JFK\",\n    y=\"Delay time in Minutes of Arrival\",\n    x=\"Airline\"\n  )\n\n\n\n\n\n\n\n\nAmong the three NYC airports, JFK performs the best for Delta arrivals, with an average arrival time of 2.38 minutes early. LGA averages about 4 minutes late, and EWR is the worst performer at almost 9 minutes late on average.\nSo, if I want to minimize the chances of arriving late on a Delta flight, JFK is the best origin airport.\n\n\n\n\n\nCode\nworst_dest <- flights_boss %>% \n  group_by(dest) %>% \n  summarise(mean = mean(arr_delay, na.rm=TRUE)) %>% \n  filter(mean > 20) %>% \n  arrange(desc(mean))\n\npander(worst_dest)\n\n\n\n\n\n\n\n\n\ndest\nmean\n\n\n\n\nCAE\n41.76\n\n\nTUL\n33.66\n\n\nOKC\n30.62\n\n\nJAC\n28.1\n\n\nTYS\n24.07\n\n\nMSN\n20.2\n\n\nRIC\n20.11\n\n\n\n\n\nCode\nggplot(worst_dest, aes(dest, mean, fill=dest)) +\n  geom_col() +\n  coord_cartesian(ylim = c(20, 40)) +\n  labs(\n    title = \"Worst Arrival Delays for each Destination\",\n    y=\"Delay time in Minutes of Arrival\",\n    x=\"Destination Airport\"\n  ) \n\n\n\n\n\n\n\n\nThe worst-performing destination is CAE (Columbia Metropolitan Airport) with an average arrival delay of almost 42 minutes, which is significantly higher than the others. TUL and OKC also stand out with delays above 30 minutes.\nSo based on average delays, CAE is the destination where you are most likely to arrive late, by a pretty wide margin."
  }
]